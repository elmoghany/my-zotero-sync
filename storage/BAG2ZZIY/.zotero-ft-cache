MoMA: Multimodal LLM Adapter for Fast
Personalized Image Generation
Kunpeng Song1,2, Yizhe zhu1, Bingchen Liu1, Qing Yan1, Ahmed Elgammal2, and Xiao Yang1
1 ByteDance 2 Rutgers University
Abstract. In this paper, we present MoMA: an open-vocabulary, trainingfree personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plugand-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. We commit to making our work open-source, thereby providing universal access to these advancements. Project page
Keywords: image generation · multimodal · personalization · LLM
1 Introduction
Image generation technology has seen remarkable advancements with the emergence of large-scale text-to-image diffusion models such as GLIDE [23], DALL-E 2 [30], Imagen [35], Stable Diffusion [32], eDiff-I [4]. These models enable users to generate vivid images from a diverse set of text prompts. Despite their effectiveness, textual descriptions often fall short in expressing detailed visual features, leading to the rise of image-conditioned generation works like Kandinsky [31], Stable Unclip [32, 37], which utilize images as inputs to create variations that maintain the visual components of the reference. A natural progression in this field is subject-driven generation or image personalization. Initial efforts in this domain involve inverting input images into textual representations and employing learnable text tokens to denote target concepts. For instance, DreamBooth [33] fine-tunes the entire diffusion model to
arXiv:2404.05674v1 [cs.CV] 8 Apr 2024


2
Åutumn leaves Winter snow Green forest
Wooden Stone Minecraft
Autumn leaves Eiffel tower Mount Fuji
Bronze Green jade Paper
Chinese art Forest Santa hat Plushie Lego Cobblestone street Grand Canyon Minecraft
Reference
Reference Autumn Grand Canyon Cobblestone Reference Black hat Water Autumn
Reference Grand Canyon Autumn Beach
Fig. 1: Example images generated by our open-vocabulary personalization model. without tuning. With just one image of a subject (circled in blue), our model can generate text-aligned, identity-preserved new images of the same subject with only a single forward pass. Our model supports both re-contextualization where the same subject is located in a new environment, as shown in green, and changing the texture of the subject itself, as shown in red.
learn a unique identifier for specific subjects. Textual-Inversion [9] inverts the input images to a unique textual embedding and learns the embedding-image mapping during finetuning. Subsequent approaches, such as Custom Diffusion [17], have optimized this process by focusing on tuning only cross-attention layers to reduce computational costs. Techniques like Low-Rank Adaptation (LoRA) [13] and SVDiff [10] further minimized trainable parameters, enhancing fine-tuning efficiency. However, these methods, regardless of their accuracy, require extensive resources for per-instance tuning and model storage, limiting their practical application.
Tuning-free approaches have gained prominence for addressing these limitations. For example, IP-Adapter [43] leverages a unique cross-attention mechanism to differentiate text and image features, facilitating the injection of reference images as visual prompts. Nevertheless, it faces notable constraints, particularly in modifying textures. Methods like InstantID [38], InstantBooth [36], and PhotoVerse [6], while maintaining identity coherence, are confined to specific domains such as human faces or cats. Recent innovations have employed transformers to integrate visual concepts with text prompts, as seen in BLIP-Diffusion [19] and KOSMOS-G [26], which extract identity information and combine it with text prompts via cross-attention. These approaches, although effective in texture modification, often result in significant detail errors in a tuning-free setting and require extra tuning for optimal outcomes with target objects.


MoMA 3
In response to these challenges, this paper introduces a novel, open-vocabulary, and tuning-free image personalization model that excels in detail fidelity, object identity resemblance, and coherent textual prompt integration. Our model harnesses the capabilities of Multimodal Large Language Models (MLLMs) to seamlessly blend text prompts with visual features of the target object, enabling alterations in both the background context and object texture. In addition, our newly proposed self-attention shortcut significantly enhances the detail quality with minimal computational overhead. Built upon Stable Diffusion [32] and LLaVA [20–22], MoMA has been rigorously evaluated on various tasks with a wide array of images and dynamic prompts:
– For exact-object recontextualization tasks, it demonstrates superior detail accuracy and faithfulness to the target object across varied backgrounds. – For texture modification tasks, our method adeptly alters the texture of target objects as dictated by text prompts while preserving unmentioned visual features. – Our model achieves the above performance through extensive pre-training, eliminating the need for tuning at the evaluation stage.
2 Related work
2.1 Text-to-Image Diffusion Models.
Text-to-image diffusion models have become the forefront of image generation technology, garnering widespread interest for their exceptional capabilities in recent years. These models typically utilize a pre-trained language decoder to transform text prompts into latent representations that guide the diffusion process for generating or editing images. Notable models like GLIDE [23] and DisCo [39] leverage text-guided diffusion architectures and CLIP [29] guidance to enhance the fidelity and relevance of the generated images. Similarly, Stable Diffusion [32] stands out by executing the diffusion process in latent image space, thus significantly lowering computational demands. It was further advanced by Stable Diffusion XL(SDXL) [28], which introduced a larger UNet and an additional text encoder for improved textual influence on the images. Also, diffusion models have shown remarkable efficiency in capturing data distributions for image synthesis, with applications expanding by integrating transformer-based architectures [27]. The advent of text-guided image synthesis, mainly through models like Stable Diffusion, highlights significant advancements in achieving top-tier results in text-to-image synthesis tasks. Stable Diffusion, a prominent latent diffusion model, operates within a latent space defined by a pre-trained autoencoder, enabling efficient handling of semantic features and visual patterns for enhanced image synthesis.
2.2 Personalized Image Synthesis.
Recently, personalization has become an emerging factor in the vision and graphics community. Previous researchers have explored optimization-based approaches,


4
such as Textual Inversion [9] and DreamBooth [33]. Later works found it’s sufficient to just update cross-attention modules in the diffusion unet. Various parameter efficient optimization methods were then developed to further speed up tuning, such as LoRA [13] and SVDiff [10]. The drawback for these methods is they require fine-tuning for each new reference image, which is computationally expensive and time-consuming. More recent efforts attempt to get rid of per-object tuning such as [15, 36, 41], which pre-train the diffusion model on domain-specific images, such as cat and human face images. These models provide class-specific prior for generation thus require less tuning within that domain. However, they are constrained to the trained class and are not able to generalize to other subjects. Another category of works [3, 7, 19] focuses on more general open-vocabulary data. AnyDoor [7] and BreakTheScene [3] generate new images of the same object under various backgrounds but fail in changing textures. More related to our work, BLIPDiffusion [19] uses a pre-trained transformer feature extractor and works on a wide range of subjects, however, its results contain abundant details mistakes and require few-step tuning to achieve good quality results. We posit that this is due to the lengthy information path negatively impacting the quality of image features. IP-adapter [43], as a lightweight plug-and-play model, directly injects the visual feature of the reference image into the UNet and achieves promising performance. However, the migrated image features can hardly interact with target prompts. This traps into a trade-off between prompt fidelity and image fidelity, especially when the prompt requests for drastic context changing or texture editing: higher strength results in better subject details at the cost of worse prompt faithfulness. Nevertheless, these methods all suffer from achieving identity preservation, edibility, generalization ability and high fidelity simultaneously. Our method, however, is able to make progress in those key directions within the single input image and tuning-free domain.
3 Method
In this section, we first introduce some preliminaries about text-to-image diffusion models and multmodal LLMs. Then, we depict in detail the motivation and the design of the proposed multimodal LLM adapter.
3.1 Preliminaries.
Text-to-Image Diffusion Models Text-to-image diffusion models generate images that align with the textual description by gradually denoising a random sample drawn from a Gaussian distribution. Our work is established on text-toimage Latent Diffusion Models (LDM), which perform the diffusion process in latent space, making it more practical and computationally efficient. Given an input image x, LDM first extracts the latent feature z = E(x) with a well-trained


MoMA 5
Multi-modal Decoder
A photo of a [dog]. Describe a new image of the same [dog]: A dog at mount Fuji.
Linear
LN
A dog at
mount Fuji
Text
Encoder
Text features
Contextualized
features Cross
Attention
Subject Cross Attention
Linear
U-Net (shared weight) t=0
U-Net
self
cross
self
cross
Trainable
Frozen
Model Structure
Context Cross Attention
Masked
Prompt: dog
Learnable Token
Igen
cross attention
self attention
cross attention
cross attention
MLLM
vision
encoder
Self-attention features
Iref
wb
Attention
Blocks
Fig. 2: Model structure. (1) On top-left, we adopt a generative multimodal image decoder to extract semantic features and modify them by the target prompt. These features are projected to text space and then injected into a pretrained frozen UNet with decoupled context cross-attentions as illustrated in light red. (2) On bottomleft, to further improve detail accuracy, we forward the clear reference image (t = 0) to the same UNet and extract self-attention features. These fine-grained features contain detailed information about the subject and are injected into UNet through decoupled object cross-attention layers as illustrated in orange. (3) The model is trained using a two-staged training pipeline: we first train the multimodal decoder (multimodal generative learning), then jointly optimize newly added attention modules in UNet.
encoder. During training, the noisy latent variables zt is obtained by gradually adding noises to z for t steps, LDM optimizes the following objective:
L = Ezt,t,C,ε∼N (0,1)
h
∥ε − εθ(zt, t, C)∥2
2
i
, (1)
where C denotes the textual embedding of prompts extracted by a pre-trained CLIP text encoder. LDM is commonly parameterized as an UNet model. We employ the pretrained Stable Diffusion as the LDM, where the UNet model has cross-attention and self-attention layers in different resolutions. Given the image features Z as query and the textual embedding C as key, the output of cross-attention Z′ can be defined by the following equation:
Z′ = Attn(Q, K, V ), (2)
where Q = ZWq, K = CWk, V = CWv are the query, key, and values matrices of the attention operation respectively, and Wq, Wk, Wv are the weight matrices of the trainable linear projection layers.
Multimodal LLM (MLLM) The field of natural language processing (NLP) has undergone a revolutionary shift with the emergence of large language models (LLMs). These models, distinguished by their comprehensive training across varied datasets, demonstrate extraordinary proficiency in a range of linguistic tasks.


6
Benefiting from robust pre-training methodologies, pioneering models, such as ChatGPT-Vision [1], Mini-GPT4 [44], CogVLM [40], and LLaVA [22], have set the stage for more sophisticated iterations in the multimodal field. These iterations notably integrate visual tasks into the LLM framework, and have proven to be exceptional in vision-related tasks, including vision-question-answering, visual-grounding, and visual-segmentation. Among these MLLMs, LLaVA [22] stands out as an open-source Large Language and Vision Assistant, synergizing a vision encoder with an LLM for comprehensive visual and language understanding. LLaVA capitalizes on the strengths of both the pre-trained LLM and a vision transformer image encoder. This model skillfully processes images in tandem with language instructions, delivering responses in natural language, thereby bridging the gap between vision and linguistic comprehension.
3.2 Methodology
We present MoMA, a multimodal LLM adapter enhanced by fine-grained feature transfer. The overall architecture is demonstrated in Figure 2. Our method consists of three parts: (1) a generative multimodal decoder is utilized to extract image features from the reference image and edit it following the target prompt, yield the contextualized image feature; (2) in the meantime, we replace the background of the original image by white color, leaving only object pixels, leveraging the original UNet’s self-attention layers to extract the object image feature; (3) finally, during the new image generation process, we injected the contextualized image features and the object image features into the UNet diffusion model with the dedicatedly trained context-cross-attention layers and object-cross-attention layers, respectively.
Multimodal Generative Image-feature Decoder We introduce a multimodal generative image-feature decoder, which actively generates target image features by combining visual information from the reference image and textual information from text prompt. Practically, we adapt a pre-trained MLLM, specifically LLaVA-7B, to serve as our generative multimodal decoder. As shown in Fig. 2 upper-left branch, given a reference image Iref and its object mask Mref , we get a white-background reference image by Iwb
ref = Iref ∗ Mref . We construct an instruction sequence as the input to MLLM: " < fref > An image of < label >. Describe < Ptgt >", where label is the subject keyword (e.g. cat, car, etc.), and Ptgt the target prompt. A learnable token is appended at the end of the instruction sequence. After forwarding the MLLM, the embedding corresponding to this learnable token is the output of our multimodal imagefeature decoder. We call it decoder as, intuitively, it is trained to combine visual features with the target prompt in a generative manner and output an image embedding. By design, the MLLM image-feature decoder edits the backgroundexcluded image feature of Iwb
ref following a background-included target prompt Ptgt that describes an entire image.


MoMA 7
Linear
Self Attention
A photo of a dog. Describe a new image of the same dog: [a 3D rendering of a Pixar dog].
(a) Multi-modal Generative Learning
MLLM Generative Learning Objective
Iref
Multi-modal Decoder
Iref
Learnable Token
MLLM vision encoder
wb
self
cross
self
cross
U-Net t ~ [T, 0]
Mask extraction
“A dog at mount Fuji”
“dog”
to t-1
Text features
Contextualized features ...
...
......
Mask injection
(b) Iterative Self Attention Masking
merge
Mgen(t)
Mref
from t+1
Mgen(t+1)
Igen
Fig. 3: Multimodal Generative Learning and iterative Self-Attention Masking
The generated image feature from the multimodal image-feature decoder is then converted into a sequence of embedding in R768 with length N (we use N = 4 in this work) through a linear layer. Inspired by IP-Adapter [43], the embedding sequence is then integrated into the pre-trained UNet model with decoupled cross-attention as shown in Fig. 2 upper-right branch.
Self-Attention Feature Transfer To further enhance the detail faithfulness, we involve image self-attention features and apply a masking mechanism. Specifically, the same pre-trained UNet is leveraged as the self-attention feature extractor. As shown in Fig. 2 lower branch, Iwb
ref is forwarded through the diffusion UNet with t = 0 as timestep and label as text condition. Features at each selfattention layer are collected and transferred into the main UNet model by the adapted modules with decoupled self-attention. The self-attention feature transfer is an effective information shortcut as the extracted feature ci carries fine-grained details. However, directly applying it will cause interference between the backgrounds of Iwb
ref and Igen. To address this issue, we present a self-attention masking procedure. Ideally, we want only the features of the foreground in Iwb
ref to be injected into the foreground of Igen. The features of the background in Iwb
ref should be eliminated and the background of Igen should remain unaffected by the self-attention feature transfer. We apply a masking mechanism using the reference image mask Mref and the generated image mask Mgen. The output of our modified self-attention is:
Znew = Attn(Q, K, V ) + λ · Attn(Q, K′, V ′, Mref ) · Mgen · β (3)
where λ is a learnable parameter. K′ and V ′ are the key and values calculated from the extracted self-attention feature ci by K′ = ciW ′
k and V ′ = ciWv′ . Here,
W′
k, Wv′ are the weight matrices of the newly introduced decoupled subjectcross-attention projections. The reference image mask Mref is applied inside of Attn in the form of the attention mask, and the generation mask Mgen is applied through an element-wise product. β is a strength scalar for additional controls. During training, the model is optimized to reconstruct the background-included reference image Iref . The white-background reference image Iwb
ref and the tar


8
get image Iref share the same mask, so Mref = Mgen. During inference, the ground truth Mref is available but the ground truth Mgen isn’t. We use the cross-attention map corresponding to the subject label to approximate Mgen. Specifically, as shown in Fig. 3 (b), during each denoising step, the attention map of label from each cross-attention layer is extracted and averaged into Mgen(t). We use it to approximate Mgen in the next denoising step.
Multimodal Generative Learning and Diffusion Learning. Unlike previous works that extract the image features of the subject as it is, we generate image features that are well-modified following the target text prompt. Previous works, like IP-Adapter, inject image features into the cross-attention layers of the UNet without interacting with the target prompt. This is problematic, especially when the target prompt involves texture-changing the subject. On the other hand, our multimodal image-feature decoder imagines the full image given a white-background object image and a text prompt describing the full image. which dramatically improves model performance, especially in changing subject textures. It ensures the output preserves the identity of the target object while respecting the text prompt. To achieve the best model performance, we propose a two-staged pre-training strategy.
First, we propose a Multimodal Generative Learning Stage, where we pre-train the multimodal image-feature decoder such that it learns to compose image features of the subject with the target prompt and output the CLIP embedding of the target image. To this end, we need to take advantage of the generative capability of the MLLM: while initially trained to generate text, we adapt it to generate image embeddings. As shown in Figure 3 (a), Iwb
ref is encoded by the MLLM vision encoder and combined with its caption Pref , together with a learnable token, into a prompt instruction. This sequence is fed into the MLLM: a 15-layer transformer. The output of the learnable token is trained to match the CLIP image embedding of the original reference image Iref . Once being welltrained, our MLLM will generate prompt-contexualized image embeddings. The loss function of this stage is formulated as:
LMLLM = MLLM CLIP Iwb
ref , Pref , Token − CLIP (Iref ) 2
2 (4)
Second, we design a Diffusion Learning Stage that faithfully converts the contextualized image embeddings to an image. During this stage, we freeze MLLM and pre-trained diffusion model and optimize only the decoupled subject and contextual attentions and their linear mappings. The model is trained on the OpenImage dataset, using the same training objective as shown in Eq. (1). Classifier-free guidance (CFG) [12, 43] improves diffusion generaton quality. However, we find it better to only enable it for the context-cross-attention side and not on the subject-cross-attention side. Specifically, in the second training stage, to enable CFG on the context-cross-attention side, we randomly replacing the contextualized feature with an all-zero image embedding.


MoMA 9
Reference Grand Canyon Autumn leaf Winter snow Beach Jungle Sunflower field Dirt road Wheat field
Fig. 4: Zero-shot new context. Visualization of our generated samples for various images and prompts. Exact subject with different context.
4 Experiments
4.1 Training and Implementation Detail
To train our model, we construct a dataset of 282K image/caption/image-mask triplets from the OpenImage-V7 [18] dataset. We use BLIP-2 OPT6.7B to generate captions [19] for the images, then remove human-related subjects and filter out color, shape, and texture keywords. We use the subject mask provided in OpenImage as Mref . Evaluation images do not come with a mask, so we use SAM [16] to extract main objects and build masks thereafter. We use Stable Diffusion v1.5 [32] with RealisticVision [35] checkpoint as our foundation diffusion model. We load LLaVA-7B as our MLLM decoder in stage-one training. In stage-two training, we load IP-Adapter [43] checkpoints to initialize our context cross-attention layers, and zero-initialize our object cross-attention layers. We evaluate the model using various images and prompts. We present qualitative examples to illustrate the effectiveness of our model. In Fig. 4, the target prompts specify a novel contextual environment. Our model seamlessly generates a high-quality background while precisely situating the same object within this new setting. In Fig. 5, the prompts indicate a change in texture. Our model showcases its ability to render realistic textures in response to the textual cues, adeptly altering specified visual elements while leaving other identity aspects of the image unaffected.


10
Reference Lego Paper Gold Wood silver Minecraft Sketch Green jade
Fig. 5: Zero-shot new texture. Visualization of our results with new texture, material, color and style. Our model correctly balances between prompt and image fidelity.
Qualitative Comparison. We conducted a comparative analysis to evaluate the performance of our method against existing tuning-free open-vocabulary personalization approaches. For recontextualization, we use a variety of prompts and images to qualitatively assess our model. To ensure a fair comparison, we generated 50 random samples for each model and presented the highest-quality examples. The results, as illustrated in Fig. 6 upper panel, show our method’s proficiency in detail and background quality. Particularly, the green-marble eyes and facial details in cat images is notably refined. The backgrounds generated by our model also exhibit enhanced appeal, diversity, and realism, a benefit attributed to our masked cross-attention mechanism. In texture editing, analyzed in Fig. 6 lower panel, our method consistently maintained the shape and contour of subjects while adapting them to eight different textures. This contrasts with baseline methods, which often struggled to balance the prompts and images effectively. Our results indicate a marked improvement in texture adaptation while preserving the integrity of the original subjects. We show more qualitative and uncurated results in Appendix B.
Quantitative Comparison We use the Dreambooth [33] dataset to conduct the quantitative evaluation. For a fair comparison, we generate 4 images conditioned on the image prompt for each dataset sample, resulting in 14k generated images for each method. In Tab. 1, we evaluate subject fidelity using DINO [5]


MoMA 11
Ours IP-adapter Blip-diffusion
A robot in autumn with falling leaves
A robot in a green garden with spring flowers
A robot in a future Cyberpunk city
A robot on a cobblestone street
Ours IP-adapter Blip-diffusion
Ours
Blip-diffusion IP-adapter Ours IP-adapter Blip-diffusion
Reference
A green jade sculpture of a dog A black and white pencil sketch of a car
Reference A cat on a beach A cat in a jungle
A robot in autumn mountain and lake A robot in a winter national park
Fig. 6: Zero-shot Qualitative Comparison. We share recontextualization in upper panel and texture editing in lower panel. Our results have significantly more accurate details for context editing and better balancing between prompt and image fidelity in texture editing.
and CLIP-I [29] scores, and prompt-following ability using CLIP-T [29] scores. We disaggregate the CLIP-T score into two distinct scenarios: the recontextualization task denoted by CLIP-I(c), and the texture editing task denoted by CLIP-I(t). To evaluate CLIP-I(t), we introduce 10 new prompts specifically tailored to texture editing. We exclude CLIP-I and DINO scores from the texture editing experiments, as they could not serve as effective metrics in this scenario: a high DINO/CLIP-I score frequently doesn’t equate to high quality; rather, it indicates a failure to alter the texture as intended. In Tab. 2, we conduct user studies on recontextualization and texture editing. Our method shows a significant performance boost across these metrics, especially in prompt-following. We show a detailed breakdown of quantitative results and more information about human evaluation in Appendix B and C.


12
Table 1: Quantitative comparisons.
Methods DINO ↑ CLIP-T(c) ↑ CLIP-I ↑ CLIP-T(t) Real Images 0.774 0.885 – Textual Inversion 0.569 0.255 0.780 Re-Imagen 0.600 0.270 0.740 IP-Adapter 0.612 0.330 0.793 0.319 BLIP-Diffusion 0.594 0.300 0.779 0.271 Ours 0.618 0.348 0.803 0.335
Table 2: Human Evaluations.
Metric Ours IP-Adapter BLIP-Diffusion
Subject Fidelity ↑ 0.550 0.416 0.034 Prompt Fidelity ↑ 0.633 0.351 0.016
Texture Editing ↑ 0.523 0.316 0.161
Overall Quality ↑ 0.596 0.368 0.0366
In Fig. 7, to visually compare model performances, we show the CLIP-T(c) (left) for recontextualization and CLIP-T(t) for texture editing experiments on 30 subjects. The left chart presents the results for the recontextualization task, where the exact subject is placed within a new context described by prompts. Our method consistently outperforms the baselines. The right chart depicts the results for the texture editing task, wherein the texture of the subject itself is altered by prompts. Both baselines experience a dramatic drop in quality. Consequently, the performance gain of our method becomes even more significant.
Ours 0.4
0.3
0.2
IP-Adapter Blip-Diffusion
context texture
Fig. 7: Visualized CLIP-T for 30 subjects. The left chart is for the context editing task and the right is for texture editing. In each chart, one vertical axis represents one subject. Results from different models are colored differently.
4.2 Ablation and Analysis
In this section, we conduct ablation studies on our proposed subject-crossattention modules and self-attention masking mechanism. The contextualized features derived from our MLLM predominantly cater to semantic understanding and the general appearance, but they inherently lack fine-grained details. To address this, we introduce a subject-cross-attention feature transfer mechanism, coupled with a masking procedure. This combination serves as a vital component for enhancing detail fidelity. The effectiveness of this approach is ablated below. We show the effectiveness of our attention module’s feature transfer ability. In Fig. 8, when β = 0, subject-cross-attention is disabled, and the model entirely relies on the multimodal decoder and its context-cross-attention injection. The generated subjects in the result images are similar to the references in shape and color, but the details are mistaken. As β increases, we observe consistent improvement in detail fidelity.


MoMA 13
Reference
β = 1.2 β = 0.6 β = 0
A bird floating in water in a forest
Reference
β = 1.2 β = 0.6 β = 0
A robot in a future cyberpunk city
A dog in autumn with falling leaves A car on a beach
(zoom in)
A cat in winter snow
Reference β from 1.2 to 0
Fig. 8: Effect of self-attention feature transfer. We test various self-attn injection strengths (β) on the same fixed ZT . Our self-attn transfer is highly effective in keeping exact subject details. As β increases from 0 to 1.2, we see results details improve consistently: the cat/dog results change from photorealism to Pixar style, and the robot/bird becomes more accurate. We show zoom-in images for the car and notice how the shape and light of the car headlight gradually become accurate as β increases.
Reference
A dog in a green garden with flowers.
Mask strength = 1 Mask strength = 0.5 w/o (Mask strength = 0)
Fig. 9: The effect of self-attention masking. We sample six ZT noises and monitor their generation results with three mask strengths. As the mask strength decreases, backgrounds get more interfered with, resulting in artifacts. Iterative Masking takes care of the background and foreground separately, greatly improving background quality. The visualization of the cross-attn mask also confirms its effectiveness, as the w/ results are much less diffused than the w/o.


14
We then analyze the effectiveness of our masking mechanism. As mentioned previously, since Mgen is unknown in generation time, the subject-cross-attention masking itself is adapted to an iterative masking manner. As shown in Fig. 9, this design ensures the generated backgrounds are unaffected by the self-attn feature transfer and greatly improves image quality. We visualize the cross-attn map Mgen(t=0) for the keyword label (in this case, dog) after the last denoising step. Notice Mgen(0) is noisy and inaccurate when w/o masking. When it is fully activated, Mgen(0) becomes clean and clear and the approximation to Mgen becomes accurate. The proposed masking technique is crucial and effective to ensure clear and diverse image backgrounds. We show quantitative results for these ablations in Tab. 3 and Tab. 4.
Table 3: Masking Ablation.
Masking DINO ↑ CLIP-T(c) ↑ CLIP-I ↑ CLIP-T(t) 1.0 0.618 0.348 0.803 0.335 0.5 0.603 0.322 0.797 0.231 w/o 0.575 0.298 0.788 0.322
Table 4: Subject-cross-attn Ablation.
β DINO ↑ CLIP-T(c) ↑ CLIP-I ↑ CLIP-T(t) 1.0 0.618 0.348 0.803 0.335 0.5 0.591 0.348 0.788 0.338 w/o 0.583 0.349 0.782 0.319
As further analysis, we test our model on more problem settings in addition to the previous results. As shown in Fig. 10, our model generates high-quality subject-coherent images across diverse problem settings, including accessory incorporation, pose modification, and camera perspective controls. This shows it can faithfully generate subject details while allowing flexibility in prompts.
Reference Black hat Christmas hat Iron man suit Reference Front side back
(no view condition)
sitting running jumping to camera lying
Reference
Fig. 10: More results on accessory incorporation, pose modification, and camera perspective control.


MoMA 15
Additionally, our model is an universal adapter because we freeze the original diffusion model in the training stage. It can generalize to the custom model checkpoints fine-tuned from the same base model. In Fig. 11, we verify this on community models from HuggingFace and CivitAi [8] including Realistic Vision V4.0 [2], ReV-Animated [34], Anything v4 [42] and Esthetic Retro Anime [24]. These models are all fine-tuned from SD v1.5. MoMA can be directly applied to these community models without any modification.
Reference
Realistic Vision V4.0
Anything V4.0
In a green garden with flowers
In autumn with falling leaves
In a flower
field In a jungle
On a city road
On a road with buildings and trees
At mountain and lake
ReV_Animated
Esthetic Retro Anime
Fig. 11: Although trained on RealisticVision, our model can be directly applied to various community checkpoints and yields high-quality coherent results.
5 Conclusion
In conclusion, we have proposed the powerful MoMA for fast image personalization on the text-to-image diffusion model. It is tuning-free, open-vocabulary, and supports recontextualization and texture editing. The results of our experiments show its superiority against existing methods. Our proposed multimodal image-feature decoder successfully harnesses the strength of MLLM for contextualized feature generation. Our masked subject-cross-attention technique provides a compelling feature shortcut significantly improving detail accuracy. Additionally, as a plug-and-play module, our model can be directly integrated with community models tuned from the same base model, extending its applications to a broader area.


16
References
1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 6 2. Adhik Joshi: realistic vision v40 (2024), https : / / huggingface . co / stablediffusionapi/realistic-vision-v40, accessed: 2024-03-06 15, 19
3. Avrahami, O., Aberman, K., Fried, O., Cohen-Or, D., Lischinski, D.: Break-a-scene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311 (2023) 4 4. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al.: ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022) 1 5. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9650–9660 (2021) 10 6. Chen, L., Zhao, M., Liu, Y., Ding, M., Song, Y., Wang, S., Wang, X., Yang, H., Liu, J., Du, K., et al.: Photoverse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793 (2023) 2 7. Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481 (2023) 4 8. Civitai: Civitai (2024), https://civitai.com/, accessed: 2024-03-06 15 9. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion (2022). https://doi.org/10.48550/ARXIV.2208.01618 2, 4
10. Han, L., Li, Y., Zhang, H., Milanfar, P., Metaxas, D., Yang, F.: Svdiff: Compact parameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305 (2023) 2, 4
11. Haotian Liu: Llava-1.5-7b (2024), https://huggingface.co/liuhaotian/llavav1.5-7b, accessed: 2024-03-14 19 12. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022) 8 13. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021) 2, 4
14. Humza Iqbal: Clip-h (2024), https://github.com/mlfoundations/open_clip, accessed: 2024-03-14 19 15. Jia, X., Zhao, Y., Chan, K.C., Li, Y., Zhang, H., Gong, B., Hou, T., Wang, H., Su, Y.C.: Taming encoder for zero fine-tuning image customization with text-to-image diffusion (2023) 4 16. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023) 9 17. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion (2023) 2 18. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al.: The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision 128(7), 1956–1981 (2020) 9


MoMA 17
19. Li, D., Li, J., Hoi, S.: Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems 36 (2024) 2, 4, 9, 30 20. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning (2023) 3 21. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024) 3 22. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023) 3, 6 23. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021) 1, 3
24. OneRing: era-esthetic retro anime (2024), https://civitai.com/models/137781/ era-esthetic-retro-anime, accessed: 2024-03-06 15 25. OpenAI: Clip-l (2024), https://huggingface.co/openai/clip- vit- largepatch14, accessed: 2024-03-14 19 26. Pan, X., Dong, L., Huang, S., Peng, Z., Chen, W., Wei, F.: Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023) 2 27. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4195–4205 (2023) 3 28. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023) 3 29. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021) 3, 11 30. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 1(2), 3 (2022) 1 31. Razzhigaev, A., Shakhmatov, A., Maltseva, A., Arkhipkin, V., Pavlov, I., Ryabov, I., Kuts, A., Panchenko, A., Kuznetsov, A., Dimitrov, D.: Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502 (2023) 1 32. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684–10695 (2022) 1, 3, 9, 19 33. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation (2022) 1, 4, 10
34. S6yx: Rev animated (2024), https://huggingface.co/s6yx/ReV_Animated, accessed: 2024-03-06 15 35. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic textto-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems 35, 36479–36494 (2022) 1, 9


18
36. Shi, J., Xiong, W., Lin, Z., Jung, H.J.: Instantbooth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411 (2023) 2, 4
37. Stable Unclip: Stable unclip (2024), https://huggingface.co/docs/diffusers/ en/api/pipelines/stable_unclip, accessed: 2024-03-06 1
38. Wang, Q., Bai, X., Wang, H., Qin, Z., Chen, A.: Instantid: Zero-shot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519 (2024) 2 39. Wang, T., Li, L., Lin, K., Lin, C.C., Yang, Z., Zhang, H., Liu, Z., Wang, L.: Disco: Disentangled control for referring human dance generation in real world. arXiv e-prints pp. arXiv–2307 (2023) 3 40. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023) 6 41. Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848 (2023) 4
42. Xyn AI: Anything v4.0 (2024), https://huggingface.co/xyn-ai/anything-v4.0, accessed: 2024-03-06 15 43. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models (2023) 2, 4, 7, 8, 9, 19, 30 44. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023) 6


MoMA 19
Appendix
A. Implementation Details
A2. Architecture
We leverage the advanced pre-existing knowledge from pretrained MLLMs to aid in model convergence. Specifically, we initialize our multi-modal decoder with LLaVA-v1.5-7B checkpoint [11] from HuggingFace. Its vision encoder is CLIP-vit-large-patch14 (CLIP-L) [25] and takes in Iwb
ref . We use ViT-H/1 [14] to extract the image embedding of Iref . Thus, a trained linear mapping is added at the output of MLLM to map the learned text token from R4096 to R1024. The model is built on top of the IP-Adapter [43] and the training code on LDM [32]. Our foundation model is StableDiffusion v1.5 [32] with Realistic-Vision-v4.0 [2] checkpoint. Both subject and context cross-attentions are implemented with the diffusers attention processor class. We call our MLLM as multi-modal decoder because: it takes in visual features of Iwb
ref and text instruction and "generates the full image Iref " in the form of CLIP embeddings. So our MLLM is not a typical subject-feature encoder like in most existing works, but instead a generative MLLM decoder.
A2. Training
we lock the parameters of the image encoders and only adjust the MLLM parameters along with the newly incorporated learnable token. In stage two training, the image/text encoder, multi-modal decoder, and the entirety of the UNet parameters are frozen. We optimize exclusively the linear mappings of subjet-crossattention and context-cross-attentions.To expedite convergence, we employ attention mapping layers from the IP-adapter [43] and start with a zero-initialized subject-cross-attention. We apply large gradient accumulation steps (200) and clip grad norm for smoother training. In both stages, we use the AdamW optimizer with a learning rate of 1−5. Training takes 7 days in total on 16 A100 80G GPUs.
A3. Evaluation
We use the DDIM sampler with default parameters and 50 denoising steps. Evaluation requires 23.5G GPU RAM for 512 × 512 resolution. The iterative masking is disabled in the first denoising step since Mgen(t) is unavailable yet. We apply classifier-free guidance on both the text prompt and Context-crossattention side, but not the subject-cross-attention. On the text prompt side, simply replace the target prompt Ptgt with null embedding. On the contextcross-attention side, we zero out the contextualized features before feeding them to attention linear mappings. We use β = 1.0 in the recontextualization task for better subject detail accuracy and, empirically, β = 0.5 in the texture editing task for better balance between subject and prompt.


20
B. Visual comparison and more Results
B1. Uncurated Comparison with Baselines
We show uncurated results for our model and the baselines. Results are random and not selected. For each method, 8 images are generated. IP-Adapter failed to accurately capture the structure of the car in Fig. 12. In Fig. 13, increasing the weight/scale to 0.85 in IP-Adapter still yields a wrong color pattern of the dog. Under this scale, The original white background strongly interferes with its generated image. Ours correctly captures the color and structure pattern with much more favorable backgrounds, respecting both the reference image and the text prompt. We conclude from above that:(i) Our multi-modal LLM decoder and its contextualized feature are crucial to our model performance. It extracts visual features from reference image Iwb
ref and contextualizes it with the target text prompt Ptgt. Its output is trained to match the overall CLIP image embedding of the resulting image and provides critical features to bridge the gap between the subject feature and the target prompt. (ii) Iterative masking helps distinguish between the background and main subject, improving CLIP-T. Removing it leads to corrupted backgrounds and significantly lower image quality. (iii) Disabling the Subject-cross-attention worsens the CLIP-I score as features from MLLM are insufficient to accurately reproduce subject details. This quantitative ablation lines up with our intuition: our multi-modal decoder and its context-cross-attention provide vital image features and serve as a foundation. Subject-cross-attention and iterative masking help in subject detail accuracy and background quality respectively.
B3. More Results
In Tab. 5, we show detailed quantitative results for the DreamBooth dataset. From Fig. 14 to Fig. 19, we provide additional qualitative results on various subjects and prompts. Reference subject images are on the left. In the rest columns, we provide generated renditions. Fig. 14 to Fig. 17 are about context editing and Fig. 18 and Fig. 19 are about texture editing.
C. Discussions
C1. Social Impact
While tuning-based personalization models are largely inaccessible to most people because of computation resource limits, our method of open-vocab tuningfree personalization model helps democratize such models to everyday users with a significantly improved quality. However, it also bears the potential risk of being exploited for the creation of deceptive content or the propagation of misinformation. To address this concern, we have specifically designed our training process to exclude person-related subjects and focus on generic objects. This intentional


MoMA 21
Ours IP-adapter Blip-diffusion
DINO CLIP-I CLIP-T(c) CLIP-T(t) CLIP-T(c) CLIP-T(t) CLIP-T(c) CLIP-T(t)
backpack 0.590 0.861 0.350 0.338 0.342 0.337 0.324 0.292 backpack2 0.438 0.698 0.361 0.351 0.357 0.365 0.323 0.296 boot 0.502 0.819 0.346 0.345 0.330 0.325 0.287 0.277 bowl 0.621 0.699 0.350 0.338 0.300 0.339 0.249 0.257 can 0.657 0.749 0.361 0.321 0.355 0.347 0.310 0.294 candle 0.474 0.734 0.361 0.353 0.348 0.330 0.329 0.297 cartoon 0.574 0.794 0.305 0.312 0.284 0.278 0.255 0.248 cat 0.813 0.840 0.354 0.339 0.330 0.300 0.320 0.261 cat2 0.750 0.845 0.358 0.345 0.327 0.310 0.292 0.263 clock 0.679 0.866 0.335 0.341 0.305 0.324 0.322 0.270 dog 0.725 0.863 0.342 0.334 0.328 0.308 0.302 0.261 dog2 0.652 0.863 0.338 0.335 0.322 0.301 0.302 0.260 dog3 0.609 0.799 0.342 0.345 0.326 0.304 0.303 0.257 dog5 0.574 0.761 0.344 0.340 0.298 0.279 0.324 0.257 dog6 0.713 0.857 0.332 0.324 0.320 0.300 0.282 0.253 dog7 0.709 0.850 0.338 0.338 0.325 0.287 0.305 0.248 dog8 0.654 0.842 0.337 0.333 0.316 0.289 0.306 0.246 plushie 0.620 0.781 0.367 0.334 0.341 0.315 0.304 0.287 plushie2 0.491 0.754 0.372 0.333 0.340 0.336 0.326 0.299 plushie3 0.621 0.794 0.366 0.323 0.343 0.317 0.289 0.271 poop 0.533 0.732 0.345 0.339 0.335 0.338 0.327 0.291 sneaker 0.665 0.814 0.350 0.347 0.333 0.312 0.276 0.256 sneaker2 0.704 0.824 0.340 0.346 0.325 0.305 0.269 0.264 sunglasses 0.655 0.851 0.349 0.344 0.339 0.348 0.299 0.293 teapot 0.642 0.852 0.383 0.361 0.366 0.359 0.294 0.288 toy1 0.508 0.765 0.341 0.306 0.332 0.292 0.292 0.258 toy2 0.627 0.821 0.334 0.310 0.320 0.328 0.287 0.260 toy3 0.532 0.741 0.348 0.313 0.337 0.311 0.292 0.266 vase 0.593 0.815 0.358 0.339 0.338 0.356 0.295 0.299 Average 0.618 0.803 0.348 0.335 0.330 0.319 0.300 0.271
Table 5: Quantitative details on DreamBooth datasets. We add 10 new prompts focusing on texture editing to calculate CLIP-T(t). These prompts are: A sculpture of a label made of (Lego/Paper/Gold/Wood/silver/green jade/glass/stone/sketch/Minecraft).


22
IP-Adapter
Ours
BLIP-diffusion
Uncurated x8. Prompt: a car in autumn with falling leaves
Fig. 12: Uncurated random renderings from our model and baselines. IP-Adapter failed in capturing car structures, despite increasing its scale from default 0.6 to 0.85. Our results have better detail accuracy, more diverse composition, and favorable quality than the baselines.


MoMA 23
IP-Adapter
Ours
BLIP-diffusion
Uncurated x8. Prompt: a dog in green garden with spring flowers
Fig. 13: Uncurated random renderings from our model and baselines. IP-adapter generates inaccurate color patterns, even with an increased scale(weight) to 0.85. Since it does not distinguish background from foreground, some results failed to follow the "garden with flower" in the prompt. Our results have more accurate details with much better backgrounds.


24
Wheat field Eiffel tower Beach Dirt road
sunglasses
Cobblestone street Floating in water Mount Fuji Autumn with leaves
Winter snow Eiffel tower Jungle Forest
cat
Grand canyon Floor Autumn with leaves Winter snow
Sunflower field Palace Versailles Grand canyon Mount Fuji
Forest Floating in water Cobblestone street Palace Versailles
sneaker
Fig. 14: Additional results from our model. Change context.


MoMA 25
Sunflower field Winter snow Jungle Eiffel tower
cat
Palace Versailles Mount Fuji Winter snow Forest
Forest Palace Versailles Beach Mount Fuji
Cobblestone street Autumn with leaves On a stone On a road
toy
Cobblestone street Winter snow Beach Spring Mount Fuji
dog
Autumn with leaves Autumn with leaves Mount Fuji Eiffel tower
Fig. 15: Additional results from our model. Change context.


26
Jungle Autumn with leaves Dirt road Wheat field
boot
Mount Fuji Palace Versailles Autumn with leaves Cobblestone street
Grand canyon Winter snow Cobblestone street Autumn with leaves
Eiffel tower Jungle Black hat and monocle Batman suit
dog
Cobblestone street Dirt road Grand canyon Autumn with leaves
cat
Forest Black hat and monocle Christmas hat Iron man suit
Fig. 16: Additional results from our model. Change context.


MoMA 27
Iron man suit Sunflower field Cobblestone street Wheat field
dog
Grand canyon Autumn with leaves Eiffel tower Black hat and monocle
Grand canyon Winter snow Beach Sunflower field
car
Floating in water Palace Versailles Jungle Mount Fuji
Cobblestone street Spring Mount Fuji Eiffel tower Autumn with leaves
Winter snow Grand canyon Spring flower Christmas hat
bird
Fig. 17: Additional results from our model. Change context.


28
gold silver sketch green jade
cat
wood paper plushie stone
Minecraft paper sketch bronze
car
plushie gold paper Minecraft
Silver gold comic art wood
sketch comic art green jade bronze
dog
Fig. 18: Additional results from our model. Change texture.


MoMA 29
glass green jade silver paper
dog
gold bronze stone wood
sketch paper green jade Lego
teapot
plushie Lego green jade stone
wood gold Minecraft stone
paper wood silver bronze
cat
Fig. 19: Additional results from our model. Change texture.


30
limitation reduces the model’s ability to generate convincing counterfeit images where individuals are central elements. To ensure the integrity of content generated by our model, we advise a thorough examination of its outputs before deploying our model in consumer-facing applications.
C2. Failure Examples
bowl
can
Fig. 20: Failure cases. The text messages displayed on the bowl and can appear distorted or missing in the resulting images, a flaw inherited from SD v1.5. This version is notably challenged in its ability to reproduce text with accuracy.
We also noticed that some subjects are much easier to learn than others [19]. For example, the model generates high-quality results for dogs and cats with consistent identity and almost identical details. Our improvement against baselines [19] [43] starts to be more noticeable for rare subjects like shoes and robots. Occasionally, as shown in Fig. 20, with subjects that are rarer especially accompanied by text, the model is unable to fully capture its details.
C3. About User Study
We design 9 questions: 6 for recontextualization task (3 about subject fidelity, 3 about background-prompt fidelity) and 3 for texture editing task. Nine ratings per user and a total of 774 ratings were collected. A sample is shown below: