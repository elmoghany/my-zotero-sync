OminiControl2: Efficient Conditioning for Diffusion Transformers
Zhenxiong Tan* Qiaochu Xue* Xingyi Yang Songhua Liu Xinchao Wang National University of Singapore
{zhenxiong, e1352520, xyang, songhua.liu}@u.nus.edu xinchao@nus.edu.sg
Abstract
Fine-grained control of text-to-image diffusion transformer models (DiT) remains a critical challenge for practical deployment. While recent advances such as OminiControl [36] and others have enabled a controllable generation of diverse control signals, these methods face significant computational inefficiency when handling long conditional inputs. We present OminiControl2, an efficient framework that achieves efficient image-conditional image generation. OminiControl2 introduces two key innovations: (1) a dynamic compression strategy that streamlines conditional inputs by preserving only the most semantically relevant tokens during generation, and (2) a conditional feature reuse mechanism that computes condition token features only once and reuses them across denoising steps. These architectural improvements preserve the original framework’s parameter efficiency and multi-modal versatility while dramatically reducing computational costs. Our experiments demonstrate that OminiControl2 reduces conditional processing overhead by over 90% compared to its predecessor, achieving an overall 5.9× speedup in multi-conditional generation scenarios. This efficiency enables the practical implementation of complex, multi-modal control for highquality image synthesis with DiT models.
1. Introduction
Diffusion Transformer (DiT) models [26] have established state-of-the-art performance in image synthesis [5, 8, 15]. The impressive performance of these models underscores the need for control mechanisms tailored specifically to these architectures. Although controllable generation [17, 24, 39–41] has been extensively studied for UNet-based diffusion models [27, 30], diffusion transformer models require dedicated control mechanisms that accommodate their unique operational paradigms. Among many attempts, OminiControl [36] stands out as the first versatile control framework designed for DiT. In
*Equal Contribution
its core, OminiControl concatenates tokens derived from various control signals with tokens representing noisy images, forming a single long sequence. This unified sequence is then processed by multi-modality attention (MMAttention) [8, 38], which enables joint modeling of multimodal inputs. Crucially, OminiControl avoids architectural complexity by reusing the diffusion model’s pretrained VAE to encode conditional images directly into the latent space. This approach eliminates the need for auxiliary modules, significantly reducing the total parameter count thus reduced the total paramter. These design choices enable OminiControl to be easily extended to support diverse control types with minimal overhead. Despite its advantages, this unified sequence design in OminiControl faces a significant challenge: computational complexity. Processing long token sequences becomes especially problematic when the conditional sequence is extensive or when multiple conditions (such as text, image, inpainting cues, etc.) are applied simultaneously. Since the attention mechanism scales quadratically with sequence length, handling multiple or lengthy conditions quickly leads to prohibitive computational costs, limiting its practical use in complex, multi-conditional scenarios. To address these challenges of computational complexity, we propose OminiControl21, which builds on the strengths of OminiControl while specifically targeting the computational bottlenecks especially in multi-conditioned image generation tasks. Our approach is grounded in two core principles: reducing sequence length and minimizing redundant computations, leading to two key innovations.
• Compact token representation. We optimize computational efficiency by selectively retaining only the most informative condition tokens during generation. For tasks where only partial image regions require synthesis (e.g., localized edits), we restrict the diffusion process to the specific areas where new content is introduced. For fullimage generation tasks, we condition the synthesis of high-resolution outputs on downsampled conditional tokens. Both strategies significantly reduce the number of
1Code and more details are available at: https://github.com/ Yuanshi9815/OminiControl
arXiv:2503.08280v1 [cs.CV] 11 Mar 2025


tokens processed by the DiT during inference, streamlining computation without compromising quality.
• Conditional feature reuse. We compute conditional token features only once during the initial inference step and reuse them across subsequent steps. That is to say, only the generated image token feature are updated across different inference steps, eliminating redundant computation. We analyze the feasibility of this reuse scheme, identifying scenarios where fixed conditional features preserve generation fidelity while offering computational benefits—particularly critical in multi-conditional generation with complex input combinations. By integrating these advancements, OminiControl2 retains the parameter efficiency and universal control capabilities of OminiControl while dramatically improving computational efficiency. Our approach reduces conditional processing overhead by over 90% compared to the original OminiControl, enabling a 5.9× speedup in multiconditional scenarios while maintaining generation quality. This significant efficiency gain enables practical implementation of complex control tasks with substantially lower computational demands, making multi-modal control feasible even with limited computational resources. In summary, our contributions with OminiControl2 are as follows: • We introduce a compact image condition encoding strategy that significantly reduces token sequence length while retaining essential conditioning information. This directly addresses the substantial computational overhead caused by long condition sequences in multi-conditional DiT models. • We implement a condition feature reuse mechanism that computes the conditional embedding once and caches it across denoising steps. This minimizes redundant computations. • We show that OminiControl2 preserves the control versatility and parameter efficiency of its predecessor while reducing condition overhead by over 90%.
2. Related Works
Diffusion models. Diffusion models have achieved remarkable success in text-to-image generation tasks [27, 30]. To enable more fine-grained control beyond text prompts, several works have incorporated image control signals into UNet-based diffusion models. These approaches primarily follow two paradigms: (1) direct feature addition, where condition features are spatially aligned and added to the hidden states of the denoising network [4, 24, 29, 40]; and (2) cross-attention mechanisms, where separate encoders extract condition features that are then integrated via attention operations [11, 37, 39, 41]. However, these methods are specifically designed for UNet architectures and cannot be directly applied to Diffusion Transformer (DiT) models
due to fundamental differences in architectural design and operational paradigms.
Controllable diffusion transformers. Recent works such as OminiControl [36], DSD [3], and others [4, 21, 32] have explored controllable generation in DiT models. These approaches elegantly leverage the existing Multi-Modal Attention (MM-Attention) mechanism [25] within DiTs to incorporate image conditions without requiring complex architectural modifications. However, they face a significant limitation: as the number of condition tokens increases (particularly in multi-conditional scenarios), the computational cost grows significantly due to the self-attention operations over the entire token sequence, making them inefficient for practical applications with multiple or high-resolution condition inputs.
Acceleration techniques for diffusion transformers. The iterative nature of diffusion models incurs substantial computational costs. Recent works have focused on enhancing DiT’s efficiency through various techniques: Model Pruning approaches dynamically skip or remove components of the model. DiP-GO [42] predicts whether to skip computational blocks during inference, while TinyFusion [9] removes redundant layers from diffusion transformers. Computation Caching stores intermediate results to reduce redundancy. FORA [31] reuses intermediate outputs from attention and MLP layers across denoising steps. TokenCache [18] and related methods [6, 19, 33, 43, 44] achieve efficiency by strategically caching less important tokens. These works provide valuable insights for our work, which focuses specifically on reducing computational overhead in multi-conditional DiT models while maintaining control versatility and generation quality.
3. Methods
3.1. Preliminaries
Diffusion transformer. The text-guided Diffusion Transformer (DiT) models [5, 8, 15, 26] generates high-quality images through an iterative denoising process. In the model of FLUX [15], at each denoising step, transformer blocks process a token sequence
S = [X; CT ] (1)
This sequence consists of noisy tokens X ∈ R∗×d and text condition tokens CT ∈ R∗×d, where d is the embedding dimension, and ∗ denotes the number of tokens.
Unified token sequence. The previous work [36] extends DiT’s input sequence by introducing image condition tokens. For an input condition image, it is first encoded into


latent tokens CI ∈ R∗×d using the VAE encoder. The extended token sequence becomes:
S = [X; CT ; CI ] (2)
This unified token framework enables flexible interactions between image and condition tokens through transformer layers, supporting various image generation tasks.
Position index in controllable DiT. For each token in the sequence, there is a corresponding position index P ∈ R2, which is involved in attention computation to capture spatial dependencies [15, 34]. Specifically, the position index for noisy tokens is determined by the spatial position (i, j) in the 2D image grid, while the position index for text condition tokens is set to a fixed value (0, 0). For image condition tokens, the position index varies depending on the condition task type. For spatially aligned imgae condition tasks (e.g., edge-guided, depth-guided generation), position indices share the same spatial positions (i, j) as the corresponding noisy tokens. For spatially nonaligned tasks (e.g., subject-driven generation), position indices are shifted by a fixed offset (∆i, ∆j). This adaptive position indexing strategy [36] enables the model to effectively handle the spatial relationships between different condition types.
Extension to multi-image condition tasks. To extend this controllable DiT formulation for multi-image condition tasks, we can simply concatenate multiple image condition tokens C(k)
I ∈ R∗×d as follows:
CI = [C(1)
I ; C(2)
I ; ...; C(K)
I ] (3)
where K is the number of condition images. While this approach is straightforward and easily implemented, it leads to a large number of condition tokens and increase the computational cost of the model.
3.2. Compact token representation
To address the issue of increasing computational cost in multi-image condition tasks, we first try to reduce the number of condition tokens. Specifically, we propose a compact encoding method based on compression and pruning.
Compression with position correcting. We implement a spatial compression strategy for condition images prior to VAE encoding, reducing spatial dimensions by a factor of a in each direction. This transformation achieves a a2 : 1 reduction in token count—converting an n × n token field into a more efficient n
a×n
a representation (see Figure 1). However, na ̈ıve compression creates spatial misalignment between the compressed condition space and the target generation space. For spatially-aligned tasks, this misalignment leads to structural inconsistencies and degraded
Noisy image token Condition token
Compress Correct position Prune tokens
Figure 1. Illustration of the compression and position correcting of compact token representation for condition images.
N tokens N tokens N tokens
Noisy Image Condition Image Latent Mask Combined
Figure 2. Illustration of token integration processing for inpainting. By combining noisy and condition tokens based on the mask, we reduce token count from 2N to N.
control precision, as compressed condition tokens no longer maintain direct correspondence with noisy image tokens. To preserve spatial coherence, we introduce an position correcting function that establishes correspondence between the compressed condition image tokens and their target regions in the generated image:
PCI (i, j) 7→ PX (a · i, a · j) (4)
This mapping is crucial and ensures each compressed token provides guidance to its appropriate region in the generated image, maintaining conditioning fidelity while substantially reducing computational requirements. Without this correction, the model would struggle to learn the spatial relationships between condition and generated tokens, leading to poor quality results (Section 4.3).
Token pruning. We also implement a token pruning strategy to eliminate non-informative tokens from condition images. This approach identifies and removes tokens that contribute minimally to the conditioning signal, particularly effective for sparse conditional inputs. Token relevance is determined by condition-specific criteria. For example, in edge-guided generation, tokens representing regions without edges (values near zero) are pruned as they provide little guidance information. The position information of retained tokens remains unchanged, preserving spatial correspondence after pruning.
Token integration. For inpainting tasks, we propose a novel token integration methodology that incorporates con


Steps
0 50
50
0 0.65
1.0
Steps
0 50
Steps
"vase"
Conditons
Output
Similarity of Similarity of
Figure 3. Feature similarity across denoising steps. While noisy image tokens X change significantly between steps (left), condition tokens CI maintain high similarity throughout the denoising process (right).
dition tokens as intrinsic components of the output token representation (Figure 2). Instead of maintaining discrete token sequences for condition images, we strategically integrate condition tokens CM=0
I (corresponding to unmasked regions) as invariant elements within the output token space. This approach preserves the computational processing of these tokens through transformer layers at each denoising step, while conceptually designating them as output tokens exempt from modification. By selectively constraining the denoising process to operate exclusively on tokens within the masked region XM=1, while simultaneously preserving unmasked areas as static contextual information, we maintain the critical spatial relationships necessary for highfidelity synthesis. The resultant output constitutes a coherent integration of newly synthesized content within masked regions alongside preserved original content in unmasked areas. This methodology ensures complete fidelity preservation in the VAE’s latent representation while optimizing computational efficiency in the processing pipeline.
3.3. Feature reuse in DiT
Another key factor contributing to the computational complexity of DiT models is the repetitive processing of condition tokens across multiple denoising steps. In the standard diffusion process, at each denoising timestep t, the DiT model processes a token sequence that includes both the current noisy latent tokens and various condition tokens [15, 26]. However, we observe a fundamental asymmetry in how these elements evolve during inference: while the noisy latent representation changes progressively as noise is removed across timesteps, the image condition inputs remain unchanged throughout the entire sampling process [36]. This observation suggests an opportunity for significant computational optimization—what if we could compute the intermediate features for condition tokens just once and reuse them across all denoising steps? To validate this, we conduct an empirical analysis by measuring the cosine similarity of token features across dif
Original Naïve feature reuse
This item drives across the moon...
Figure 4. Visual comparison of the original inference pipeline (left) and naive feature reuse strategy (right).
ferent timesteps. As shown in Figure 3, while noisy image tokens X exhibit substantial feature variation between timesteps, condition tokens CI maintain remarkably high similarity throughout the diffusion process. This high similarity indicates significant computational redundancy when recalculating condition features at each step.
Na ̈ıve cache strategy. Inspired by caching techniques in large language models (LLMs) [28] and DiT [6, 20, 31, 35], we initially attempted an approach where condition features are computed only once during the first inference step and then reused across subsequent steps. Since the interaction between condition tokens C and noisy tokens X occurs exclusively within the MM-attention modules [25, 36], we specifically reuse the key-value (KV ) projections of condition tokens (see Figure 5(a, b)). However, despite the high similarity observed in Figure 3, this approach produced unsatisfactory results as shown in Figure 5. This performance gap reveals that while condition features remain largely consistent, they still undergo subtle dynamic changes during the denoising process. In our naive implementation, condition features remain entirely static, creating a training-inference discrepancy that degrades output quality.
Asymmetric attention masking. To resolve this discrepancy, we need to ensure that condition token features remain consistent across denoising steps. Unlike LLMs, which can effectively implement KV-caching due to their causal attention mechanism (where subsequent tokens cannot influence preceding ones), DiT employs full multi-modal attention where changes in noisy tokens X affect condition tokens C and vice versa [8, 36]. We address this by introducing an asymmetric attention mechanism for DiT, as illustrated in Figure 5(c). In this modified approach, noisy image tokens X retain their original attention pattern, attending to both image and condition tokens [X; C]. However, we apply attention masking to prevent condition tokens C from attending to noisy image tokens X. This asymmetric setup, incorporated during model training, ensures that while image tokens benefit


Activated
Masked
Denoise step full computed
Denoise step with feature reuse
Condition token
Noisy image token
Dynamic features
Static features
Reuse
(a) Denoise pipeline (b) Feature reuse (c) Attention mask
Original
New
Figure 5. Illustration of our feature reuse strategy. (a) Overview of the denoising pipeline, with full computation performed only at the first step. (b) Detailed view of the feature reuse mechanism in Attention, where condition token features(KV ) computed in the first step are reused in subsequent steps. (c) Asymmetric attention mask that prevents condition tokens from attending to noisy image tokens, enabling consistent feature reuse.
from conditioning information, the condition token features remain independent of the evolving noisy image representation. This alignment between training and inference enables effective feature reuse, allowing us to compute condition features just once while maintaining generation quality.
3.4. Computational complexity analysis
To quantify the efficiency gains of our approach, we analyze the computational complexity of controllable DiT models and identify key optimization opportunities. For each image generation process with the original method, the time complexity can be formulated as:
O(n · (d2N + N 2d))
where n is the number of denoising steps, N = |X| + |C| is the total token count (image and condition tokens combined), and d is the feature dimension. We can simplify this expression to:
O(n · (c1N + c2N 2))
where c1N represents token-independent operations (projections, MLPs, normalization) and c2N 2 represents the attention computation between tokens. Our empirical measurements in Figure 6 reveal that for practical use cases with 1-4 condition sources, the computational overhead is dominated by token-independent operations rather than the quadratic attention computations. Under this, the computational overhead introduced by condition tokens can be approximated as O(n · |C|), where |C|
0 10 20 30 40 Number of tokens (k)
0
20
40
60
80
Time (s)
0 Conditions
1 Conditions
2 Conditions
3 Conditions
4 Conditions
8 Conditions
Computation time by number of conditions
Token-independen Ops Attention Ops O(n) O(n2)
Figure 6. Computational time breakdown analysis. For typical scenarios (1-4 conditions), token-independent operations (blue circles) dominate the computational cost, while attention operations (red triangles) become significant only with higher condition counts (8). X-axis shows token count (|X| + |C|) in thousands.
is the number of condition tokens. Our proposed methods address this overhead from two complementary directions: Compact token representation reduces condition token count from |C| to r ·|C| (where r < 1), providing a speedup factor for condition-related overhead of:
αcompact = 1
r
Conditional token reuse computes condition token features once instead of n times, yielding a speedup factor for condition processing of:
αreuse = n
When combined, these approaches deliver a cumulative theoretical speedup for condition-related computational overhead of:
αtotal = n
r
For a common configuration with n = 28 and r = 0.25, this yields a theoretical speedup of αtotal = 112 for condition token processing. For a step-distilled model with n = 4 and r = 0.25, the speedup factor is αtotal = 16.
4. Experiments
4.1. Setup
Base model and training. We adopt FLUX.1 [15] as our base model. Following OminiControl [36], we fine-tune using LoRA [7] with identical rank and parameter settings. Training employs the Prodigy optimizer [22] for 25k iterations per task, with batch size 1 and gradient accumulation over 4 steps. All experiments are conducted on a single NVIDIA H100 GPU. The training dataset is Text-to-Image2M [10] following the same setup as OminiControl [36].


Tasks. We evaluate our methods on four conditional generation tasks: Canny-to-image, depth-to-image, mask inpainting, and image deblurring. Additionally, we assess performance in a multi-condition setting where all four condition types are simultaneously applied.
Implementation Details. For our compact token representation, task-specific strategies are implemented: mask inpainting adopts the approach of token integration (Section 3.2), while other tasks use spatial compression with r = 0.25, reducing token dimensions by 50% in each direction. Additionally, the non-informative tokens in canny edge maps are pruned. Baselines. We first compare against the original OminiControl [36] as our primary baseline. We also compare against the ControlNet [40] which is implemented on StableDiffusion 1.5 [30] and ControlNetPro [16] which is implemented on FLUX.1. For OminiControl [36], we trained the model with the same training data and settings as our method. Additionally, we evaluate against two efficiencyfocused methods for DiT models: token-merging techniques [1, 2] 2 that reduce computation by strategically combining similar tokens, and naive feature caching that introduced in Section 3.3. Evaluation. Efficiency metrics include inference latency and condition overhead, measured on an single NVIDIA RTX 6000 Ada GPU. Generation quality is evaluated using FID [13], CLIP [12], NIQE [23], and MUSIQ [14]. We use total 5,000 images from COCO 2017 validation set, and resize them to 512×512, then generate images with taskspecific conditions and associated captions as prompts with a fixed seed of 42.
4.2. Main results
Efficiency. Figure 9 illustrates computation times across different configurations. The experimental results show that our combined approach offers speedups ranging from 3.8× to 5.9× compared to the original OminiControl implementation. We observe that each optimization strategy contributes differently depending on the configuration: compact token representation tends to be more effective in low-step settings (FLUX schnell with 4 denoising steps), while token reuse shows greater efficiency gains with more steps (FLUX dev with 28 denoising steps). Notably, the measurements indicate that our combined method maintains relatively consistent computation time as the number of conditions increases, whereas the baseline method exhibits a more linear growth pattern. This efficiency characteristic becomes particularly valuable at higher resolutions (1024×1024), potentially making multi-conditional generation more practical in computationally intensive scenarios.
2Due to constraints imposed by positional embeddings in FLUX [15], token-merging approaches can only be applied to MLP and feedforward components while preserving the original attention mechanisms.
1234
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
Resolution: (512 x 512)
Computation time (s)
3.8×
Steps = 4 (FLUX schnell)
1234
10
15
20
25
30
4.0×
Steps = 28 (FLUX dev)
1234 Number of conditions
5
10
15
20
Resolution: (1024 x 1024)
Computation time (s)
5.6×
1234 Number of conditions
20
40
60
80
100
120
140
160
5.9×
Methods Original Compact token Token reuse T.R. + C.T.
Figure 7. Computation time comparison across different resolution and sampling step configurations. Our combined approach (T.R. + C.T.) achieves 3.8-5.9× speedup over the original method, with greater gains at higher resolutions and more conditions. Both individual optimizations show significant improvements, but their combination delivers maximum efficiency. (Compact token representation reduces token count by approximately 75%.)
Qualitative comparison. Figure 8 compares generation quality and inference time across different optimization approaches with varying numbers of conditions. Both our compact token representation and feature reuse methods produce high-quality images while significantly reducing generation time. Notably, even as the number of conditions increases from 1 to 4, our methods maintain consistent image quality while achieving substantial speedups (up to 3.85× with the combined approach). In contrast, baseline methods such as na ̈ıve cache and token merging show clear degradation in image quality when handling multiple conditions, particularly with 4 conditions where visual artifacts become apparent. We observe that token-merge approaches face challenges in the FLUX architecture, as they require additional computational overhead for token grouping operations, and their effectiveness is constrained by the position embedding mechanism. This may explain why tokenmerge methods, despite compromising on image quality, do not deliver the performance improvements that might be expected in this particular context.


Figure 8. Performance comparison of different methods with varying condition counts for 512×512 image generation. Our approach maintains consistent quality with increased conditions while achieving the fastest inference speeds (up to 3.85× speedup), whereas naive caching and token merging show deteriorating performance as condition count increases.
Task Method FID ↓ CLIP-Text ↑ CLIP-Image ↑ NIQE ↓ MUSIQ ↑ Latency↓ Condition
overhead (s)↓
Depth
OminiControl 26.879 0.308 0.745 4.247 73.498 13.30 6.05 / 9.9% Naı ̈ve cache 28.885 0.306 0.743 4.597 73.349 8.16 0.91 / 15.0% Ours 28.719 0.308 0.731 4.296 73.669 7.85 0.60 / 9.9% Ours / F.R. only 28.719 0.308 0.731 4.296 73.669 8.18 0.93 / 15.3% Ours / C.T.R only 27.730 0.308 0.741 4.244 72.470 9.38 2.13 / 35.2%
Inpainting
OminiControl 12.078 0.304 0.884 3.781 71.889 13.30 6.05 / 100% Naı ̈ve cache 16.727 0.296 0.854 4.204 64.955 8.16 0.91 / 15.0% Ours 11.712 0.307 0.891 3.878 71.045 7.85 0.60 / 9.9% Ours / F.R. only 12.672 0.305 0.876 3.741 72.732 8.18 0.93 / 15.3%
Deblur
OminiControl 16.381 0.303 0.851 4.060 71.796 13.30 6.05 / 100% Naı ̈ve cache 45.039 0.285 0.773 5.572 55.700 8.16 0.91 / 15.0% Ours 20.775 0.302 0.809 4.856 69.474 7.85 0.60 / 9.9% Ours / F.R. only 18.638 0.303 0.836 4.014 71.863 8.18 0.93 / 15.3% Ours / C.T.R only 19.664 0.303 0.831 4.204 72.260 9.38 2.13 / 35.2%
Canny
OminiControl 22.836 0.307 0.780 4.002 74.570 13.30 6.05 / 9.9% Naı ̈ve cache 31.497 0.302 0.748 5.198 75.128 8.16 0.91 / 15.0% Ours 29.766 0.308 0.724 4.069 73.316 7.85 0.60 / 9.9% Ours / F.R. only 22.274 0.306 0.758 4.150 74.483 8.18 0.93 / 15.3% Ours / C.T.R only 26.654 0.309 0.742 3.979 74.094 9.38 2.13 / 35.2%
Multi-condition
OminiControl 7.296 0.301 0.924 3.878 70.961 31.63 24.38 / 100% Naı ̈ve cache 19.965 0.292 0.876 4.236 63.984 9.66 2.41 / 9.8% Ours 10.718 0.303 0.903 3.872 71.273 8.18 0.93 / 3.8% Ours / F.R. only 8.492 0.302 0.920 3.846 70.099 9.78 2.53 / 10.4% Ours / C.T.R only 8.409 0.302 0.919 3.846 71.024 13.32 6.07 / 24.9%
Table 1. Comparison of efficiency and generation quality across various conditioning tasks. Bold indicates best results between compared methods (blue-highlighted rows). White rows show ablation studies: F.R. = Feature Reuse only, C.T.R. = Compact Token Representation only. The rightmost column shows condition overhead in seconds / percentage relative to original method (OminiControl).


Task Method FID ↓ CLIP-Text ↑ CLIP-Image ↑ NIQE ↓ MUSIQ ↑ Parameters
overhead ↓ Condition
overhead (s)↓
Depth
SD 1.5 + ControlNet 23.029 0.308 0.7264 4.5802 70.73 361M 0.52 FLUX + ControlNet 62.203 0.212 0.5477 4.0151 66.849 3,300M 1.16 Ours 28.719 0.308 0.731 4.296 73.669 14.5M 0.60
Canny
SD 1.5 + ControlNet 18.74 0.305 0.752 4.751 67.899 361M 0.52 FLUX + ControlNet 98.689 0.192 0.537 4.636 56.907 3,300M 1.16 Ours 29.766 0.308 0.724 4.069 73.316 14.5M 0.60
Table 2. Quantitative comparison of different control methods across various conditioning tasks.
Quantitative evaluation. Table 2 presents quantitative results across various conditioning tasks. OminiControl2 better preserves generation quality compared to the baseline while significantly reducing computational overhead. For multi-condition tasks, our method reduces conditional overhead by 96.2% (from 24.38s to 0.93s) while achieving competitive FID scores. The na ̈ıve caching approach shows efficiency gains but suffers from quality degradation, particularly in deblur tasks. These results confirm that OminiControl2 successfully addresses computational bottlenecks while better preserving generation quality across diverse conditioning tasks.
Comparison with other control methods. As shown in Table 2, OminiControl2 maintains the superior generation quality of its predecessor while significantly reducing computational overhead. These results confirm that our method successfully preserves the effectiveness of the original OminiControl framework while achieving computational efficiency comparable to traditional control methods like ControlNet [40]. Furthermore, our approach inherits the parameter efficiency advantage of OminiControl, requiring only 14.5M additional parameters, making it both computationally efficient and parameter efficient for conditional generation tasks.
4.3. Empirical studies
Position correcting for compressed tokens. To evaluate the effectiveness of the proposed position correcting mechanism for compressed condition image, we compared image generation with and without this technique. Figure 9 shows that models trained with position correcting produce significantly better outputs. Without position correction, the model struggles to establish proper spatial correspondence between compressed condition tokens and the generated image, resulting in structural inconsistencies.
Ablation studies. Our ablation studies (white rows in Table 1) reveal the individual contributions of our two components. Feature Reuse (F.R.) alone offers substantial speedup
"vase"
100 iter. 5k iter. 10k iter. 15k iter.
compression only
with position correcting
Figure 9. Comparison of image generation results with and without position correcting during training.
(84.7% overhead reduction) with minimal quality degradation across most tasks. Compact Token Representation (C.T.R.) alone provides a balanced tradeoff, reducing overhead by approximately 65% while maintaining competitive metrics, notably achieving better MUSIQ scores for deblurring (72.26 vs. 69.47) than our complete model. For inpainting, our full method reduces FID from 12.07 to 11.71 while cutting overhead by 90.1%. These results demonstrate that both techniques are complementary, with their combination achieving the optimal balance between quality and efficiency.
5. Conclusion
We presented OminiControl2, an efficient framework for controllable image generation with Diffusion Transformers. By introducing compact token representation and conditional feature reuse, our approach achieves speedups of up to 5.9× across various control tasks. These optimizations effectively address the computational bottlenecks in multiconditional generation, enabling practical deployment of complex control scenarios. Our work demonstrates that computational efficiency can be improved simultaneously by eliminating redundancies in the diffusion process, making powerful controllable generation more accessible for real-world applications.


6. Acknowledgment
We would like to acknowledge that the computational work involved in this research work is partially supported by NUS IT’s Research Computing group using grant numbers NUSREC-HPC-00001.
References
[1] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4599–4603, 2023. 6 [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. 6
[3] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion self-distillation for zero-shot customized image generation. arXiv preprint arXiv:2411.18616, 2024. 2
[4] Ke Cao, Jing Wang, Ao Ma, Jiasong Feng, Zhanjie Zhang, Xuanhua He, Shanyuan Liu, Bo Cheng, Dawei Leng, Yuhui Yin, et al. Relactrl: Relevance-guided efficient control for diffusion transformers. arXiv preprint arXiv:2502.14377, 2025. 2 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1, 2
[6] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: A training-free acceleration method tailored for diffusion transformers, 2024. 2, 4 [7] Shilpa Devalal and A Karthikeyan. Lora technology-an overview. In 2018 second international conference on electronics, communication and aerospace technology (ICECA), pages 284–290. IEEE, 2018. 5 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mu ̈ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 2, 4 [9] Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang. Tinyfusion: Diffusion transformers learned shallow, 2024. 2 [10] Jacky Hate. Text-to-image-2m dataset. https : / / huggingface . co / datasets / jackyhate / text to-image-2M, 2024. 5
[11] Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, and Liefeng Bo. Anystory: Towards unified single and multiple subject personalization in text-to-image generation. arXiv preprint arXiv:2501.09503, 2025. 2
[12] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 6
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [14] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5148–5157, 2021. 6 [15] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. Accessed: 2024-11-12. 1, 2, 3, 4, 5, 6 [16] Shakker Labs. Flux.1-dev-controlnet-union-pro. https: / / huggingface . co / Shakker - Labs / FLUX . 1 dev-ControlNet-Union-Pro, 2024. 6
[17] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback. In European Conference on Computer Vision, pages 129–147. Springer, 2025. 1 [18] Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. Token Caching for Diffusion Transformer Acceleration, 2024. 2 [19] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching, 2024. arXiv:2406.01733 [cs]. 2 [20] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. Advances in Neural Information Processing Systems, 37:133282–133304, 2025. 4 [21] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 2
[22] Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free learner. In Forty-first International Conference on Machine Learning, 2024. 5
[23] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a “completely blind” image quality analyzer. IEEE Signal processing letters, 20(3):209–212, 2012. 6
[24] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 4296–4304, 2024. 1, 2 [25] Zexu Pan, Zhaojie Luo, Jichen Yang, and Haizhou Li. Multimodal attention for speech emotion recognition. arXiv preprint arXiv:2009.04107, 2020. 2, 4
[26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023. 1, 2, 4 [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu ̈ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 2


[28] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5: 606–624, 2022. 4 [29] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. 2
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo ̈rn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 1, 2, 6 [31] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. FORA: Fast-Forward Caching in Diffusion Transformer Acceleration, 2024. arXiv:2407.01425 [cs]. 2, 4 [32] D She, Mushui Liu, Jingxuan Pang, Jin Wang, Zhen Yang, Wanggui He, Guanghao Zhang, Yi Wang, Qihan Huang, Haobin Tang, et al. Customvideox: 3d reference attention driven dynamic adaptation for zero-shot customized video diffusion transformers. arXiv preprint arXiv:2502.06527, 2025. 2 [33] Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, et al. Lazydit: Lazy learning for the acceleration of diffusion transformers. arXiv preprint arXiv:2412.12444, 2024. 2
[34] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [35] Wenzhang Sun, Qirui Hou, Donglin Di, Jiahui Yang, Yongjia Ma, and Jianxun Cui. UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation, 2025. 4 [36] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. 1, 2, 3, 4, 5, 6 [37] Xueyun Tian, Wei Li, Bingbing Xu, Yige Yuan, Yuanzhuo Wang, and Huawei Shen. Mige: A unified framework for multimodal instruction-based image generation and editing. arXiv preprint arXiv:2502.21291, 2025. 2
[38] Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng Wu. Multi-modality cross attention network for image and sentence matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10941–10950, 2020. 1 [39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1, 2 [40] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847, 2023. 2, 6, 8 [41] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al.
Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8069–8078, 2024. 1, 2 [42] Haowei Zhu, Dehua Tang, Ji Liu, Mingjie Lu, Jintu Zheng, Jinzhang Peng, Dong Li, Yu Wang, Fan Jiang, Lu Tian, Spandan Tiwari, Ashish Sirasao, Jun-Hai Yong, Bin Wang, and Emad Barsoum. Dip-go: A diffusion pruner via few-step gradient optimization, 2024. 2 [43] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with tokenwise feature caching. arXiv preprint arXiv:2410.05317, 2024. 2 [44] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating Diffusion Transformers with Token-wise Feature Caching, 2025. arXiv:2410.05317 [cs]. 2