Skip to main content
Computer Science > Computer Vision and Pattern Recognition
arXiv:2505.04512 (cs)
[Submitted on 7 May 2025 (v1), last revised 8 May 2025 (this version, v2)]
HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation
Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, Qinglin Lu
View PDF
HTML (experimental)
Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at this https URL.
Subjects:	Computer Vision and Pattern Recognition (cs.CV)
Cite as:	arXiv:2505.04512 [cs.CV]
 	(or arXiv:2505.04512v2 [cs.CV] for this version)
 	
https://doi.org/10.48550/arXiv.2505.04512
Focus to learn more
Submission history
From: Teng Hu [view email]
[v1] Wed, 7 May 2025 15:33:18 UTC (45,083 KB)
[v2] Thu, 8 May 2025 08:29:00 UTC (43,135 KB)

Access Paper:
View PDFHTML (experimental)TeX SourceOther Formats
view license
Current browse context: cs.CV
< prev next >

newrecent2025-05
Change to browse by: cs
References & Citations
NASA ADS
Google Scholar
Semantic Scholar
Export BibTeX Citation
Bookmark
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer (What is the Explorer?)
Connected Papers Toggle
Connected Papers (What is Connected Papers?)
Litmaps Toggle
Litmaps (What is Litmaps?)
scite.ai Toggle
scite Smart Citations (What are Smart Citations?)
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)
About
Help
Contact
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance

arXiv Operational Status 
Get status notifications via email or slack