Video Colorization with Pre-trained Text-to-Image Diffusion Models
Hanyuan Liu1 Minshan Xie1 Jinbo Xing1 Chengze Li2 Tien-Tsin Wong1* 1 The Chinese University of Hong Kong 2 Caritas Institute of Higher Education
{liuhy, msxie, jbxing, ttwong}@cse.cuhk.edu.hk czli@cihe.edu.hk
”koala bear in the winter season” ”koala bear in the autumn season”
”yellow skirt” ”vivid street view”
”sunsetting” ”two men playing chess in front of the sea”
Figure 1. Our framework can generate high quality video colorization results based on textual descriptions.
Abstract
Video colorization is a challenging task that involves inferring plausible and temporally consistent colors for grayscale frames. In this paper, we present ColorDiffuser, an adaptation of a pre-trained text-to-image latent diffusion model for video colorization. With the proposed adapterbased approach, we repropose the pre-trained text-to-image model to accept input grayscale video frames, with the optional text description, for video colorization. To enhance the temporal coherence and maintain the vividness of colorization across frames, we propose two novel techniques: the Color Propagation Attention and Alternated Sampling Strategy. Color Propagation Attention enables the model to refine its colorization decision based on a reference latent frame, while Alternated Sampling Strategy captures spatiotemporal dependencies by using the next and previous adjacent latent frames alternatively as reference during the generative diffusion sampling steps. This encourages bidirectional color information propagation between adjacent video frames, leading to improved color consistency across
frames. We conduct extensive experiments on benchmark datasets, and the results demonstrate the effectiveness of our proposed framework. The evaluations show that ColorDiffuser achieves state-of-the-art performance in video colorization, surpassing existing methods in terms of color fidelity, temporal consistency, and visual quality.
1. Introduction
Colorization of grayscale images and videos enables the transformation of historical contents, enhancing visual aesthetics, and aiding in the delivery of the video contents. Various approaches have been proposed for this purpose, ranging from hand-crafted techniques [13, 40, 52, 78] to datadriven deep learning methods [13, 20, 77, 83], and recent advances in generative models [34, 73]. While the existing methods manage to output high-quality color propagation on still images, video colorization remains challenging due to the simultaneous fulfillment over multiple visual aspects including temporal coherence, spatial and se
1
arXiv:2306.01732v1 [cs.CV] 2 Jun 2023


mantics consistency, as well as color richness and faithfulness over the contents (especially on colorizing artificial content). Naturally, training over large-scale dataset can achieve high-quality colorization, but the computational resource required is prohibitively large. In this paper, instead of relying on large-scale data training solely for colorization purposes, we re-propose a pretrained text-to-image diffusion model for our video colorization purpose. The text-to-image diffusion framework, originally designed for generating realistic images from textual descriptions, presents a promising potential for video colorization due to its ability to incorporate high-level semantic cues and the rich color variety learned from its largescale dataset. To exploit the color information within such a pre-trained text-to-image model, we build an adapterbased model to redirect the pre-trained text-to-image diffusion model to generate color image that matches our input grayscale frame in terms of luminance and structure. In this way, the redirected text-to-image model not just synthesizes the colors, but also provides a diverse color variety due to its generative nature. Note that the pre-trained text-to-image model only generates static images, and hence, it does not guarantee temporal coherence across all video frames. In this paper, instead of relying on the unstable optical flow, typically used in existing methods, we propose a novel color propagation attention module to consistently colorize video frames over time. Such colorization is not a post-processing step, but a step during the color inference.
To evaluate the effectiveness of our proposed approach, we conduct extensive experiments on benchmark video colorization datasets and compare our results against several baseline methods. The experimental analysis demonstrates the superiority of our adapted text-to-image diffusion model in terms of color accuracy, temporal coherence, and controllability. Our contributions can be summarized as follows: • We propose a novel video colorization method, ColorDiffuser, that can colorize monochrome videos with diverse and consistent colors through multiple frames based on optional textual descriptions. • We propose the color propagation attention and alternated sampling strategy, enabling video colorization with a conditional text-to-image diffusion model.
2. Related Work
2.1. Image Colorization
Semi-Automatic Colorization. Early colorization methods focus on using local user hints, such as user scribbles [40], and global hints, such as color palette [7] or text [72], to colorize images. These color hints are then propagated to the entire image and optimized based on hand-crafted low-level features [10,13,40,52,78]. Recently,
Zhu et al. [83] proposed a deep-learning-based method to propagate the sparse color hints by incorporating semantic information, and achieve real-time performance and remarkable quality. But, these methods usually require intensive manual annotations to generate plausible colorful images, which poses challenges to users. Another category of work colorizes the grayscale image by transferring color information from a reference image with similar content. These methods transfer the color information to corresponding regions by matching low-level hand-crafted features [6, 9, 44, 62, 70]. But, these correspondence are not robust to objects with complex appearance as they do not capture high-level semantic information. To establish semantic correspondences, some researchers [19, 20, 77] made use of deep semantic features extracted by a pre-trained neural network and generate plausible results. However, these methods may fail to provide robust and automatic colorization when content-related images are not available for reference. Instead of relying on user-provided references, our method exploits the color priors within pre-trained T2I model trained on large-scale datasets to generate diverse and vivid results. Furthermore, our method also accepts textual descriptions, a more convenient global hint, for generating controllable results.
Fully Automatic Colorization. With the advances of deep learning techniques, several automatic colorization methods attempted to learn the grayscale-to-color mapping from large-scale datasets [8,11,26,81]. These methods predict the color by considering both low and high-level semantics to achieve compelling colorized results [25, 37, 38, 61, 84]. But, these methods lack the modeling of color ambiguity (especially on coloring artificial objects) and thus cannot generate diverse results. To alleviate this obstacle, diverse colorization methods have been proposed using generative models [11, 26, 66] and transformer models [29, 34, 71]. Some follow-up works achieve controllable and diverse colorization by predicting sparse guidance, such as color palette [69], local color hints [75]. However, these automatic methods are prone to produce visual artifacts like unnatural and incoherent colors when colorizing objects with color uncertainty. Recently, some approaches [33, 48] attempt to achieve diverse colorization by exploiting generative priors of pretrained GANs [31,32]. Wu et al. [74] utilize GAN inversion techniques [86, 87] to obtain a content-related image as a reference and then warp the color features into the grayscale image. However, the colorized results highly depend on the quality of images generated by GAN inversion. In contrast, we build a adapter-based model to redirect the powerful T2I diffusion model to generate diverse colorized results. In addition, it also supports conditional video colorization using different textual descriptions to specify the desired color
2


composition and object colors.
2.2. Video Colorization
Video colorization considers temporal constraints on top of image colorization. Existing video colorization can be classified into three categories. The first is to perform post-processing to impose the temporal coherence in order to suppress the flickers of per-frame colorization with a general temporal filter [4, 35], but these works tend to wash out the colors. Another category on video colorization is mainly exemplar-guided, including propagating the user scribbles [40, 78], attaching the colors from colorized frames [28], or given images [53] to the rest of frames. These approaches rely on the optical flow to propagate colors in videos either from scribbles or fully colored frames [40, 78]. However, inaccuracies in optical flow may lead to color artifacts with the accumulated errors over time. Some other work uses one colored frame as an example and colorizes the following frames in sequence. While conventional methods rely on hand-crafted low-level features to find the temporal correspondence [3, 27, 76], deep learning methods further improve colorization quality by taking advantage of semantics to learn the temporal propagation [28, 43, 68]. Zhang et al. [79] colorized frames by considering both the matched features of the reference image and the propagated color from previous frames. However, the color propagation of these methods will be problematic when the scene disparity of examples and grayscale frames cannot be ignored. In contrast, our method does not require user to provide any reference, instead it exploits the color information hidden inside the pretrained T2I model trained on the large-scale data set. Our generative nature allows use to generate diverse colorization results for content without definitive colors. Conditional video colorization can also be achieved by feeding texts. The temporal coherence is achieved in our method via an innovative frame-alternating cross attention approach.
2.3. Diffusion Models
Diffusion Models [59] are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable. This denoising process or the generation process corresponds to learning the reverse process of a fixed Markov Chain of length T [12, 22, 56]. For image synthesis, the most successful models rely on a reweighted variant of the variational lower bound on p(x) based on denoising score-matching [60]. These models can be interpreted as an equally weighted sequence of denoising autoencoders εθ(xt, t); t = 1 . . . T , which are trained to predict a denoised variant of their input xt, where xt is a noisy version of the input x. Instead of directly learning the target data distribution in the high-dimensional pixel space, Latent Diffusion Models [54] leverage perceptual
compression with an autoencoder E and D for efficient low-dimensional representation features. The model learns p(z|y), in which E(x) = z, E(D(x)) ≈ x for reconstruction:
LLDM = EE(x),y,ε∼N (0,1),t
h
∥ε − εθ(zt, t, y)∥2
2
i
(1)
with t uniformly sampled from {1, . . . , T }. The neural backbone εθ(◦, t, τθ(y)) is generally realized as a denoising U-Net [55] with cross-attention conditioning mechanisms [64] to accept additional conditions. The research community also proposes instruction fine-tuning [5], adapter-based modification [47, 80] as extra conditioning mechanisms for a pre-trained text-to-image latent diffusion model. Recent works [14, 58] also extend the pre-trained text-to-image latent diffusion model for text-to-video generation. These works are trained on large scale video data, requiring extensive computational resources.
3. Method
3.1. Overview
In this work, we propose a framework to produce a highquality and diverse colorization, optionally conditioned on user-input textual descriptions for grayscale videos. Specifically, the framework is built upon a pre-trained text-toimage (T2I) latent diffusion model, Stable Diffusion model, to exploit its capability in visual semantic understanding and image synthesis. As shown in Figure 2, we extend the pre-trained Stable Diffusion model to a reference-based frame colorization model using Diffusion Coordinator F. With the adapter-based mechanism [80], we obtain a conditional text-to-image diffusion model that leverages the power of the pre-trained Stable Diffusion model to render colors in the latent space zc, according to the visual semantics of the grayscale input, the text input, and the reference color latent. To generate video outputs with coherent colorization, we introduce two key components: the Color Propagation Attention and the Alternated Sampling Strategy. For each frame in the input grayscale video, we perform a parallel sampling process. Each sampling step for a particular frame is conditioned on the latent information from the previous sampling step of an adjacent frame. Essentially, the Color Propagation Attention and the Alternated Sampling Strategy coordinate the reverse diffusion process and enable bidirectional propagation of color information between adjacent frames to ensure consistency in colorization over time. At last, we design the novel video colorization VQVAE model D that fuses the compressed VQ color prior {zc} and Ig to reconstruct the final colorization {Ic} that precisely aligns with the structure and texture of {Ig} to mitigate information loss of the latent diffusion.
3


Grayscale Inputs
Colorization Results "horse jumping"
Inference
Shortcut
Grayscale Encoder
Frozen weights
"Text Condition"
Diffusion Coordinator
Denoising U-Net
CPA
C
Q
K
V
C Concatenation
Diffusion
Color Propagation Attention
Training
Figure 2. Overview. Our framework consists of three components: a diffusion coordinator model, a pretrained stable diffusion model and a video colorization VQVAE model. The diffusion coordinator modelextends the pretrained T2I diffusion model into a conditional T2I diffsuion model. With the grayscale image Ig and the optional text input text and a reference color frame latent zref, the diffusion coordinator model guides the pretrained Stable Diffusion model to generate a “colorized” latent zc through the generative diffusion process. The video colorization VQVAE model then uses zc as the latent color prior and incorporates the grayscale information of Ig to produce a pixel-aligned colorization result.
3.2. Adapting text-to-image latent diffusion model towards video colorization
The recent state-of-the-art text-to-image diffusion models [54] ) have demonstrated successful learning of the conditional distribution, p(z|text), using large-scale textimage datasets [57]. These models exhibit a deep understanding of the intrinsic structure of natural images and the semantic relationship that exists between textual descriptions and corresponding images. Consequently, these models can serve as effective generative priors. Building upon this insight, we propose a framework that harnesses the ca
pabilities of pre-trained text-to-image latent diffusion models as a color prior for video colorization.
3.2.1 Conditional generation
Normal distribution
Grayscale Input Condition
Grayscale Input Condition +
Color Propagation
="koala bear"
Figure 3. Given a sequence of grayscale frames, the diffusion coordinator redirects the SDE trajectories towards the target conditional data distribution p(z|text, g, gref, zref).
Specifically, given a pre-trained text-to-image latent diffusion model εp for p(z|text), we propose to leverage adapter-based approach [42, 47, 80] to extend εp to a conditional text-to-image latent diffusion model εθ for p(z|text, g, gref, zref). Instead of extending the pretrained text-to-image diffusion model εp into a video diffusion model like [14, 58] which directly operates on an entire sequence of video frames and requires extensive computational resources for training and inference, we design to keep εθ as a conditional text-to-image diffusion model for synthesizing each video frame in a coordinated manner. We design a Diffusion Coordinator module Fθ(εp, g, gref, zref) that can guide the generative diffusion process of a pre-trained diffusion model towards a certain subspace, in which rgb2gray(D(zc)) ≈ Ig and D(zc) should have the same color distribution of D(zref), as shown in Figure 3. The diffusion coordinator shares a similar architecture of the denoising U-Net εp(◦, t, text)
4


except that it contains additional encoders for the grayscale input images and the reference color latents. As shown in Figure 2, we take the feature map FF from each downsampling/upsampling layer of Diffusion Coordinator F and inject them into the corresponding layer of εp:
F′
εpi = Conv1x1(Concat(Fεpi , Conv1x1(FFi ))) (2)
The Diffusion Coordinator takes the grayscale images and a reference color latent as input and learns to adjust the inner features of εp to generate a modified score, which leads the generation result conforming to the grayscale image and reference color latent. In this way, we can consider Fθ(εp, g, gref, zref) as a large diffusion model εθ conditioned on (g, gref, zref).
3.2.2 Color Propagation Attention
The Diffusion Coordinator F takes the grayscale input frame, grayscale version of the reference frame, and the latent version of reference frame as inputs and guides the pretrained text-to-image latent diffusion εp to generate the colorized latent zc for the grayscale input frame g.
We propose the color propagation attention module to propagate the color information from the reference frame to the current frame generation. This reference frame can be a neighboring frame in time. Here, we modify the crossattention mechanism [65] to achieve this propagation. As illustrated in Figure 2, instead of deriving the key K and the value V from the same feature map as in typical usages, we project the features of the current grayscale frame, the features of the reference grayscale frame, and the features of the reference color frame to Q, K, and V , respectively. With this design, our module effectively pairs features from the reference grayscale and reference color frames, by exploiting the locality of latent representations for the reference grayscale and the reference color frames, i.e. the fact that the features at the same spatial location in K and V are likely representing the same patch from the original pixel space. Then, this cross-attention mechanism works like a “table lookup” operation for each query feature Q (from the current grayscale frame), into the “table” of K (from the reference grayscale frame) and V (from reference color frame). Under such design, our module not just determines the latent color features for the current grayscale frame, but also simultaneously propagates the latent color features across frames in the temporal domain:
Fc = Attention(Q, K, V ) = softmax( QKT
√dk
)V
(3)
3.2.3 Alternating Sampling Strategy
With the color propagation attention module, our diffusion model can generate colorized latent based on the neighboring reference frame, e.g. the previous frame. If we simply use the framework in an auto-regressive manner, the temporal coherence is only limited in the forward-time direction. Consequently, the color propagation may degrade through the long-term generation, especially when new content appears in the subsequent frames. To mitigate this issue, we propose to exploit the sampling steps of the diffusion sampling process to ensure long-term information propagation. Recall that we train εθ conditioned on noisy version of reference latents, we do not need to restrict the reference frame to be the previous frame throughout the whole diffusion process. Instead, during the generative diffusion sampling process, εθ can take the intermediate sampling results as the reference latents from either the previous or the next frames in an alternating fashion. Practically, for a certain frame (zt,i, gi), we use εθ(zt,i, t, text, gi, gi−1, zt,i−1) at odd steps and use εθ(zt,i, t, text, gi, gi+1, zt,i+1) at even steps as the score function. By doing so, we gradually propagate the color information in a bidirectional manner, through the diffusion sampling steps and thereby enable the video colorization even we only have a text-to-image model. We show the detailed algorithm in Algorithm 1.
3.2.4 Loss Function
Treating εθ as a conditional diffusion model, the loss function can be written as follows:
L = EE(x),ε∼N (0,1),t
h
∥ε − εθ(zt, t, text, g, gref, zt,ref)∥22
i
(4)
To reduce the learning difficulty and fully utilize diffusion prior, we freeze the weights of the original diffusion model εp during the training.
ALGORITHM 1: Inference
Input: grayscale frame features {gi}N 1: for i = 1, . . . , N do 2: zT,i ∼ N (0, I) 3: end for
4: for t = T, . . . , 1 do 5: for i = 1, . . . , N do
6: μ = εθ(zt,i, t, text, gi, gi−1, zt,i−1) if t is even, else μ = εθ(zt,i, t, text, gi, gi+1, zt,i+1) 7: zt−1,i = update(μ, zt,i) 8: end for 9: end for 10: return {D(z0)}N
5


3.3. Video Colorization VQVAE Decoder
With our conditional latent diffusion model εθ, we manage to achieve video colorization in the latent space. However, the visual quality and temporal consistency may not be guaranteed in the pixel space as we model the conditional data distribution in a compressed vector quantized space [15,63]. The output of the original latent decoder D may exhibit significant distortion and artifacts because the precise structure and texture information are only approximated during the vector quantization process. Consequently, the generated output {zc} is only an estimation for the color
frames I ̃c = D(zc), where the intensity of I ̃c closely resem
bles Ig through the relationship rgb2gray(I ̃c) ≈ Ig. This approximation cannot be directly used as colorization results, as it fails to align the structure and texture accurately with the original grayscale input frames {Ig}.
Unfortunately, we are constrained to the latent space of the pre-trained latent diffusion model in order to exploit its capabilities for image generation. Although the resulting I ̃c falls short in terms of visual quality, it is noteworthy that the generated latent representation zc has already produced commendable colorization outcomes for the input grayscale frames, both at a global and local level, as depicted in Figure 8.
In order to address the aforementioned artifacts and generate high-quality video colorization results, we propose a solution that combines the generated latents zc with pixellevel details extracted from the input grayscale frames. This combination enables precise alignment of the colorization output with the original frame structure. Our approach introduces the video colorization VQVAE model, which consists of a replica of the original latent diffusion VQVAE decoder and a grayscale encoder module G. Instead of relying solely on the decoder D to reconstruct the compressed structure and texture details, we incorporate the grayscale encoder to shortcut and inject grayscale features from Ig into the decoder. This integration provides essential cues to facilitate accurate reconstruction at the pixel level during the decoding process. To further enhance the temporal coherence, we extend D by incorporating pseudo 3D convolution [51, 58]. This is achieved by adding an additional Dirac-initialized temporal convolutional layer at the end of each residual block. Through this extension, we obtain a video VQVAE decoder capable of sharing information between a sequence of frames, thus improving the temporal consistency of the colorization results.
Motivated by the concept of shortcut learning [16], the grayscale encoder G closely resembles the original VQVAE encoder, with the exception that we extract the intermediate features {FGi } = G(Ig) at each downsampling layer as the encoder’s output. These processed grayscale features are then added to the corresponding feature maps in the upsam
pling layers of the VQVAE decoder D.
F′
Di = FDi + Conv1x1(FGi ) (5)
To promote the effective utilization of grayscale features by the video colorization VQVAE model, we adopt a strategy where we freeze the original weights of the VQVAE decoder and solely optimize G and newly added temporal convolutional layers during the training. In this way, we consider Conv1x1(FGi ) as learnable residuals [18]. Consequently, the architecture of the video colorization VQVAE model closely resembles a UNet [55] but is disconnected in the middle layers. Loss function. To enhance the quality of reconstruction, we use a combination of L1 loss, perceptual loss [30], and discriminator loss [15]. The loss function is defined as:
L = L1 + λpLp + λdLd (6)
where the coefficient λp is set to 0.1 and λd is an adaptive weight determined using the strategy proposed in [15].
4. Experiments
4.1. Implementation and Training
Implementation details. We implement our framework in JAX and Flax based on the open-source codebase of diffusers [67]. We use the pre-trained miniSD [50], a fine-tuned Stable Diffusion v1.4 model [54] on image size 256x256 as the latent diffusion prior inside our framework. Data preparation. To train the diffusion coordinator model, we utilize the WebVid-2M video-text dataset [2]. For each video in the dataset, we sample pairs of adjacent frames at random frame rates to create the training data. In order to enable unconditional colorization, we randomly replace the original caption with empty string at a chance of 50 percent and randomly set the reference frame to zeros at a chance of 10 percent. We use spatial resolution of 256 × 256 for training. For frames with an original caption, we first resize the frame to 454 × 256 and perform a center crop. For frames with a replaced caption, we apply a random crop. Additionally, the video colorization VQVAE model is trained on the WebVid-2M video-text dataset [2]. Training setup. For both models, we utilize the AdamW optimizer [46] with a learning rate of lr = 0.00001. The diffusion coordinator model is trained with a per-device batch size of 8 for 200,000 steps on a TPUv3-32. On the other hand, the video colorization VQVAE model is trained with a per-device batch size of 6 for 100,000 steps on a TPUv3-32. To reduce the learning burden, we employ a two-stage training strategy for the diffusion coordinator model: in the first stage, we use the unnoised version of reference frame latent for training; in the second stage, we use the noisy version of reference frame latent for training.
6


Table 1. Quantitative results on the evaluation datasets from different methods. The best items and second best items are highlighted in bold and underline respectively. Ours† uses generated captions from BLIP captioning model [41] as additional inputs. Ours‡ uses prompt ”high quality color photo” and negative prompt ”color bleeding” as additional inputs.
Method DAVIS30 (medium frame length) Videvo20 (long frame length)
FID ↓ Colorfulness ↑ PSNR ↑ SSIM ↑ LPIPS ↓ CDC ↓ FID ↓ Colorfulness ↑ PSNR ↑ SSIM ↑ LPIPS ↓ CDC ↓
AutoColor 83.05 14.14 24.41 0.915 0.264 0.003734 76.28 13.23 25.90 0.925 0.277 0.001668 Deoldify 76.21 25.47 23.99 0.885 0.306 0.004901 66.89 22.05 24.31 0.895 0.325 0.003134 DeepExemplar 77.26 28.82 21.78 0.846 0.325 0.004006 76.63 32.44 20.63 0.831 0.348 0.002011 DeepRemaster 97.54 25.66 21.95 0.848 0.354 0.005098 86.23 28.72 21.88 0.856 0.358 0.003607 TCVC 74.94 21.72 25.17 0.921 0.239 0.003649 76.02 18.89 25.18 0.929 0.273 0.001629 VCGAN 70.29 15.89 23.90 0.910 0.247 0.005303 63.83 14.90 24.67 0.919 0.276 0.002998 Ours 69.51 29.13 23.73 0.939 0.213 0.003607 66.11 20.73 25.27 0.951 0.205 0.001591
Ours† 63.06 32.00 23.12 0.943 0.219 0.003963 61.69 33.44 23.23 0.922 0.235 0.002029 Ours‡ 70.98 36.03 22.33 0.931 0.233 0.003819 63.50 34.37 23.24 0.938 0.226 0.001974
4.2. Evaluations
Dataset. In this section, we assess the performance of our framework along with existing representative works using two widely-used evaluation benchmarks: the DAVIS dataset [49] and the Videvo dataset [36]. We follow the evaluation protocols established by previous studies [45, 85] and conduct evaluations on specific subsets of these datasets. Specifically, we evaluate all methods on the validation split of the DAVIS dataset and the test split of the Videvo dataset. Evaluation metrics. Evaluation of video colorization involves assessing perceptual realism, color vividness, and temporal consistency. To quantitatively measure the perceptual realism of the colorized videos, we employ the Fre ́chet Inception Score (FID) [21]. FID measures the distribution similarity between the predicted colors and the ground truth, thus providing an indication of perceptual realism. For evaluating color vividness, we utilize the Colorfulness metric [17], which closely aligns with human visual perception. This metric enables us to assess the richness and vibrancy of colors in the colorized videos. To evaluate temporal consistency, we utilize the Color Distribution Consistency (CDC) index [45]. This index is derived from the Jensen-Shannon divergence of the color distribution between consecutive frames, providing a measure of how consistent the color transitions are across frames in the colorized videos. In addition to the aforementioned metrics, we also report evaluation results using PSNR, SSIM, and LPIPS [82]. These metrics provide further insights into the perceptual quality of the colorized videos. Comparisons. We conducted a comparative analysis of our approach against two types of video colorization baselines: automatic video colorization and exemplar-based video colorization. In the case of automatic video colorization, we compared our method with four state of the art approaches: AutoColor [39], DeOldify [1], TCVC [45], and VCGAN [85]. For the exemplar-based video colorization baselines,
we compared our approach with DeepExemplar [79] and DeepRemaster [24]. As exemplar-based video colorization methods rely on reference images for color propagation, we utilized the state-of-the-art image colorization method UniColor [23] to generate the necessary references. To assess the quality of our approach, we conducted both qualitative and quantitative evaluations. The qualitative comparison is presented in Figure 4, while the quantitative results are reported in Table 1. The results demonstrate that our approach consistently achieves superior colorization across frames, producing more vivid colors compared to the baseline methods. Our proposed method has achieved state-of-the-art performance in both evaluations. Notably, we observed that providing textual descriptions can significantly enhance the colorization results in terms of perceptual quality and colorfulness. Furthermore, owing to the stochastic nature of the reverse diffusion process, our framework can natively generates diverse colorization results, as depicted in Figure 5.
4.3. User Study
In order to validate the effectiveness of our proposed method and to address the absence of a universally accepted evaluation standard for colorization techniques, we conducted a user study to obtain subjective assessments. This section presents the details of the user study and provides the results. To conduct the user study, we randomly selected 9 videos from the DAVIS validation dataset and 6 videos from the Videvo test dataset. For each video, we presented the colorization results generated by different methods in a grid, with the order of presentation randomized. The participants were then asked to rank the colorization results based on visual quality and temporal consistency respectively. Additionally, each participant was required to answer a validation question. We recruited participants with good vision and color recognition to ensure reliable evaluations. In total, 31 participants successfully completed the
7


Grayscale Input AutoColor Ground Truth DeepExemplar Deoldify DeepRemaster Ours VCGAN TCVC
T=0 T=15 T=30 T=45 T=0 T=15 T=30 T=45
Grayscale Input AutoColor Ground Truth DeepExemplar Deoldify DeepRemaster Ours VCGAN TCVC
T=0 T=15 T=30 T=45 T=0 T=15 T=30 T=45
Figure 4. Qualitative comparison with different video colorization methods.
8


T=0 T=15 T=30 T=45
Figure 5. Our method can generate diverse colorization results.
evaluation. The results of the user study are summarized in Table 2, revealing that our method was preferred by users in terms of both visual quality and temporal consistency.
Table 2. The preferred rate of our framework against other methods in terms of visual quality (VQ) and temporal consistency (TC)
AutoColor Deoldify DeepExemplar DeepRemaster TCVC VCGAN
VQ 0.76 0.69 0.58 0.61 0.79 0.84 TC 0.62 0.81 0.57 0.69 0.55 0.68
(a) (b) (c) (d) (e)
Figure 6. The effect of color propagation attention: (a) input frame; (b) reference frame; (c) w/o color propagation attention; (d) w/ UNet self-attention; (d) w/ color propagation attention
4.4. Ablation Study
The effect of color propagation attention and alternated sampling strategy. In our framework, the combination of Color Propagation Attention and Alternated Sampling Strategy allows us to transform a sequence of image diffusion processes into a coordinated video diffusion process. In this ablation study, we investigate the impact of color propagation attention and the alternated sampling strategy. We specifically train the following variants of our framework: a) accepting reference frame latent without color propagation attention; b) accepting reference frame latent via U-Net self-attention. Figure 6 demonstrates the effects of these variants. It can be observed that the color propagation attention is capable of generating consistent colorization based on a reference frame, while the other variants fail to do so. It is important to note that, for visual comparison purposes, we employ the original VQVAE decoder, excluding the influence of our proposed video colorization decoder. Furthermore, we compare the Alternated Sampling Strategy with the autoregressive generation strategy and the unidirectional sampling strategy. Figure 7 presents the visual comparisons among these strategies. The alternated sampling strategy produces better results than others. The video colorization VQVAE model. To verify the effect of our video colorization VQVAE model in reconstructing videos with grayscale features and temporal coherence, we compare it with the original VQVAE decoder used in Stable Diffusion. The results of this comparison are presented in Table 3, indicating that our video colorization VQVAE model achieves the highest performance when benefiting from grayscale features and pseudo-3D enhancement Furthermore, Figure 8 provides a visual comparison
9


of the decoder outputs, supporting our findings. It is evident from the visual results that our video colorization VQVAE model is capable of producing the sharpest and most visually appealing results.
(a)
(b)
(c)
T=0 T=10 T=20 T=30 T=40
Figure 7. The effect of alternated sampling strategy: (a) autoregressive sampling; (b) unidirectional color information propagation; (c) bidirectional color information propagation
(a)
(b)
(c)
T=0 T=2 T=4 T=8 T=10
Figure 8. The effect of video colorization VQVAE model: (a) original VQVAE decoder (w/ ograyscale feature and temporal convolutional layers); (b) ours (w/o temporal convolutional layers); (c) ours
Table 3. Quantitative comparisons of reconstruction quality by different decoders on our evalutation datasets
Model DAVIS Videvo
PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓
kl-f8 24.29 0.6996 0.1678 26.10 0.7734 0.1539 Ours 35.74 0.9769 0.0356 37.02 0.9790 0.0354
4.5. Limitations and Discussions
Runtime performance. The diffusion model utilized in our framework requires an time-consuming iterative sampling
process, which may take minutes to complete on CPU devices (around 100 seconds per frame on Intel Xeon 6326 with 50 sampling steps and a spatial resolution 256 × 256). Hence, it is imperative that users should utilize GPUs when running our of pipeline (around 6 seconds per frame on NVIDIA RTX 3090 with 50 sampling steps with a spatial resolution of 256 × 256). Besides, the alternated sampling strategy requires a certain number of sampling step to ensure temporal coherence and color information propagation. Consequently, efficient diffusion samplers or diffusion model distillation techniques are incompatible with our framework. Potential biases. We have leveraged a pre-trained model based on miniSD [50] (a fine-tuned version of Stable Diffusion v1.4 [54]), which was trained on the LAION dataset [57], a dataset known to have social and cultural biases. It remains unclear how these biases can manifest themselves in the color space and potentially influence the outputs of our framework.
5. Conclusion
In this paper, we propose an effective video colorization pipeline based on the pre-trained T2I stable diffusion models for realistic and diverse video colorization. As evidenced by extensive evaluation, our method achieves stateof-the-art performance for video colorization.
10


References
[1] Jason Antic. Deoldify: A deep learning based project for colorizing and restoring old images, 2019. 7 [2] Max Bain, Arsha Nagrani, Gu ̈l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. 6
[3] Nir Ben-Zrihem and Lihi Zelnik-Manor. Approximate nearest neighbor fields in video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5233–5242, 2015. 3 [4] Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, and Hanspeter Pfister. Blind video temporal consistency. ACM Transactions on Graphics (TOG), 34(6):1–9, 2015. 3 [5] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. 3 [6] Aur ́elie Bugeau, Vinh-Thong Ta, and Nicolas Papadakis. Variational exemplar-based image colorization. IEEE Transactions on Image Processing (TIP), 23(1):298–307, 2013. 2 [7] Huiwen Chang, Ohad Fried, Yiming Liu, Stephen DiVerdi, and Adam Finkelstein. Palette-based photo recoloring. ACM Transactions on Graphics (TOG), 34(4):139–1, 2015. 2
[8] Zezhou Cheng, Qingxiong Yang, and Bin Sheng. Deep colorization. In IEEE/CVF International Conference on Computer Vision (ICCV), 2015. 2
[9] Alex Yong-Sang Chia, Shaojie Zhuo, Raj Kumar Gupta, YuWing Tai, Siu-Yeung Cho, Ping Tan, and Stephen Lin. Semantic colorization with internet images. ACM Transactions on Graphics (TOG), 30(6):1–8, 2011. 2 [10] Seonghyeon Cho, Hyojin Bahng, Sanghyuk Lee, and Jaegul Cho. Text2colors: Guiding image colorization through textdriven palette generation. In European Conference on Computer Vision (ECCV), 2018. 2
[11] Aditya Deshpande, Jason Rock, and David Forsyth. Learning large-scale automatic image colorization. In IEEE/CVF International Conference on Computer Vision (ICCV), 2015. 2
[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 3
[13] Yuki Endo, Satoshi Iizuka, Yoshihiro Kanamori, and Jun Mitani. Deepprop: Extracting deep features from a single image for edit propagation. In Computer Graphics Forum (CGF), 2016. 1, 2 [14] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models, 2023. 3, 4 [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6
[16] Robert Geirhos, Jo ̈rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe
lix A. Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665–673, nov 2020. 6 [17] David Hasler and Sabine E Suesstrunk. Measuring colorfulness in natural images. In Human vision and electronic imaging VIII, volume 5007, pages 87–95. SPIE, 2003. 7 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 6
[19] Mingming He, Dongdong Chen, Jing Liao, Pedro V Sander, and Lu Yuan. Deep exemplar-based colorization. ACM Transactions on Graphics (TOG), 37(4):1–16, 2018. 2
[20] Mingming He, Jing Liao, Dongdong Chen, Lu Yuan, and Pedro V Sander. Progressive color transfer with dense semantic correspondences. ACM Transactions on Graphics (TOG), 38(2):1–18, 2019. 1, 2 [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Conference on Neural Information Processing Systems (NeurIPS), 2017. 7
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 3
[23] Zhitong Huang, Nanxuan Zhao, and Jing Liao. Unicolor: A unified framework for multi-modal colorization with transformer. ACM Transactions on Graphics (TOG), 41(6):1–13, 2022. 7 [24] Satoshi Iizuka and Edgar Simo-Serra. DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement. ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019), 38(6):1–13, 2019. 7
[25] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification. ACM Transactions on Graphics (TOG), 35(4):1–11, 2016. 2 [26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2
[27] Vivek George Jacob and Sumana Gupta. Colorization of grayscale images and videos using a semiautomatic approach. In 2009 16th IEEE International Conference on Image Processing (ICIP), pages 1653–1656. IEEE, 2009. 3 [28] Varun Jampani, Raghudeep Gadde, and Peter V Gehler. Video propagation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 451–461, 2017. 3 [29] Xiaozhong Ji, Boyuan Jiang, Donghao Luo, Guangpin Tao, Wenqing Chu, Zhifeng Xie, Chengjie Wang, and Ying Tai. Colorformer: Image colorization via color memory assisted hybrid-attention transformer. In European Conference on Computer Vision (ECCV), 2022. 2
[30] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision (ECCV), 2016. 6
11


[31] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2
[32] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [33] Geonung Kim, Kyoungkook Kang, Seongtae Kim, Hwayoon Lee, Sehoon Kim, Jonghyun Kim, Seung-Hwan Baek, and Sunghyun Cho. Bigcolor: Colorization using a generative color prior for natural images. In European Conference on Computer Vision (ECCV), 2022. 2
[34] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. Colorization transformer. In International Conference on Learning Representations (ICLR), 2021. 1, 2
[35] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In Proceedings of the European conference on computer vision (ECCV), pages 170–185, 2018. 3
[36] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In European Conference on Computer Vision, 2018. 7
[37] Gustav Larsson, Carl Doersch, Aaron Sarna, and Kevin Murphy. Pixcolor: Pixel recursive colorization, 2017. 2 [38] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. In European Conference on Computer Vision (ECCV), 2016. 2
[39] Chenyang Lei and Qifeng Chen. Fully automatic video colorization with self-regularization and diversity. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 7 [40] Anat Levin, Dani Lischinski, and Yair Weiss. Colorization using optimization. In ACM SIGGRAPH Conference, 2004. 1, 2, 3 [41] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. 7 [42] Hanyuan Liu, Jinbo Xing, Minshan Xie, Chengze Li, and Tien-Tsin Wong. Improved diffusion-based image colorization via piggybacked models, 2023. 4 [43] Sifei Liu, Guangyu Zhong, Shalini De Mello, Jinwei Gu, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz. Switchable temporal propagation network. In Proceedings of the European Conference on Computer Vision (ECCV), pages 87–102, 2018. 3 [44] Xiaopei Liu, Liang Wan, Yingge Qu, Tien-Tsin Wong, Stephen Lin, Chi-Sing Leung, and Pheng-Ann Heng. Intrinsic colorization. In ACM SIGGRAPH Asia 2008 papers, pages 1–9. 2008. 2 [45] Yihao Liu, Hengyuan Zhao, Kelvin C. K. Chan, Xintao Wang, Chen Change Loy, Yu Qiao, and Chao Dong. Temporally consistent video colorization with deep feature propagation and self-regularization learning, 2021. 7
[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2017. 6 [47] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. 3, 4 [48] X Pan, X Zhan, B Dai, D Lin, CC Loy, and P Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In European Conference on Computer Vision (ECCV), 2020. 2
[49] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. 7
[50] Justin Pinkney. minisd. https://huggingface.co/ justinpinkney/miniSD, 2022. 6, 10
[51] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatiotemporal representation with pseudo-3d residual networks. In ICCV, 2017. 6 [52] Yingge Qu, Tien-Tsin Wong, and Pheng-Ann Heng. Manga colorization. ACM Transactions on Graphics (TOG), 25(3):1214–1220, 2006. 1, 2 [53] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Computer graphics and applications, 21(5):34–41, 2001. 3
[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo ̈rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3, 4, 6, 10 [55] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015. 3, 6 [56] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 45(4):4713–4726, 2022. 3 [57] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 4, 10
[58] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. 3, 4, 6 [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. 3
[60] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3
12


[61] Jheng-Wei Su, Hung-Kuo Chu, and Jia-Bin Huang. Instanceaware image colorization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2
[62] Yu-Wing Tai, Jiaya Jia, and Chi-Keung Tang. Local color transfer via probabilistic segmentation by expectationmaximization. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), volume 1, pages 747–754. IEEE, 2005. 2
[63] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Conference on Neural Information Processing Systems (NeurIPS), 2017. 6
[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. 3
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. 5
[66] Patricia Vitoria, Lara Raad, and Coloma Ballester. Chromagan: Adversarial picture colorization with semantic class distribution. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2020. 2
[67] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https : / / github . com / huggingface / diffusers, 2022. 6
[68] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by colorizing videos. In Proceedings of the European conference on computer vision (ECCV), pages 391–408, 2018. 3
[69] Yi Wang, Menghan Xia, Lu Qi, Jing Shao, and Yu Qiao. Palgan: Image colorization with palette generative adversarial networks. In European Conference on Computer Vision (ECCV), 2022. 2
[70] Tomihisa Welsh, Michael Ashikhmin, and Klaus Mueller. Transferring color to greyscale images. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques, 2002. 2
[71] Shuchen Weng, Jimeng Sun, Yu Li, Si Li, and Boxin Shi. Ct2: Colorization transformer via color tokens. In European Conference on Computer Vision (ECCV), 2022. 2
[72] Shuchen Weng, Hao Wu, Zheng Chang, Jiajun Tang, Si Li, and Boxin Shi. L-code: Language-based colorization using color-object decoupled conditions. In AAAI Conference on Artificial Intelligence (AAAI), 2022. 2
[73] Yanze Wu, Xintao Wang, Yu Li, Honglun Zhang, Xun Zhao, and Ying Shan. Towards vivid and diverse image colorization with generative color prior. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 1
[74] Yanze Wu, Xintao Wang, Yu Li, Honglun Zhang, Xun Zhao, and Ying Shan. Towards vivid and diverse image colorization with generative color prior. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2
[75] Menghan Xia, Wenbo Hu, Tien-Tsin Wong, and Jue Wang. Disentangled image colorization via global anchors. ACM Transactions on Graphics (TOG), 41(6):1–13, 2022. 2
[76] Sifeng Xia, Jiaying Liu, Yuming Fang, Wenhan Yang, and Zongming Guo. Robust and automatic video colorization via multiframe reordering refinement. In 2016 IEEE International Conference on Image Processing (ICIP), pages 40174021. IEEE, 2016. 3 [77] Zhongyou Xu, Tingting Wang, Faming Fang, Yun Sheng, and Guixu Zhang. Stylization-based architecture for fast deep exemplar colorization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, 2
[78] Liron Yatziv and Guillermo Sapiro. Fast image and video colorization using chrominance blending. IEEE Transactions on Image Processing (TIP), 15(5):1120–1129, 2006. 1, 2, 3 [79] Bo Zhang, Mingming He, Jing Liao, Pedro V Sander, Lu Yuan, Amine Bermak, and Dong Chen. Deep exemplarbased video colorization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8052–8061, 2019. 3, 7 [80] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 3, 4
[81] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Conference on Computer Vision (ECCV), pages 649–666, 2016. 2 [82] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 7 [83] Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A Efros. Real-time user-guided image colorization with learned deep priors. ACM Transactions on Graphics (TOG), 36(4):1–11, 2017. 1, 2 [84] Jiaojiao Zhao, Jungong Han, Ling Shao, and Cees GM Snoek. Pixelated semantic colorization. International Journal of Computer Vision (IJCV), 128:818–834, 2020. 2
[85] Yuzhi Zhao, Lai-Man Po, Wing Yin Yu, Yasar Abbas Ur Rehman, Mengyang Liu, Yujia Zhang, and Weifeng Ou. Vcgan: Video colorization with hybrid generative adversarial network. IEEE Transactions on Multimedia, pages 1–1, 2022. 7 [86] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. Indomain gan inversion for real image editing. In European Conference on Computer Vision (ECCV), 2020. 2
[87] Jun-Yan Zhu, Philipp Kr ̈ahenbu ̈hl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision (ECCV), 2016. 2
13