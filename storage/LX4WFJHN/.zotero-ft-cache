Training-Free Motion-Guided Video Generation with Enhanced Temporal
Consistency Using Motion Consistency Loss
Xinyu Zhang1 Zicheng Duan1 Dong Gong2 Lingqiao Liu1
1The University of Adelaide 2The University of New South Wales
https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/
Abstract
In this paper, we address the challenge of generating temporally consistent videos with motion guidance. While many existing methods depend on additional control modules or inference-time fine-tuning, recent studies suggest that effective motion guidance is achievable without altering the model architecture or requiring extra training. Such approaches offer promising compatibility with various video generation foundation models. However, existing trainingfree methods often struggle to maintain consistent temporal coherence across frames or to follow guided motion accurately. In this work, we propose a simple yet effective solution that combines an initial-noise-based approach with a novel motion consistency loss, the latter being our key innovation. Specifically, we capture the inter-frame feature correlation patterns of intermediate features from a video diffusion model to represent the motion pattern of the reference video. We then design a motion consistency loss to maintain similar feature correlation patterns in the generated video, using the gradient of this loss in the latent space to guide the generation process for precise motion control. This approach improves temporal consistency across various motion control tasks while preserving the benefits of a training-free setup. Extensive experiments show that our method sets a new standard for efficient, temporally coherent video generation.
1. Introduction
Recent advancements in video diffusion models [3, 5, 6, 15, 22, 23, 54, 60, 67, 85], have greatly improved the quality of videos generated from text instructions. Nonetheless, in many cases, text instructions alone may fall short of fully conveying the user’s intent, emphasizing the need for additional guidance in video generation. Motion customization presents a promising solution by enabling the generation of videos that integrate content from text descriptions with pre
Reference video
Motion Director [93]
Ours
Box trajectory
FreeTraj [47]
Ours
A bear climbing down a tree after spotting a threat.
A panda is lifting weights in a garden
Figure 1. Visualization comparisons on our method and two existing motion customization methods, including the reference video based Motiondirector [93], and the bounding box trajectory based FreeTraj [47]. Methods in the upper part use the inversion noise from the reference video, while methods in the lower part use the well-designed noise as initialization. The red circle regions represent the inconsistent temporal coherent, while the green circle regions represent the correct one.
cise motion guidance drawn from a reference movement. This approach proves valuable in many scenarios. For instance, it can be useful for creating content that is unlikely to occur in real-world scenarios, such as “a panda lifting weights”. Through motion customization, it becomes possible to adapt the “lifting weights” motion style from a “human lifting weights” video to animate a panda.
Existing approaches to motion customization take various guidance types, such as reference videos, trajectories, bounding box movements, or motion fields, as control signals. Many of these methods depend on additional modules, including Low-Rank Adaptations (LoRAs) [24, 49, 80, 93],
1
arXiv:2501.07563v1 [cs.CV] 13 Jan 2025


adapters or embeddings [33–35, 44, 69, 74, 75, 81, 90, 92], or fine-tuning the video generation modules [7, 27, 53, 76, 78, 84, 91], which incur extra costs during either training or testing. When new foundation models or updates are released, these methods require retraining to adapt to the updated models. This challenge has led to the development of trainingfree solutions [26, 28, 37, 46, 47, 79, 82], which aim to achieve motion guidance by designing specific noise initialization schemes [46, 47, 79] and using attention mask operations [26, 28, 47, 82]. Despite these advancements, many current methods still struggle with maintaining temporal coherence across frames. The lower part of Figure 1 illustrates this issue, showing that existing methods often suffer from temporal inconsistency, e.g., the loss of detail on a bear when occluded by a tree. To address the challenge of inconsistent temporal coherence, we propose a simple yet effective training-free solution in this paper. Our key insight is that better motion guidance can be achieved by combining two complementary approaches: implicit motion guidance through initial noise and explicit guidance via a motion pattern consistency loss. Specifically, we begin by using the inversion noise from the reference video as the noise initialization for the video diffusion model. Recent studies [35, 47, 53, 79, 93] have demonstrated the effectiveness of this strategy, as the initial noise serves as a strong prior that captures the movement within the video. However, our empirical findings indicate that relying solely on initial noise is not always effective; in some cases, it can lead to inconsistencies in appearance across frames, such as the case shown in Figure 1. As a remedy, we propose a more explicit control scheme to enhance temporal consistency. We start by characterizing the motion pattern of the reference video through the feature correlation patterns from a diffusion model across different frames. We then design a loss function to encourage similar correlation patterns in the generated video, incorporating the gradient of this loss into the denoising inference process in a classifier-guidance-like manner [21]. Moreover, rather than examining correlation between all pairs of local features across frames, we focus on the correlation patterns between a few key points in one frame and features in neighboring frames. This selective approach avoids overly rigid motion transfer, which is especially useful when the reference video motion does not perfectly align with the desired motion in the generated video. Since the initial noise from the reference video already provides a reasonable level of motion control, and video diffusion models naturally maintain a degree of temporal consistency, we find that focusing on a few key points is sufficient to achieve strong temporal coherence. Quantitative and qualitative experiments on three benchmarks, including LOVEU-TGVE-2023 [77], UCF Sports
Action [58], and prompts from [47], and real collected videos from Phone, show that our method can follow the motion movement with enhanced temporal coherent. Our contributions are as follows: • We propose a simple and effective training-free technique to improve the motion-guided video generation by enhancing frame-to-frame coherence. • We design an approach that represents the reference motion through the inter-frame feature correlation patterns of sparse points and transfers motion by replicating these reference matching patterns. • We demonstrate that the proposed approach complements the initial-noise-based motion guidance method. Together, they consistently enhance temporal coherence across a wide range of diverse and complex motions.
2. Related Work
Video diffusion models. A range of approaches approaches have been explored to achieve high-quality video generation, including Generative Adversarial Networks (GANs) [1, 51, 52, 63, 64, 66], autoregressive models [12, 23, 31, 32, 59, 62, 71, 83], and implicit neural representations [55, 87]. With diffusion models [3, 16, 45, 50] appear, recent diffusion-based video generation models [4, 14, 15, 22, 29, 36, 39, 43, 54, 65, 72, 76, 88, 89] have shown high ability to produce high-quality generation. These foundation models are usually pre-trained on large-scale video datasets [8, 42, 61, 73]. Some open-sourced foundation models, such as [23, 60, 67, 85], provide base video generation models for users to customize and build specific models.
Controllable video generation. Controllable video generation aims to generate synthetic data according to the given explicit control signals, such as motion, human pose, layout, optical flows, etc. [13, 38, 90, 94]. The controllable text-to-video generation methods focus on generating videos based on the control signals. Some methods, e.g., VideoCrafter [15], VideoComposer [70], Control-AVideo [9], employ depths, sketches, or movement information from the reference videos as conditions to control motions, whose structures are like ControlNet [90]. Many methods [49, 80, 93] have attempted to train lightweight modules, like Low-Rank Adaptations (LoRAs) [24], for motion customization to transfer the motion from the reference videos to the target ones. However, these reference video based methods usually need to train different modules for different motion types, limiting the generalization ability. Some approaches [7, 10, 25, 26, 37, 47, 68, 84] explore to use trajectories or bounding boxes to provide motion information, which are more flexible and user-friendly. In this work, we focus on a general training-free approach, which can be integrated into various video generation models using various motion formats, e.g., reference
2


DDIM inversion
Ref video latent zref Inversed noise zT
DDIM inversion
Ref video latent zref Inversed noise zt′
t′-step
T-steps
(a) Inversion noise initialization
3D U-Net
Null text ∅
3D U-Net
Text y
Given Point p Motion pattern M
Given Point p Motion pattern M′
Frame-to-frame motion consistency loss Lc
(b) Motion pattern extraction
zt−1
(c) Denosing process × T
Figure 2. Overview of our method. We first conduct (a) inversion noise initialization on the reference video to obtain the initial noise zT (Section 3.2). Then we (b) extract the motion pattern M M M from the reference video for each tracked point p (Section 3.3). During the (c) denoising process, we use the proposed frame-to-frame motion consistency loss Lc, calculated with Eq. 4 based on M M M and newly extracted M M M′ from the noise zt as the motion guidance for the noise estimation (Section 3.4). The detail of our method is in Algorithm 1.
video based or trajectory based motions, without requiring additional modules or training.
3. Method
3.1. Preliminaries
Video diffusion model. Video diffusion models generate videos by repeatedly denoising a randomly sampled sequence of Gaussian noises. They are commonly represented as functions εθ(zt, t, y). A common implementation of this mapping function is the 3D U-Net, which consists of downsampling, middle, and up-sampling blocks. Each block includes multiple convolutional layers, as well as spatial and temporal transformers. The function εθ(zt, t, y) predicts the noise from a noisy latent zt conditioned on y at step t of the diffusion process. εθ(zt, t, y) can be trained by minimizing:
L = Ez0,y,ε∼N (0,I),t∼U(0,T ) ∥ε − εθ(zt, t, y)∥2
2 , (1)
where z0 ∈ R4×F ×H×W is the latent code from the encoder, where F , H, W represents the video length, height and width of the latent code. t ∼ U(0, T ) is random sampled when training, where T is the maximum timestep.
Guidance in diffusion models. Classifier guidance is a widely used technique for injecting external control in content generation [2, 11, 13, 21, 57]. It introduces a loss term, Le, to measure how well the generated latent aligns with the control target, integrating the gradient of this loss function into the denoising process to achieve controlled generation. Formally, it augments the noise estimate with the following formula:
εˆθ(zt, t, y) := εθ(zt, t, y) + σt∇zt Le(zt), (2)
where, εˆθ represents the modified noise estimate, and σt is a weighting schedule. These methods offer the advantage of not requiring model retraining, as they simply update the score function calculation using the gradients of Le.
3.2. Inversion Noise Initialization
Several image and video editing studies [19, 28, 46, 47, 53, 79] have demonstrated that initial noises significantly impact the generated content. For example, some reference video-based or trajectory-based video generation methods [46, 47, 53] have shown that the direction of motion in the generated video often aligns with the motion flow present in the noise initialization. Building on this concept, recent works like [47] inject a user-provided motion trajectory into customized initial noise, enabling motion control within the video. In another approach, [53] optimizes a nulltext embedding to capture and replicate the motion in a reference video. Our method is grounded in the same principle: we treat a reference video—either provided directly by the user or synthesized based on a user-defined trajectory—as the primary guide for motion in the generated video. To incorporate this motion information, we employ DDIM [56] inversion to convert the reference video into a specific Gaussian noise, denoted as zT . This inversion process captures the motion characteristics of the reference, embedding them as initial noise. Once this initial noise is established, one can use an existing video diffusion model to generates a video based on this motion-guided initialization. As illustrated in the third row in Figure 5, while this straightforward approach may introduce some artifacts in the generated video, it still manages to follow the reference video’s motion to a significant extent. This indicates that the
3


initial noise derived from reference video inversion can effectively guide the generated video’s movement, preserving key aspects of the motion while allowing room for refinement.
3.3. Motion Pattern Extraction
As discussed in the Introduction (Section 1), this paper aims to establish a more explicit motion control to enhance the temporal coherence of initial-noise-based approaches. The first question we address is how to effectively represent motion patterns. Traditionally, dense motion fields [9, 44, 81] or optical flow [13, 86] have been used for this purpose. However, these are calculated at the pixel level, making it inconvenient for guiding video generation, as one would need to generate the video first and then verify if the motion pattern aligns with the reference video.
In this paper, we propose an alternative solution by using inter-frame feature correlation patterns to represent the motion pattern of the reference video. This is a reasonable assumption, as the features from a diffusion model can capture certain semantic correspondence [18, 41]. Specifically, we first extract the feature maps {Fl}L
l=1 from all temporal attention modules in the video diffusion model with the input of zt′ , where zt′ is obtained by adding a t′-step (we empirically choose t′ = 1) noise to the reference video. We run a 1-step denoising process εθ(zt′ , t′, ∅). l = {1, 2, ..., L} is the layer number of temporal transformer modules. ∅ represents the null-text prompt. Here, Fl ∈ RCl×F ×Hl×Wl , where Cl, Hl and Wl represents the channel number, spatial height and width in the output feature Fl of the l-th temporal attention module. We then extract the feature correlation patterns from Fl. Rather than examining each pair of features in Fl that is time-consuming, we instead select a few key points, which can be collected from human-interactive click, centroids of the box trajectories, tracking with detector, or random selection, to calculate their correlation patterns with features in the other frames. In our implementation, we use one point from the human-interactive click as the starting point and track it through the video, and omit l in the rest of this subsection for clear description since we operate the same operation in each temporal transformer module.
Given one point p = (y, x) in frame f ∈ {1, 2, ..., F }, we can extract its feature fff = F[:, f, y, x] ∈ RC from one temporal transformer module. To obtain the correlation pattern of p across frames, we calculate the similarity between fff at p and the feature fff (i,j,k) = F[:, i, j, k] ∈ RC of each point in other frames, where j ∈ {1, 2, ..., H}, k ∈ {1, 2, ..., W }. The calculation is only conducted on the subsequent frames, i.e., i ∈ {f + 1, f + 2, ..., F }.
Algorithm 1: Our training-free motion trajectory guided denoising process.
Require: 3D U-Net denoiser εθ; 3D VAE Encoder and Decoder; Maximum timestep T ; Reference video xref ; Target text prompt y; The number of gradient guidance steps n; Guidance scale σt. Initialization: i) Compute the latent code zref =Encoder(xref );
ii) Compute the inversed noisy latent zT from zref ; or use the trajectory-based noise as in [47]; iii) Compute the inversed noisy latent zt′ from zref ;
iv) Conduct 1-step denoising εθ(zt′ , t′, ∅) to extract feature maps from temporal transformer modules; v) Obtain one point p on a specific frame f ; vi) Calculate motion correlation pattern M M M for p according to Eq. 3; 1 for t = T to 1 do 2 if T − t <n then
3 Conduct εθ(zt, t, y) to extract feature maps from temporal transfer modules; 4 Calculate motion correlation pattern M M M′ for p according to Eq. 3; 5 Calculate consistency loss Lc according to Eq. 4; 6 Calculate noise estimation according to Eq. 2 with εˆθ(zt, t, y) := εθ(zt, t, y) + σt∇zt Lc(zt); 7 else
8 εˆθ(zt, t, y) := εθ(zt, t, y) 9 end
10 Calculate latent code zt−1; 11 end
12 x0 = Decoder(z0) Output : x0
We then formulate the correlation pattern M M M as follows:
M M M = {MMMf+1, MMMf+2, ..., MMMF },
MMMi(j, k) = exp(sim(fff · fff (i,j,k))/τ )
PH h=1
PW
w=1 exp(sim(fff · fff (i,h,w))/τ ) , (3)
where sim(uuu, vvv) = uuuTvvv/ ||uuu|| ||vvv|| denotes the cosine similarity between two vectors uuu and vvv. τ is a temperature hyper-parameter. MMMi(j, k) represents the distribution of the temporal coherent in the i-th frame for the given point p. The higher value of MMMi(j, k), the higher semantic correspondence between the point (j, k) in the i-th frame and the point p in the f -th frame. Here, we use the Sof tmax to ensure M M M focuses on strongly matched features.
Remark:
i) The motion pattern of a key point can be understood as a soft trajectory. If a point has a high matching score (e.g., cosine similarity) with a point in another frame while its matching scores with other locations remain low, then MMMi approaches a one-hot vector. This simplifies to a standard trajectory, where one point is distinctly matched to another point in a different frame. However, in diffusion models—particularly when using features from large time steps or specific layers—a distinct matching point often does not exist. In such cases, MMMi becomes more uniform, resembling a relaxed trajectory. Empirically, we find it beneficial to in
4


clude features from various time steps and blocks, even if these features may not exhibit precise semantic correspondence. ii) Depending on the video generation model [17, 60], features from different spatial locations at different frames may not directly interact; for example, some models do not implement full spatial-temporal attention. As a result, the motion pattern MMMi differs from the attention maps typically extracted from a diffusion model.
3.4. Frame-to-Frame Motion Consistency Loss
With the extracted motion pattern M M M from the reference video, we further consider how to transfer this motion pattern to the generated video.We design a frame-to-frame consistency loss Lc to measure how well the motion trajectory M M M′, derived from the output feature from the noisy latent zt, is consist with the motion trajectory guidance M M M. Note that zt is the tth-timestep latent in the denoising process, initialized with zT from the inversion of the reference video. For each timestep, we minimize the loss Lc:
Lc =
F
X
f =1
F
X
i=f +1
||MMM′
i − MMMi||2
2, (4)
where M M M′ = {MMM′
f+1, MMM′
f+2, ..., MMM′
F } shares the same calculation as M M M, while is extracted at the current time step from the noisy latent. Here, we simply use the same M M M as the motion guidance for all time steps1. Then we can replace Le in the noise estimation in Eq. 2 as our proposed Lc, formulated as εˆθ(zt, t, y) := εθ(zt, t, y) + σt∇zt Lc(zt). The overall details of our method is in Algorithm 1, which is a training-free technique without any parameter update. Therefore, it is flexible to use our method to any updated or state-of-the-art foundation models.
Variant on trajectory-controllable video generation. The method above is based on a given reference video. When meeting trajectory based methods [76, 92], i.e., trajectorycontrollable video generation, we follow the the original noise initialization in their methods, such as [47]. For the motion pattern extraction in Section 3.3, we first build a box moving video with the given trajectory, which is used as the reference video xref in Algorithm 1. Therefore, our method supports both reference video based and trajectory based approaches.
4. Experiments
4.1. Datasets and Evaluation Metrics
Datasets. We conduct the comparison experiments on three benchmarks. The first one is the open-source benchmark
1It is possible to use different M M M in different time steps, which we leave it for the future since it may need more time consumption.
LOVEU-TGVE in the CVPR 2023 competition [77]. This dataset comprises 76 videos, each originally associated with 4 editing text prompts, which will generate 304 videos. The second one is 56 prompts used in FreeTraj [47], which are mostly extended from previous baselines [26, 37]. For each prompt, we initialize 16 random noises with 8 different trajectories and 2 random initial noises, resulting in total 896 generated videos. We also conduct experiments on UCF Sports Action [58] for motion customization following the adapted text prompts in MotionDirector [93].
Evaluation metrics. We compare the proposed method with reference video based MotionDirector [93] that is a training-based controllable generation method. Following the LOVEU-TGVE competition [77], we use the average CLIP score [20] between the diverse text prompts and all frames of the generated videos to compute the appearance diversity (AP), and the average CLIP score between frames to compute the temporal consistency (TC), and the average PickScore [30] between all frames of output videos to compute the Pick Score (PS). We also compare a trajectory-controllable method FreeTraj [47]. We report CLIP Similarity (CLIP-SIM) [48] to measure the semantic similarity among frames, and propose to use CLIP Similarity in the ground-truth trajectory boxes (CLIP-SIM-GTBox) to measure the semantic similarity in the trajectory boxes among frames, evaluating the trajectory following and temporal coherent ability. Following [47], we also alculate the Mean Intersection of Union (mIoU) and Centroid Distance (CD) to evaluate the trajectory alignment by obtaining the bounding box of the synthesized objects with OWL-ViT-large [40] detector.
Human Preference. We conduct user study by shuffling our generated videos and videos from MotionDirector [93] or FreeTraj [47]. A total of 25 users were asked to pick the best one video in three dimensions, i.e., video-text alignment (text align), the trajectory or motion alignment (trajectory / motion align), and video quality, respectively. To simplify the comparison for raters, users are asked to compare the results pairwise and select their preferred one. The pairwise numbers “x1 vs. x2” in the human evaluation represents that x1% proportion that generated videos from the compared method are preferred, and x2% proportion that generated videos from ours are preferred.
4.2. Implementation Details
For fair comparison, we use VideoCrafter [17] and ZeroScope [60] as the pre-trained video model following [47] and [93] for the trajectory based setting and the reference video based setting, respectively. By default, we set the weighting schedule σt to 10000.0, and temperature in Eq. 3 to 10.0. The number of gradient guidance steps n = T , where T = 50 for trajectory-based methods and T = 30 for reference video based methods. To generate the sparse
5


Automatic Evaluations Human Evaluations Method CLIP-SIM (↑) CLIP-SIM-GTBox (↑) mIoU (↑) CD (↓) Comparison Trajectory Align Text Align Video Quality Peekaboo [26] 0.942 0.869 0.143 0.23 vs. Ours 10.94 vs. 89.06 24.44 vs. 75.56 30.20 vs. 69.80 FreeTraj [47] 0.951 0.886 0.268 0.11 vs. Ours 47.88 vs. 52.12 49.11 vs. 50.89 42.66 vs. 57.34 Ours 0.947 0.889 0.272 0.11 - - - 
Table 1. Quantitative comparison of trajectory control. Automatic and human evaluations results with the trajectory based videos. We re-implement Peekaboo [26] and FreeTraj [47] using their official code with the same prompts as ours. Our method achieves competitive performance in metrics about video quality and gains the best scores in metrics that are related to trajectory control.
A man in gray clothes running in the summer. A kangaroo jumping in the Australian outback.
Direct
Peekaboo [26]
FreeTraj [47]
Ours
Figure 3. Qualitative comparison of trajectory control. We evaluate our method and other trajectory based approaches, i.e., Peekaboo [26] and FreeTraj [47]. The “Direct” means the direct inference with random noise and no other guidance. We use the same initial noises as in [47] for better visual comparison. Our method shows better ability on trajectory follow and temporal coherent consistency.
points, we design a simple GUI to collect points from human-interactive click. We simply use the centriod point of the box trajectory as the sparse point. More details are in the supplementary material.
4.3. Evaluation of Trajectory Control
We compare our method to other training-free trajectorycontrollable video generation method with diffusion models, including Peekaboo [26] and FreeTraj [47]. Our method uses the same noise initialization as in FreeTraj [47] for fair comparison.
Quantitative evaluation. The left part in Table 1 shows the quantitative evaluations on these training-free trajectorycontrollable video generation methods. Our method and FreeTraj [47] uses trajectory-based noise initialization and attention operation, improves the model’s ability of trajectory following. With the same initialization as in [47], our method with the motion pattern consistency guidance can further improves the control ability of trajectory, e.g., improves 0.4% on mIoU and 0.3% on CLIP-SIM-GTBox, showing the strong ability of our method on improving temporal coherent. Meanwhile, we find the CLIP-SIM, which are the references for video quality, are slightly worse than FreeTraj. The underlying reason is that FreeTraj sometimes generate videos with slight motion, failing to follow trajectories, e.g., left part in Figure 3; so that the the similarity among frames will be improved. The similar observation is
also in FreeTraj that the naive baseline has the best CLIPSIM since the model tends to generate static videos. We then crop the videos in the ground-truth trajectory boxes to show whether the temporal content in the GT box is consistent or not, i.e., calculating CLIP-SIM-GTBox. It clearly shows that our method can achieve the best performance since our method can maintain the temporal coherent and follow the trajectory movement.
Quantitative human evaluation. We also conduct a user study to evaluate our results based on human subjective preference. Users are instructed to select the preferred generated videos from two candidates in three evaluation metrics: trajectory alignment, video-text alignment, and video quality. The results are shown in the right part in Table 1. It clearly shows that our approach outperforms all other trajectory-based methods by a significant margin in all metrics. Notably, our method can further improve the trajectory alignment even applying the motion pattern consistency guidance on the relative strong method FreeTraj, and improve a lot on the video quality due to the better temporal coherent consistency.
Qualitative evaluation. As shown in Figure 3, Peekaboo [26] has the worst control to follow the given trajectory. FreeTraj [47] can follow the given trajectory but not always precisely. In addition, generated videos from FreeTraj sometimes have weird artifacts and inconsistent content among frames, e.g., there is a box artifact in the left
6


Reference video & click point
A panda is riding a bicycle in a garden.
Motion Director [93]
Ours
An alien is riding a bicycle on Mars.
Motion Director [93]
Ours
A monkey is playing golf on a field full of flowers.
A man is playing golf in front of the White House
Figure 4. Qualitative comparison of reference video control. We evaluate our method and MotionDirector [93]. The red circle represents the given point clicked by users. The red and green rectangle are highlight areas to show the temporal coherent clearly. We keep the initial noises same in [47] and our method for fair comparison.
Automatic Evaluations Human Evaluations Method AD (↑) TC (↑) PS (↑) Comparison Text Align Motion Align Video Quality MotionDirector [93] 25.95 93.1 20.37 vs. Ours 48.68 vs. 51.32 46.05 vs. 53.95 44.74 vs. 55.26 Ours 25.74 93.3 20.34 - - - 
Table 2. Quantitative comparison of reference video control. Automatic and human evaluations results of motion customization on single videos. We re-implement MotionDirector [93] using the official code with the same prompts as ours.
example, and the kangaroo has two tails in the last frame. Our method can not only succeed in driving the target object following the given trajectories, but also maintain consistent temporal coherent.
4.4. Evaluation of Reference Video Control
We compare our method to the training-based reference video based video generation method, MotionDirector [93]. We use the same noise initialization as in MotionDirector [47] for fair comparison, i.e., inversion noise initialization as described in Section 3.2.
Quantitative evaluation. In Table. 2, we refer to the alignment between the generated videos and the 4 editing text prompts as the appearance diversity. The results show that our method achieves competitive performance compared with the reference video based method, MotionDirector [93], which requires additional training. Our method and MotionDirector achieve competitive performance on appearance diversity (AD) and Pick Score (PS), while the latter has slightly better result. The underlying reason is that
the our method make all 4 generated video to be similar to the motion of reference videos, resulting in the reduction of diversity. In contrast, with motion pattern consistency guidance, our method achieves better result than MotionDirector on temporal consistency (TC) metric. To intuitively reflect the effectiveness of our method, we add the user study to evaluate the quality of generated videos by humans.
Quantitative human evaluation. We evaluate our results based on human subjective preference, with the selection of the preferred generated videos from two candidates in three evaluation metrics: video-text alignment, motion alignment with the reference video, and video quality. The results are shown in the right part in Table 2. Since generated videos from MotionDirector sometimes have appearance artifacts, or inconsistent motion among frames, the results are lower than that of our method on all three human evaluation metrics. It shows that our method achieves high temporal coherent among frames.
Qualitative evaluation. We take a set of videos from UCF Sport Action [58] as the reference videos for the qualitative
7


Reference video & click point
A lion is running on the road.
No inversion noise initialization
No frame-toframe consistency guidance
Ours
Figure 5. Ablation study on each component in our method, including the inversion noise initialization and frame-to-frame consistency guidance.
comparison. To compare MotionDirector [93] and proposed method fairly, we feed the same initial noise2, consisting of the weighted sum of the inversion noise from the reference video and a random Gaussian noise, to both [93] and our method to generate videos. As shown in Fig. 4, the Baseline can correctly generates the appearance but lacks the realistic motion from the reference video. MotionDirector [93] can generate the desired motion since temporal loras are trained on the reference video to inject the model. However, it still suffer the issue of frame-to-frame temporal coherent inconsistency with obvious appearance artifacts. Our method can draw a similar motion from the reference video while has good consistency among frames, showing the the ability of following the motion from the reference video and temporal consistency.
4.5. Ablation Studies
Impact on inversion noise initialization. As shown in Figure 5, when not using the inversion noise initialization, our method reduces to apply the frame-to-frame consistency guidance on a random noise. It can follow a rough route of the reference car motion, while failing to simulate the large motion with appearance variance, i.e., car turn. It shows that the inversion noise initialization could provide the motion trajectory implicitly.
Impact on frame-to-frame consistency guidance with Lc. If there is no frame-to-frame consistency guidance with Lc, our method reduces to [93] as we use its pipeline. [93] trains temporal LoRAs to learn the motion from the reference video, thus the model can successfully capture the motion movement. However, the temporal consistency is not good, especially on the content details. For example, the
2We find that MotionDirector [93] is sensitive to the random seed, i.e., the initial noise. To show the effectiveness of our method, we here use the same random seed for fair comparison.
Reference video & click point
A rabbit is moving its ear.
Ours
Reference video & click point
A snake is moving forward.
Ours
Figure 6. Gesture simulation. We use two gestures captured from the camera to simulate animal’s movement. Our method can successfully generate the video with accurate ear moving of rabbit and body moving of snake when the given point is on the finger and ball. (Best view in video in the project link.)
hairs on the lion’s head vary among frames. With our proposed frame-to-frame consistency guidance, the temporal coherent clearly improves, showing the effectiveness of our method. An interesting observation from Figure 5 is that the tracking error in the last frame, where the point is tracked on the car body instead of the car front, does not affect temporal coherence. This is because our approach characterizes motion through feature correlations between key points and local features across multiple frames, allowing minor errors in individual frames to be tolerated.
Gesture simulation. We also conduct an interesting experiment to verify the effectiveness of our method on gesture simulation. We use iPhone 15 pro max to take two videos of hand moving; one is with two fingers bent, and another one with holding a ball moving. We find that our method can successfully simulate the motion of fingers to that of the rabbit’s ear and snake’s head. It will be useful when the target video is hard to be captured, e.g., when a child talks to a snake in the movie (a similar scene as in Harry Hotter), we can use gesture to simulate the snake’s behavior and generate it.
5. Conclusion
This paper addresses the challenge of achieving temporal consistency in motion-guided video generation. We propose a simple yet effective training-free approach that combines inversion noise initialization with a novel motion consistency loss to improve both temporal coherence and motion accuracy. By leveraging inter-frame motion feature pattern correlations at sparse points, our method transfers motion from reference videos or trajectories by replicating these patterns in the generated videos. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our approach in enhancing temporal consistency.
8


References
[1] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, page 2, 2019. 2 [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models, 2023. 3 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2
[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 22563–22575, 2023. 2
[5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 1
[6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 7310–7320, 2024. 1 [7] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. 2
[8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In CVPR, pages 13320–13331, 2024. 2 [9] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 2, 4
[10] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. In ECCV, pages 183–199, 2025. 2 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:8780–8794, 2021. 3 [12] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In ECCV, pages 102–118. Springer, 2022. 2 [13] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. In ICLR, 2024. 2, 3, 4
[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2
[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 1, 2 [16] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. 2
[17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Videocrafter: A toolkit for text-to-video generation and editing. https://github.com/AILabCVC/VideoCrafter, 2023. 5, 1
[18] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. NeurIPS, 36, 2024. 4 [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3
[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 5 [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 2, 3 [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 2
[23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1, 2
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 1, 2
[25] Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, and Ming-Hsuan Yang. Fine-grained controllable video generation via object appearance and context. arXiv preprint arXiv:2312.02919, 2023. 2
[26] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskeddiffusion. In CVPR, pages 8079–8088, 2024. 2, 5, 6, 4 [27] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In CVPR, pages 9212–9221, 2024. 2 [28] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and Jong Chul Ye. Dreammotion: Space-time self-similar score distillation for zero-shot video editing. In ECCV, pages 358376, 2025. 2, 3
9


[29] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2
[30] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023. 5
[31] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose ́ Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 2
[32] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs: context-aware controllable video synthesis. Advances in Neural Information Processing Systems, 34:14042–14055, 2021. 2 [33] Mingxiao Li, Bo Wan, Marie-Francine Moens, and Tinne Tuytelaars. Animate your motion: Turning still images into dynamic videos. arXiv preprint arXiv:2403.10179, 2024. 2 [34] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. arXiv preprint arXiv:2406.17601, 2024.
[35] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, pages 8599–8608, 2024. 2 [36] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2
[37] Wan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. arXiv preprint arXiv:2401.00896, 2023. 2, 5
[38] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023. 2
[39] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 9117–9125, 2023. 2 [40] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European Conference on Computer Vision, pages 728–755. Springer, 2022. 5 [41] Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seunggyu Chang. Dreammatcher: Appearance matching self-attention for semantically-consistent text-toimage personalization. In CVPR, pages 8100–8110, 2024. 4
[42] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.
Openvid-1m: A large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 2
[43] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18444–18455, 2023. 2 [44] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. arXiv preprint arXiv:2405.20222, 2024. 2, 4
[45] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu ̈ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2
[46] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 2, 3
[47] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. 1, 2, 3, 4, 5, 6, 7 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 5 [49] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customization of textto-video diffusion models. arXiv preprint arXiv:2402.14780, 2024. 1, 2 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo ̈rn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 2 [51] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 2830–2839, 2017. 2 [52] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal motion styles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5652–5661, 2023. 2 [53] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-a-video: Single video editing with object-aware consistency. In ACML, pages 1215–1230, 2024. 2, 3 [54] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1, 2
10


[55] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. arXiv preprint arXiv:2112.14683, 2021. 2
[56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 3, 1
[57] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 3
[58] Khurram Soomro and Amir R Zamir. Action recognition in realistic sports videos. In Computer vision in sports, pages 181–208. Springer, 2015. 2, 5, 7, 1
[59] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International conference on machine learning, pages 843–852. PMLR, 2015. 2
[60] Spencer Sterling. Zeroscope. https://huggingface. co/cerspense/zeroscope_v2_576w, 2023. 1, 2, 5
[61] TempoFunk. webvid-10m. https://huggingface. co / datasets / TempoFunk / webvid - 10M / tree / main, 2023. 2
[62] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 2
[63] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In International Conference on Learning Representations, 2020. 2
[64] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526–1535, 2018. 2
[65] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. NeurIPS, 35:23371–23385, 2022. 2
[66] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. Advances in neural information processing systems, 29, 2016. 2
[67] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 1, 2
[68] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. 2
[69] Luozhou Wang, Ziyang Mai, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, and Yingcong Chen. Motion inversion for video customization. arXiv preprint arXiv:2403.20193, 2024. 2
[70] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. arXiv preprint arXiv:2306.02018, 2023. 2
[71] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2
[72] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 2
[73] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2
[74] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In ACM SIGGRAPH, pages 1–11, 2024. 2
[75] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In CVPR, pages 65376549, 2024. 2 [76] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pages 7623–7633, 2023. 2, 5 [77] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003, 2023. 2, 5, 1 [78] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn a motion pattern for few-shot video generation. In CVPR, pages 7089–7098, 2024. 2 [79] Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, and Ziwei Liu. Freeinit: Bridging initialization gap in video diffusion models. arXiv preprint arXiv:2312.07537, 2023. 2, 3
[80] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239, 2024. 1, 2
[81] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In ECCV, pages 331–348, 2025. 2, 4
11


[82] Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller. arXiv preprint arXiv:2405.14864, 2024. 2
[83] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 2
[84] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userdirected camera movement and object motion. In ACM SIGGRAPH, pages 1–12, 2024. 2 [85] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2
[86] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 4
[87] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2021. 2
[88] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In CVPR, pages 18456–18466, 2023. 2 [89] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023. 2 [90] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 3836–3847, 2023. 2 [91] Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Motioncrafter: One-shot motion customization of diffusion models. arXiv preprint arXiv:2312.05288, 2023. 2
[92] Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. 2, 5
[93] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In ECCV, pages 273–290, 2025. 1, 2, 5, 7, 8 [94] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. arXiv preprint arXiv:2305.16322, 2023. 2
12


Training-Free Motion-Guided Video Generation with Enhanced Temporal
Consistency Using Motion Consistency Loss
Supplementary Material
6. Implementation Details
Hyperparameters. For trajectory control, we use VideoCrafter [17] as the pre-trained text-to-video model and generate images at a resolution of 320 × 512, following [47]. Based on the box trajectory, we simply use the centroid point of the box as the sparse point. Additionally, the box trajectory with a black box and white background is used as the reference video to extract the motion correlation pattern M M M in Eq. 3. The initial noise and random seed are kept same in [47] and ours for a fair comparison. For reference video control, we use ZeroScope [60] as the pre-trained video model and generate images at a resolution of 384 × 384, following [93]. The initial noise is obtained by inverting the reference video (refer to Section 3.2) and adding sum-weighted random noise, as the implementation in [93]. The DDIM [56] inversion step is set to 30 to compute zT . Sparse points in the first frame are extracted through human-interactive clicks and tracked throughout the video for subsequent frames. For fair comparison, we use the same random seed in [93] and our method. By default, we set σt = 10000.0 and τ = 10.0 in Eq. 3. The frame count F is set to 16 for trajectory control-based methods and 32 for reference video-based methods. To conserve VRAM, we employ mixed precision inference using FP16. All experiments are conducted on an NVIDIA L40 graphics card with 48GB of GPU memory3. The classifierfree guidance scale is set to 12, and the number of gradient guidance steps for motion consistency is n = 50 for trajectory-based methods and n = 30 for reference videobased methods.
Quantitative comparison. For trajectory control, we utilize 56 prompts following [47], where each prompt is applied to 8 different trajectories with 2 random initial noises. This results in a total of 896 generated videos. All prompts are identical to those provided in the supplementary material of [47]. However, the trajectories and random initial noises were re-implemented by us, as they were not publicly available in the original project. The 8 trajectories are shown in Figure 7. For reference video control, we adopt the open-sourced benchmark released by the LOVEU-TGVE competition at CVPR 2023 [77], following the motion customization setting on a single video as described in [93]. The dataset includes 76 videos, with each video paired with 4 editing text
3In theory, a GPU with 32GB of memory is sufficient for all experiments in this paper
Figure 7. The 8 different box trajectories in the evaluation of trajectory control methods.
prompts focusing on object, background, style, and multiple changes. This results in a total of 304 generated videos. The additional 3 prompts with large appearance changes for each video in [93] are not available; therefore, we reimplement [93] on the original dataset.
Details of human preference. We use the Gradio4 toolbox to build a web interface that allows annotators to evaluate the generated videos across three factors:
i) Video-text alignment: Which video better matches the caption “text prompt”?
ii) Trajectory or motion alignment: Which video more closely follows or aligns with the trajectory or motion of the reference video? (Focus solely on motion information, disregarding appearance or style quality.) iii) Video quality: Which video is smoother, exhibits less flicker, and is more free from artifacts?
An example of the web interface is shown in Figure 8. We recruited 25 users with undergraduate or postgraduate degrees to participate in the human preference evaluation. For simplicity, users were asked to perform pairwise comparisons and select their preferred results between our method and the compared method with random orders. To ensure a fair comparison, we used identical random seeds across our method and the compared method, which may result in differences in local regions and details in the generated videos. Users were instructed to carefully review the videos frame by frame to identify subtle differences accurately. For trajectory control comparisons, all 896 generated videos were used. For reference video control methods, 304 generated videos were included. Among these, 292 videos were randomly selected from the LOVEUTGVE [77] benchmark, with one prompt randomly chosen from the 4 prompts available for each video. The remaining 12 videos were selected from the UCF Sport Action [58]
4Abid, Abubakar, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. “Gradio: Hassle-free sharing and testing of ML models in the wild.” arXiv preprint arXiv:1906.02569 (2019).
1


(a) Human preference on trajectory control methods
(b) Human preference on reference video control methods
Figure 8. Visualization of the website for human preference. Users are asked to compare the results pairwise and select their preferred one.
benchmark. The statistical results are presented in Table 1 and Table 2 in the main paper.
7. More Ablation Studies
Frame number in the motion pattern extraction. In the main paper, we use F = 16 for trajectory control and F = 32 for reference video control to calculate the motion correlation pattern M M M in Section 3.3 (Line 261, “The calculation is only conducted on the subsequent frames, ...”). Given a point p = (y, x) in frame f ∈ {1, 2, . . . , F }, we adjust the calculation of M M M to a smaller range of subsequent frames instead of all subsequent frames. Specifically, the calculation is conducted on subsequent frames, i.e., i ∈ {f +1, f +2, . . . , f +local}, where f +local ≤ F .
Here, we set local ∈ {1, 5, 8, 12, F − f }. If the number of subsequent frames is less than local, we use the maximum of local and the actual number of subsequent frames instead. Figure 9 shows that as local increases to 8, the motion becomes more similar to the reference video. When local increases further, the motion remains stable, while the appearance details improve slightly, such as the lion’s tail and claws. By default, we set local = F − f .
Sparse point selection. In the main paper, we select a single point for motion guidance: the centroid of the box trajectory for trajectory control methods and a humaninteractive click for reference video control methods. Here, we study the influence of increasing the number of selected points. Figure 10 demonstrates that with our proposed frame-to-frame consistency guidance, performance
2


Reference video & click point
No frame-to-frame consistency guidance
local = 1
local = 5
local = 8
local = 12
local = F − f
Figure 9. The local range for the calculation of the motion correlation pattern. The text prompt is “A lion is running on the road.”.
Reference video & 1 click point
Ours
No frame-toframe consistency guidance
Reference video & 2 click point
Ours
Reference video & 3 click point
Ours
Reference video & 4 click point
Ours
Figure 10. The number of sparse points selected for the calculation of the motion correlation consistency. The text prompt is “A panda is lifting weights”.
consistently improves regardless of the number of sparse points. However, increasing the number of sparse points does not lead to further performance improvements. The underlying reason is that our motion patterns are soft correlations between the selected point and other points (refer to Eq. 3). Adding more points may introduce some uncertainty or inaccurate correlations during inference with motion guidance. We leave this issue for future research. In this paper, we use a single point by default.
Reference video & click point
No frame-to-frame consistency guidance
σ! = 1000
σ! = 6000
σ! = 10000
σ! = 20000
Figure 11. The impact on the weight schedule σt when using the frame-to-frame motion consistency guidance.
A red car turning around on a countryside road, photorealistic, 4k.
Reference video & click point
Ours
Ours
A dolphin is lifting weights.
Figure 12. Failure case. The results are suboptimal when the target prompt deviates significantly from the reference video.
Impact on the weight schedule σt. In the main paper, we set the weighting schedule σt = 10000.0 in Eq. 2 when applying the frame-to-frame motion consistency guidance. Here, we ablate to study the impact of the weighting schedule σt. Figure 11 illustrates that as σt increases, the motion becomes more consistent with the reference video. Setting σt too small reduces its influence on aligning the motion with the reference video, while setting σt too large will have negative influence on the appearance details, such as the color of the lion’s hair. When σt = 10000.0, the performance achieves a relatively optimal balance.
8. Additional Results
We provide more comparison examples of trajectory control based methods in Figure 13 and reference video control based methods in Figure 14.
Limitation and failure case. Our method focuses on generating videos that simulate the motion of reference videos. However, when the target prompt deviates significantly from the reference video, the results are suboptimal. For instance, a car struggles to follow a “V” trajectory, and it is unrealistic to make a dolphin lift weights, as illustrated in Figure 12.
3


A lion running on the grasslands.
Direct
Peekaboo [26]
FreeTraj [47]
Ours
A panda surfing in the universe.
Direct
Peekaboo [26]
FreeTraj [47]
Ours
A rabbit burrowing downwards into its warren.
Direct
Peekaboo [26]
FreeTraj [47]
Ours
Figure 13. Qualitative comparison of trajectory control. We compare our method with other trajectory based approaches, i.e., Direct inference, Peekaboo [26] and FreeTraj [47]. The “Direct” means the direct inference with random noise and no other guidance. We use the same initial noises as in [47] for better visual comparison. Our method shows better ability on trajectory follow and temporal coherent consistency. (Best viewed with zoom for finer details.)
4


Reference video & click point
A robot is skateboarding in a cyberpunk city.
Motion Director [93]
Ours
A teddy bear skateboarding in Times Square New York.
Motion Director [93]
Ours
Reference video & click point
A bear is lifting weights.
Motion Director [93]
Ours
A dog is lifting weights.
Motion Director [93]
Ours
Figure 14. Qualitative comparison of reference video control. We evaluate our method and MotionDirector [93]. To ensure a fair comparison, we use the same initial noises for [47] and our method. Our method demonstrates superior motion alignment with the reference video and improved temporal coherence. (Best viewed with zoom for finer details.)
5