Nested Attention: Semantic-aware Attention Values for Concept Personalization
Or Patashnik†,§ Rinon Gal† Daniil Ostashev§ Sergey Tulyakov§ Kfir Aberman§ Daniel Cohen-Or†,§
†Tel Aviv University §Snap Research
<pet>
“... in the snow” “Watercolor
painting, jumping” “... on the beach” “... as pop figures”
<person> “... papercraft style” “...in Halloween” “... as superheroes
in comic book” “... are hiking in the
mountains”
<pet>
<person>
Figure 1. Our nested attention mechanism attaches a localized, expressive representation of a subject to a single text token. This approach improves identity preservation while maintaining the model’s prior, and can combine multiple personalized concepts in a single image.
Abstract
Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model’s prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model’s existing cross-attention layers. Our key idea is to generate querydependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.
1. Introduction
Personalization of text-to-image models [12, 22, 32, 37] enables users to generate captivating images featuring their own personal data. To introduce new subjects into the textto-image model, initial approaches conduct per-subject optimization [15, 27, 40], achieving impressive results but requiring several minutes to capture each subject. To reduce this overhead, more recent approaches train image encoders [4, 16, 17, 19, 41, 49, 51, 52, 54, 55]. These encoders embed the subject into a latent representation, which is then used in conjunction with diverse text prompts to generate images of the subject in multiple contexts. A key challenge in personalizing text-to-image models is balancing identity preservation and prompt alignment [5, 17, 19, 55]. Most encoder-based works [17, 19, 52, 53, 55] tackle personalization by encoding the subject into a large number of visual tokens which are injected into the diffusion model using new cross-attention layers. Such approaches are highly expressive and can achieve high fidelity to the subject, but they tend to overwhelm the model’s prior, harming text-to-image alignment (see Section 2). A common alternative is to tie the encoded subject to a small set of word embeddings [16, 30, 51, 54], introduced as part of the original cross-attention mechanism. This limits the impact
1
arXiv:2501.01407v1 [cs.CV] 2 Jan 2025


on the model’s learned prior, but greatly limits expressivity, reducing identity preservation. In this work, we propose a novel injection method that draws on the benefits of both approaches, employing a rich and expressive representation of the input image while still tying it to a single textual-token injected through the existing cross-attention layers. Our key idea is to introduce query-dependent subject values using a Nested Attention mechanism, comprised of two attention-layers. The external layer is the standard text-to-image cross-attention layer, where the novel subject is tied to a given text token. However, rather than assigning the same attentionvalue to this token across the entire image, we use an additional, “nested” attention layer to construct localized, query-dependent attention-values. In this nested layer, the generated image queries can attend to a rich, multi-vector representation of the novel subject, learning to select the most relevant subject features for each generated-image region. Intuitively, instead of having to encode the subject’s entire appearance in a single token, the model can now encode smaller semantic visual elements (e.g., the mouth or the eyes), and distribute them as needed during generation. This nested mechanism thus has the advantages of both prior approaches – a rich, multi-token representation, while bounding its influence to a single textual token which can be easily controlled. This not only leads to better trade-offs between prompt-alignment and subject-fidelity, but also to an increasingly disentangled representation, allowing us to combine several personalized concepts in a single image simply by using a different nested attention layer for each subject (Figure 1). Importantly, while recent encoderbased methods focus on face-recognition based features and losses [17, 19, 30, 52], our approach is general and also enhances performance for non-human domains. Moreover, it does not require specialized datasets with repeated identities, and can be trained on small sets like FFHQ [26]. We show that our approach achieves high identity preservation while better preserving the model’s prior, allowing diverse prompting capabilities. Importantly, our experiments reveal that under similar data and trainingcompute budgets, the nested attention approach outperforms common subject-injection methods like decoupled cross-attention [55], on both identity similarity and editability. Finally, we analyze the behavior of the nested attention blocks, showing that our performance can be enhanced even further by supplying multiple subject-images at test time (without re-training), and show additional applications like identity-blending and semantic subject variations.
2. Related Work
Text-to-image personalization Text-to-image personalization aims to expand a pre-trained model’s knowledge with new concepts, so that the model will be able to syn
thesize them in novel scenes following a user’s prompt [15, 40]. Initial methods achieve this goal by learning a textembedding [15] to represent the concept, or by fine-tuning the generative network itself [40]. When learning textembeddings, improved results can be achieved through careful expansions of the embedding space, for example by learning a different embedding for each denoising network layer [50], for every time-step [2, 16] or by encoding information in negative prompts [13]. For fine-tuning based methods, a common approach is to restrict tuning to specific weights [6, 7, 14, 20, 23, 25, 27, 42, 43, 45], with the aim of better preserving the pre-trained model’s prior. While these approaches are largely successful, they require lengthy training for every subject, with training times and costs only increasing as models become larger and more complicated. A few recent methods [39, 46] explore training-free personalization by mixing cross-image attention features. However, they struggle to preserve identities, and are largely limited to styles and simple objects. To overcome these challenges, considerable effort has gone into encoder-based solutions, which train a neural network to assist in the task of personalization. Our method improves on this encoder-based approach.
Encoder-based personalization Initial efforts into textto-image encoders focused on a two-step approach which first trains an encoder to provide an initial guess of a subject embedding [16, 29] or a set of adjusted network weights [4, 41]. These were then further tuned at inference time, to achieve high-quality personalization in as few as 5 steps. More recently, a long line of works sought to avoid inference-time optimization, relying only on a pre-trained encoder to inject novel concepts into the network in a feedforward manner [9, 24, 30, 44, 53, 55]. Among these, particular effort has been directed at the personalization of human faces [49, 52, 54, 56]. This domain is of particular challenge, as humans are sensitive to minute details in human faces. Hence, common approaches seek to improve identity preservation by relying on features extracted from an identity recognition network [48, 52, 55] or by leveraging an identity network as an auxiliary loss [17, 19, 34]. A common thread among these methods is the use of an additional cross-attention layer as a means to inject the encoded subject’s likeness. However, this approach commonly leads to degraded prompt adherence because the new layers draw the model away from its learned prior. Hence, such approaches commonly employ specialized datasets [17], losses [19], or significant test-time parameter tuning [55] to better enable a user to freely modify the encoded subject using text prompts. In contrast, methods that encode subjects into tokens in the existing crossattention layers [2, 16, 30, 51, 54] can more easily preserve the prior by aligning the subject’s attention masks to an existing word [45], but struggle to preserve subject identity.
2


Q
K
V
Q K V
Q K V
Q K V
Q K V
Q
K
V
Nested Attention Layer l
WV෱
CLIP
WK෱
CAl
Input Image Target Image
K෱
Q
V෰
Vq [s ∗ ]
“A <person> holding a plate of pasta”
Input Text
Q-Former
Figure 2. Method overview. The input image is passed through an encoder that produces multiple tokens to represent it. These tokens are projected to form the keys and values of the nested attention layers. The result of each nested attention layer is a new set of perquery values, V ∗
q , which then replace the cross-attention values of the token s∗ representing the subject. One nested attention layer is added to each of the cross-attention layers of the model.
Here, we propose to tackle this challenge through a novel nested attention mechanism. Instead of having to balance separate cross attention layers, we use the nested layer to compute a per-region attention-value vector, which can be injected using the existing cross-attention layers. Doing so allows us to enjoy an expressive multi-vector representation, while better preserving the model’s prior.
3. Method
Our method builds on a pretrained text-to-image diffusion model [35]. Given an input image of a specific subject and a text prompt, we generate a novel image of this subject that aligns with the prompt. To achieve this, we employ an encoder-based approach that takes the input image and converts it into a set of tokens. These tokens are then used to calculate per-query attention values using a novel nested attention layer (see overview in Figure 2). Specifically, the nested attention mechanism selectively overrides the cross-attention values associated with a target token (e.g., “person”) to which we apply the personalization, enabling the model to incorporate the unique features of the subject while adhering to the given prompt. In the following subsections, we first provide background on cross-attention layers in diffusion models. We then introduce our nested attention mechanism, which is central to our personalization approach. Finally, we describe the architecture and training process of the encoder used to generate personalized tokens from input images.
3.1. Preliminaries: Cross-Attention
Diffusion models typically incorporate text conditions into the generation process using cross-attention layers [35, 38]. Let c denote the text encoding. In each cross-attention layer l, c is projected into keys K = f l
K (c) and values
V = fl
V (c), where f l
K and f l
V are learned linear layers
parameterized by W l
K and W l
V , respectively. The input
...
...
Textual Keys Textual Values
Cross-attention output feature map
Nested Keys Nested Values
Figure 3. The nested attention mechanism. We replace the value of the token s∗ with the result of an attention operation between the query and the nested keys and values produced by the encoder, resulting in a query-dependent value.
feature map of the l-th layer of the diffusion model, denoted as φl
in(zt), is projected into queries Q = f l
Q(φl
in(zt)),
where f l
Q is another learned linear layer parameterized by Wl
Q. The output of the attention layer is formed using these queries, keys, and values. Each location in the output feature map, φl
out(zt)ij, is a weighted sum of the values, as illustrated in Figure 3. Formally, the output feature map of the attention layer is given by:
φl
out(zt) = softmax QKT
√d V.
Previous works [1, 8, 18, 21, 33, 47] have shown that each component of the attention mechanism in diffusion models serves a specific role. Consider qij, the query at spatial location (i, j). Its dot product with each of the keys measures the semantic similarity between this spatial location and the concept represented by the key. These similarity scores are used to weight the concept values, which are then added to the existing features at the query’s spatial location. Hence, the query dictates which concepts should appear in each image region, while the values control the appearance. We will build on this insight for our nested attention layer.
3.2. Nested Attention
In standard diffusion models, the same value V [s] corresponding to a specific textual token s is used to form all the features φl
out(zt)ij corresponding to s in the output feature map. For instance, when generating an image from the prompt “a person on the beach”, the value corresponding to the token “person”, f l
V (“person”), influences all tokens representing the person, regardless of their diverse appearances (e.g., mouth and hair). This means that the single token value corresponding to the word “person” must represent all the high-dimensional information about the many different intricate details of the person being generated. However, for personalization tasks, we require particularly high accuracy when generating a specific subject. Our key idea is to increase the expressiveness of the token corresponding to the subject, denoted by s∗, without overwhelming the rest of the prompt. We do so by introducing localized values that depend on the queries. These localized values can then be more specialized, representing for example the appearance of the individual’s eyes or hair, without
3


Input Generated Vq [s∗] w/o n- Vq [s∗] w/
image ested attention nested attention
“An abstract pencil drawing...”
“... holding a coffee cup in a coffee shop”
Figure 4. We visualize the values Vq[s∗] generated for a subject in two different layers, with a vanilla cross-attention, and with our nested approach. Vanilla layers use the same value to represent the subject throughout the entire image (column 3). Nested attention assigns a different subject-value per query (columns 4 and 5), encoding fine-grained semantic information.
having to represent the individual’s entire appearance in a single embedding (see Figure 4). To compute these perregion values, we propose to use another attention mechanism, which can itself link the semantic content of each region to a set of feature vectors extracted from the image (see Figure 5). We term this internal attention layer “Nested Attention”, and its output is given by:
v∗
qij = softmax qij K ̆ T
√d
!
V ̆ ,
where qij is the query vector of spatial patch (i, j) in the external cross-attention layer. K ̆ and V ̆ are the keys and values of the nested attention layer, given through linear projections parameterized by WK ̆ and WV ̆ . Finally, vq∗ij
are the query-dependent values of the personalized token s∗ at spatial index (i, j) (i.e., corresponding to qij). These are then used in the external cross-attention layer through:
φl
out(zt)ij = softmax qij KT
√d Vqij ,
Vqij [s] =
(
vq∗ij , if s = s∗
V [s], otherwise,
where s are the prompt’s textual tokens. Through this twostage attention mechanism, we allow the model to benefit from a rich-multi token representation of the image, while still tying all features to a single prompt token. The full nested attention mechanism is illustrated in Figure 3.
Regularizing Vq[s∗]
Vq [s∗ ]
Vq[s∗] Prior work [2, 45] has shown that personalization approaches that tie the novel subject to an embedding in the existing cross-attention layers can suffer from “attention overfitting”, where the new token draws the attention from all image queries, leading the rest of the prompt to be ignored. Our approach aims to avoid this pitfall by predicting only attention values, while preserving the original keys assigned to the un-personalized word.
Input image Vq[s∗] (nested Nested attention map (qK ̆ ) attention output)
Generated image Q-Former attention map for the token with highest attention in the nested attention map
Figure 5. Analyzing the query-dependent values (Vq[s∗]) from a nested attention layer. For three queries of the generated image (purple, orange, blue points), we first show their attention maps in a nested attention layer (graph). There, each point corresponds to a token produced by the encoder. In each graph, 1-2 tokens dominate the attention. To analyze the information encoded in the most dominant token, we show the Q-Former attention map of its corresponding learned query. These show the semantic alignment between the probed query, and the source of values assigned to it.
However, we note that this property can break if the norm of values generated by the nested attention, Vq[s∗], is significantly higher than that of the original cross-attention values V [s∗] obtained from the text embedding. Indeed, increasing the norm of Vq[s∗] resembles the case where the attention given to s∗ is higher. To avoid this issue, we regularize the norm of each of the learned values vq∗ij in Vq[s∗] to be
α|V [s∗]| where α is a fixed hyperparameter, and |V [s∗]| is the norm of the cross-attention value of the un-personalized word. In our experiments, we set α = 2. Ablations on this choice are provided in Appendix C.
3.3. Encoder for Personalization
To personalize the text-to-image model, we incorporate nested attention layers into all of its cross-attention layers while keeping the original model’s weights frozen during training. We train an encoder that produces tokens from which the keys and values of the nested layers are derived. An overview of this architecture is shown in Figure 2. The encoder’s backbone is based on CLIP [36]. Given an input image, we pass it through CLIP and extract tokens from its last layer before pooling. These tokens are then processed by a Q-Former [29], where the number of learned queries of the Q-Former determines the number of nested keys and values. During training, CLIP remains frozen while the Q-Former is trained from scratch. For training the encoder and nested attention layers, we utilize datasets consist of (input image, text prompt, target image) triplets. For human faces the target image is an in-the-wild image of the person and the input image is the cropped and aligned face. For pets, the input and target
4


Figure 6. Attention maps between Q-Former learned queries and input image features. Each column shows a distinct query’s attention map, illustrating how queries capture different facial features.
images are identical. In each triplet, the text prompt describes the target image, where we replace the word related to the subject (e.g., “girl”) with the token s∗, which is set to “person” for human faces, and “pet” for pets. The training procedure follows that of diffusion models: we add noise to the target image and then predict this noise using the diffusion model conditioned on input image. This approach allows our model to learn personalized representations while maintaining the prior of the original diffusion model.
4. Analysis
What does the Q-Former learn? We begin by examining the features learned by the Q-Former component of the encoder. To do so, we visualize the attention maps between each learned query and the input image features. Figure 6 displays attention maps for five sample learned queries across two different input images. The figure demonstrates that each learned query captures distinct semantic facial features. For instance, the leftmost column’s query focuses on the eyes, while the rightmost column’s query captures the nose. In the second column, the query attends to part of the glasses in the top image. Notably, the man in the bottom row does not wear glasses, resulting in a less meaningful attention map for this query with that particular input image.
The importance of query-dependent values To achieve accurate identity preservation, nested attention layers generate query-dependent values, Vq[s∗], for the personalized subject token, s∗. These query-dependent values, enhance identity preservation by encoding fine-grained details from the input image. Figure 4 visualizes these values for two prompts, using the same input image. We show Vq[s∗] from two different layers: at a 32 × 32 resolution, and at 64 × 64. These are captured at two-thirds of the way through the full denoising process. For clarity, we show the values of s∗ from a standard cross-attention layer without nested attention, which remains constant across the image. The visualizations show that our generated values are context-aware and capture the intricate visual details of the input image. Finally, we show the semantic connection between the localized attention-values produced by the nested attention layer, and the input image. In Figure 5, we select three lo
cations on the generated image: the eye, nose, and arm. For each of these queries, we show the attention map of a single nested attention layer. As can be seen, there are typically 12 dominant encoder tokens for the each query location. We can then follow these tokens to their source in the input image, using the approach of Figure 6. As can be seen, the information encoded in the query-dependent value corresponding to the generated eye and nose mostly comes from the eye and nose regions of the input image, respectively. This indicates that the query-dependent values produced by our nested attention layer contain relevant, localized information matching the semantics of the input image. When considering the arm region, the input image does not contain an area with matching semantics, but we can observe that the model focuses on the neck region (and the boy’s shirt), and partially on skin areas on the boy’s face.
5. Experiments
Implementation details We implement our method with SDXL [35] which generates 1024 × 1024 images. The encoder is trained in two phases: 100 epochs on resolution of 512, and then additional 100 epochs on resolution of 1024. We train the human face model on FFHQ-Wild [26], and the pets model on a combination of datasets [10, 31]. Additional implementation details are provided in Appendix A.
5.1. Qualitative Results
We first show qualitative results generated by our model for both the humans and pet domains (Fig. 7). Our method accurately preserves the identity of the subject while adhering to prompts ranging from clothing and expression changes, to scenery modifications and new styles. The initial sampled noise is fixed across each column, leading to consistent composition, colors, and background that demonstrate our method’s ability to preserve the model’s prior.
Controlling identity-editability tradeoff Since our approach attaches the personalized concept to a single textual token, we can easily adjust the attention it gets during inference time. This can be used to control the tradeoff between identity preservation and prompt alignment, in a similar manner to the adapter-scale commonly used in decoupledattention methods [55]. Specifically, we adjust the subject’s attention as follows:
QKT [s∗] = max(QKT [s∗], λQKT [s∗]),
where KT [s∗] is the special token’s key, and λ is the hyperparameter that controls the tradeoff. We use max operation because attention value before applying softmax can be negative, and we do not want to further reduce the subject’s attention. Figure 8 shows the effect of varying λ.
5.2. Comparisons
Image injection mechanism At its core, nested attention is a method for injecting reference image features into a pre
5


Input “Firefighter” “Watercolor, “Laughing, in holding flower” the park”
Input “In the super- “Oil Painting, “Looking outside market with a cart” running, meadow” a car window”
Figure 7. Qualitative results of our method trained on human faces (left) and pets (right). The sampled noise is fixed across each column.
Input λ = 1 λ = 2 λ = 3 λ = 4
“A cubism painting of a person”
“A high quality portrait photo of a pet as a chef in the kitchen”
Figure 8. By manipulating the attention given to the personalized token, we control the identity-editability tradeoff. λ denotes the factor in which we increase the attention to the special token.
trained text-to-image model through its cross-attention layers. To show the benefit of this approach, we first compare it with other feature injection methods. We consider four alternative mechanisms. First, IPAdapter’s [55] decoupled cross-attention (CA) mechanism, where text and image features are processed through parallel cross-attention layers that share queries and their outputs are summed. The second mechanism, known as ‘Simple Adapter’ [55], concatenates image features with text tokens in existing cross-attention layers, requiring no additional parameters beyond the encoder. The third alternative, which we call ‘Global V ’, explores the importance of query-dependent values. There, the special token’s value is set to the mean of the encoder-produced tokens, projected through a per-layer projection matrix. This approach results in an identical value being used for all subject-queries. The final mechanism, termed ‘Multiple Tokens’, demonstrates the significance of the nested mechanism by replacing the subject’s token in the prompt with one token for each of the encoder’s outputs (i.e. the number of Q-Former tokens). To avoid attention-overfitting [45], we fix the keys of these tokens to the key of the original subject, but maintain a different encoder-produced value for each.
Importantly, we conduct these comparisons while maintaining consistent experimental conditions across all relevant parameters. Specifically, each method uses an identical Q-Former encoder architecture with the same number of learned queries, trained from scratch using the same dataset and number of training epochs. For all methods, we conduct only the first training stage (512 × 512 training resolution). In Figure 9 we show a qualitative comparison of the two most performant approaches – ‘Nested Attention’ and ‘Decoupled CA’. For the results of other methods, see Appendix B. For each method, we display results using three inference-time hyperparameters that balance identity preservation and prompt alignment. For nested attention, λ is the attention factor detailed in Section 5.1. For decoupled CA, λ is the scale parameter introduced in IP-Adapter [55]. Our results demonstrate that nested attention achieves better balance, providing superior identity preservation while maintaining better alignment with the text prompt. In Figure 10 we show a quantitative comparison of the four different methods. We measure text similarity using CLIP [36], and ID similarity with a face recognition model [3, 28]. For nested attention, global V and multiple tokens, we use λ values of 1.0, 1.5, 2.0, 2.5, 3.0, 4.0. For decoupled CA we use λ values of 0.5, 0.6, 0.7, 0.8, 0.9, 1.0. As illustrated in the graph, nested attention provides the best trade-off between identity preservation and text alignment.
Comparison to prior work Next, we compare our model to other recent face personalization models, including two versions of IP-Adapter [55] (IPA-Face and IPA-CLIP), InstantID [52], PulID [19], PhotoMaker [30], and LCMLookahead [17]. Qualitative and quantitative results are presented in Figure 11, and user study results are in Table 1. Note that while our method was trained solely on FFHQ, all other methods were trained on larger datasets, some consisting of tens of millions of images (compared with FFHQ’s 70, 000). Additionally, most of these baselines are
6


Input ←− Varying λ −→ Input ←− Varying λ −→
Decoupled CA
Nested Attention
“An abstract ink drawing of a person” “A high quality portrait photo of a person in the forest during fall”
Decoupled CA
Nested Attention
“A high quality photo of a person as an astronaut” “A watercolor painting of a person laughing, he is wearing a hat”
Figure 9. Comparing nested attention with decoupled cross attention. λ balances between identity preservation and prompt alignment. We use the following λ values from left to right (top two rows). Decoupled CA: 0.5, 0.6, 1.0, nested attention: 1.0, 2.0, 4.0. Our method achieves better identity preservation while being aligned with the text prompt.
Figure 10. Quantitative comparison of various personalization injection mechanisms. All models were trained from scratch under the same setting, with a resolution of 512 × 512.
specifically designed for human faces, using face-identity detection networks for feature extraction or as a loss. Our approach is more general, and can be applied to different domains. To better differentiate the results, we mark methods that utilize a CLIP-encoder with a full marker, and those that use a face-detection network as a feature extractor are shown with an unfilled marker in the graph of Figure 11. Our method outperforms all other CLIP-based encoder methods in both automatic metrics and throughout the user study. Notably, it does so when training on the comparatively small FFHQ dataset, without specialized data or losses. When considering the identity-network based approaches, we note that those that preserve the input face landmarks (e.g., InstantID) show artificially inflated identity scores [17] and a user study finds our identity preservation comparable, but with better prompt alignment and higher overall preference. This is particularly noticeable
in prompts that require changes to pose and expression (Figure 11, rows 1 & 4). Similarly, our approach significantly outperforms IPA-Face in user evaluations. PulID outperforms our approach across both identity similarity and prompt-alignment, but we note that it was trained on roughly a million curated images, uses both an identity network backbone and an identity loss which limit its extension to other domains, and proposes ideas which are largely orthogonal to our own, and could likely be merged with them.
Multiple subjects comparison Our method can generate images with multiple personalized subjects (Figure 1). For each domain, we run its own encoder and use its own nested attention layers to calculate the localized attention-values associated with its matching subject word (e.g., “person” and “pet”). The subject-specific values are injected into the original cross-attention layers. As demonstrated, our approach effectively handles the generation of images with both a person and a pet subject, without requiring additional training, specialized components or adjustments. However, generating multiple subjects from the same domain remains challenging due to overlapping attention maps and selfattention leakage between subjects [11].
Comparing multi-subject generation with IP-Adapter [55], the only strong baseline supporting non-face images, our approach shows superior identity preservation and prompt adherence when combining people and pets (Figure 12). This is in part because the decoupled crossattention approach is global, and its injected features can influence the entire image rather than the subject’s regions.
7


Input IPA-Face InstantID PhotoMaker Lookahead PulID Ours
Smiling at
her birthday
Watercolor pai
nting, sideview
Pointillism
Sticker, stick
ing tongue out
Figure 11. Left: qualitative comparison of human faces personalization methods. Our method successfully changes expressions and pose while preserving the identity. Right: quantitative comparison. Methods that use CLIP as their backbone encoder are marked with filled markers, while methods that build on face recognition models as their backbone encoder are marked with unfilled markers.
Table 1. User study results. We show winrate of our method in user preference against each method.
Metric IPA-Face InstantID Lookahead PulID
Prompt adherence 65% 68% 55% 42% ID similarity 86% 47% 52% 50% Overall preference 71% 66% 56% 39%
5.3. Additional Results
Here we show additional results. Additional results and applications are shown in the Appendix.
Multiple input images When multiple images of a subject are available, our approach can be improved even further, without any re-training or architecture changes. This can be particularly useful when capturing a subject’s identity from a single image is challenging due to occlusions or ambiguity. To leverage multiple input images, we encode each image separately and concatenate the resulting tokens. These tokens are then used as the input to the nested attention layers. Figure 13 shows an example of combining multiple input images. Consider for example the leftmost input image, where it is ambiguous whether the orange part of the dog is part of the leg or the main body. Similarly, the dog’s eyes appear smaller in the second column, and in the third column its fur appears shorter, with larger ears. By providing all input images to the encoder at the same time, the model is able to better capture the full identity of the dog, resulting in a higher-quality final output compared to using any single input image alone.
6. Conclusions
We introduced nested attention, a novel identity injection technique that provides a rich subject representation within
“... digital art” “... pointillism”
Person input Pet input Ours IPA Figure 12. Multi-subject generation comparison.
Input
“in a living room”
Figure 13. Using multiple images of the same concept increases the subject’s fidelity in the generated image.
the existing cross-attention layers of the model. It is based on two key principles: (i) modifying only the attention value of the subject token while keeping keys and other values unchanged, and (ii) making the subject token’s attention value dependent on the query, i.e., assigning the subject a different value for each image region. In this sense, nested attention can be interpreted as an IP-Adapter that anchors the subject’s encoding to a single textual token. This design better preserves the model’s prior, while enabling a detailed and accurate representation of the subject.
Future work could explore adaptations of nested atten
8


tion to other tasks, such as subject-conditioned inpainting or style transfer. Another promising direction involves extending the encoder to a domain-agnostic approach, which could tackle subjects from unseen classes. Finally, since IPAdapter’s decoupled-attention mechanism is a core component of many recent personalization encoders, we hope replacing it with our approach could boost their performance.
Acknowledgment
We thank Ron Mokady, Amir Hertz, Yuval Alaluf, Amit Bermano, Yoad Tewel, and Maxwell Jones for their early feedback and helpful suggestions.
References
[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar AverbuchElor, and Daniel Cohen-Or. Cross-image attention for zeroshot appearance transfer, 2023. 3 [2] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. A neural space-time representation for textto-image personalization. ACM Transactions on Graphics (TOG), 42(6):1–10, 2023. 2, 4 [3] Mohamad Alansari, Oussama Abdul Hay, Sajid Javed, Abdulhadi Shoufan, Yahya Zweiri, and Naoufel Werghi. Ghostfacenets: Lightweight face recognition model from cheap operations. IEEE Access, 11:35429–35446, 2023. 6 [4] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano. Domainagnostic tuning-encoder for fast personalization of text-toimage models. In SIGGRAPH Asia 2023 Conference Papers, pages 1–10, 2023. 1, 2 [5] Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, and Ariel Shamir. Palp: Prompt aligned personalization of text-toimage models. arXiv preprint arXiv:2401.06105, 2024. 1 [6] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers, New York, NY, USA, 2023. Association for Computing Machinery. 2 [7] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. The chosen one: Consistent characters in text-toimage diffusion models. arXiv preprint arXiv:2311.10093, 2023. 2 [8] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22560–22570, 2023. 3 [9] Wenhu Chen, Hexiang Hu, YANDONG LI, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W. Cohen. Subject-driven text-to-image generation via apprenticeship learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2
[10] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 5, 12
[11] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multi-subject text-to-image generation, 2024. 7 [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780–8794, 2021. 1
[13] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 2
[14] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora. In European Conference on Computer Vision, pages 181–198. Springer, 2025. 2 [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 1, 2 [16] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):1–13, 2023. 1, 2
[17] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Lcmlookahead for encoder-based text-to-image personalization, 2024. 1, 2, 6, 7 [18] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. 3
[19] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. arXiv preprint arXiv:2404.16022, 2024. 1, 2, 6 [20] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 7323–7334, 2023. 2 [21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022. 3 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 1 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021. 2 [24] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. 2
9


[25] Maxwell Jones, Sheng-Yu Wang, Nupur Kumari, David Bau, and Jun-Yan Zhu. Customizing text-to-image models with a single image pair. arXiv preprint arXiv:2405.01536, 2024. 2
[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4401–4410, 2019. 2, 5, 12
[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv, 2022. 1, 2
[28] Leondgarse. Keras insightface. https://github.com/ leondgarse/Keras_insightface, 2022. 6
[29] Dongxu Li, Junnan Li, and Steven CH Hoi. Blipdiffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023. 2, 4
[30] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 2, 6, 12
[31] Ron Mokady, Michal Yarom, Omer Tov, Oran Lang, Michal Irani Daniel Cohen-Or, Tali Dekel, and Inbar Mosseri. Self-distilled stylegan: Towards generation from internet photos, 2022. 5, 12
[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1
[33] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings, page 1–11. ACM, 2023. 3
[34] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: A versatile portrait model for fast identity-preserved personalization. arXiv preprint arXiv:2312.06354, 2023. 2
[35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu ̈ller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 3, 5
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR, 2021. 4, 6
[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1
[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo ̈rn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 3 [39] L Rout, Y Chen, N Ruiz, A Kumar, C Caramanis, S Shakkottai, and W Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control. 2024. 2 [40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022. 1, 2 [41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. 1, 2 [42] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. https : / / github . com / cloneofsimo/lora, 2023. 2
[43] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422–438. Springer, 2025. 2 [44] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2 [45] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1–11, 2023. 2, 4, 6 [46] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation, 2024. 2 [47] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921–1930, 2023. 3 [48] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by fine tuning an image generation model on a single image. arXiv preprint arXiv:2210.09477, 2022. 2
[49] Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning a text-toimage model on a face. In SIGGRAPH Asia 2023 Conference Papers, New York, NY, USA, 2023. Association for Computing Machinery. 1, 2 [50] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 2
[51] Kuan-Chieh Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, and Kfir Aberman. Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation, 2024. 1, 2 [52] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving gener
10


ation in seconds. arXiv preprint arXiv:2401.07519, 2024. 1, 2, 6 [53] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15943–15953, 2023. 1, 2 [54] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fr ́edo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. arXiv, 2023. 1, 2 [55] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1, 2, 5, 6, 7, 12 [56] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting anybody in diffusion models via celeb basis. arXiv preprint arXiv:2306.00926, 2023. 2
11


Appendices
A. Implementation Details
Method We train the human face model on FFHQWild [26], where the encoder’s input image is aligned and cropped, and the target image is the full in-the-wild image. For the pets model we use SDXL to generate a synthetic data of 50, 000 pet images, where the background in these images is white. Additionally, we use 15, 000 images from AFHQ [10] and 1, 000 dog images from [31]. The human face model has 1024 learned queries in the Q-Former, while the pets model has 256. We find that better results for combining a person and a pet in the same image are obtained by segmenting the pet, and setting a white background. All models are trained on NVIDIA A100 80GB GPUs. The face model uses 4 GPUs and a total batch size of 32 in the first phase, and 8 GPUs with a total batch size of 16 in the second phase. For the pets model, both phases are trained on 8 GPUs with total batch sizes of 128 and 32 respectively.
User Study The results of our user study are provided in the main paper. A total of 22 participants took part, each evaluating 12 tuples consisting of an input image, an input prompt, our method’s result, and a result from one of the other methods. For each tuple, participants were asked three questions: (1) which output image is better aligned with the prompt, (2) which output image better preserves the identity of the input image, and (3) which result is better overall. This setup yielded 264 responses for each question type.
B. Additional Results
Additional results of our method are presented in Figures 19 and 20.
Identities Mixing Following prior works [30], our method enables mixing between two identities, as illustrated in Figure 14. To achieve this, we use our trained encoder to independently encode each image and concatenate the resulting tokens. These concatenated tokens are then fed into the nested attention layers. This approach is analogous to the technique used for handling multiple images of the same subject, except that in this case, the images represent different subjects.
Semantic Variations When training our face model, we attach the concepts to the token person. In Figure 15 we show that, similarly to prior embedding-based personalization encoders [30], our method supports semantic variations
Input 1 Input 2 “person” “child”
Input 1 Input 2 “person” “girl”
Figure 14. Our method allows mixing two identities by encoding them separately, and pass the concatenated representations to the nested attention layer.
Input “person” “woman” “child”
Input “person” “man” “girl”
Figure 15. By simply changing at inference time the token to which we inject the personalized concept (e.g., “person” to “woman”) we can have various semantic variations.
by changing the textual token to which we attach the subject during inference time.
Image injection mechanism comparison Qualitative results of all baseline injection methods are presented in Figure 16. As can be seen in the qualitative results, the ‘Simple Adapter’ method struggles to faithfully adhere to the input prompt, with notable deviations in style, expression, and clothing. The method’s approach of concatenating a large number of image tokens with textual tokens appears to disproportionately distribute attention, prioritizing image tokens at the expense of textual tokens. This limitation is also evident by the quantitative evaluation. In IP-Adapter [55], it has been shown that using a small amount of tokens with this approach leads to poor identity preservation. In the ‘Multiple Tokens’ method, the generated images adhere to the text prompt, but the identity preservation is poor. In this method, all the image tokens get the same amount of attention, and they are not query dependent. Having such a large amount of tokens that should together en
12


Input ←− Varying λ −→ Input ←− Varying λ −→
Simple Adapter
N/A N/A N/A N/A
Decoupled CA Multiple Tokens
Nested Attention Global V
“A watercolor painting of a person smiling, he is wearing a hat” “A high quality photo of a person as an astronaut”
Figure 16. Qualitative comparison of injection mechanism. λ balances between identity preservation and prompt alignment. We use the following λ values from left to right. Decoupled CA: 0.5, 0.6, 1.0, global V , multiple tokens and nested attention: 1.0, 2.0, 4.0.
code the information about the input image make the optimization process difficult. This method captures attributes such as gender and hair color, but the identity preservation is overall poor.
Decoupled Cross-Attention struggles to adhere to some text prompts, especially when they consist of nonphotorealistic styles. The summation of the image-crossattention with the text-cross-attention allows the model to generate image features that overwhelm the features coming from the text.
In ‘Global V ’, averaging the tokens produced by the image encoder results in values that do not depend on the queries and hence struggle to convey all the facial details of the specific identity. The optimization in this method, however, is easier than the one in ‘Multiple Tokens’.
Overall, our method achieves the best tradeoff between identity preservation and prompt alignment. This is evident both in the qualitative and quantitative results.
C. Ablation Studies
Number of learned queries Here, we ablate the effect of the number of learned queries in the Q-Former on model performance. Since the number of learned queries determines the number of nested keys and values, increasing them leads to a richer image representation. In Figure 18, we present results from models trained with varying numbers of learned queries. The bottom of the figure shows the average ID score computed across different prompts on our test set. All models undergo only the first training phase at a resolution of 512. Both qualitative and quantitative results demonstrate that a higher number of learned queries enhances identity preservation and captures subtle identity features more accurately. Note that the ID scores shown here are lower than our final model’s scores, as these results reflect performance after only the first training phase.
Normalizing Vq[s∗]
Vq [s∗ ]
Vq[s∗] Figure 17 demonstrates the importance of regularizing Vq[s∗]. All models shown were trained with 256 learned queries and underwent only the first training phase (at a resolution of 512). Without regularization,
13


Input w/o reg. α = 1 α = 2 α = 3
“Wearing a headset”
“In a coffee shop”
Figure 17. Ablating the regularization performed on Vq[s∗]. Without normalization, the image looks red and contains artifacts. Setting α = 2 provides a good balance between identity preservation and prior preservation.
Input 16 64 256 1024
“Hiking on a mountain”
“In a living room, reading a book”
ID Score 0.299 0.318 0.302 0.363
Figure 18. Results of models trained with varying number of learned queries. Increasing the number of learned queries improves identity preservation. All models used in this figure underwent only the first phase of training.
the input image dominates the output, resulting in poorer prior preservation. Additionally, unregularized models produce images with reddish tints and visible artifacts (see second column of Figure 17). We further ablate the choice of the regularization constant α (Section 3.2 in the main paper). With α = 1, artifacts are eliminated but identity preservation suffers. At α = 3, while identity preservation improves, prior preservation slightly degrades. We find that α = 2 offers a good balance between identity preservation, image quality, and prior preservation.
14


Input “Bar tender” “Cyborg” “Grafiiti” “Pop figure” “Wearing a suit” “Carnival”
Figure 19. Additional results on human faces. The initial noise is fixed across each column.
15


Input “pencil drawing” “bike” “digital art” “forest” “garden” “helicopter”
Figure 20. Additional results on pets. The initial noise is fixed across each column.
16