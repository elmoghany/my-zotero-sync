DocVideoQA: Towards Comprehensive
Understanding of Document-Centric Videos through
Question Answering
Haochen Wang Peking University Beijing, China wanghaochen326@stu.pku.edu.cn
Kai Hu
University of Science and Technology of China Hefei, China hk970213@mail.ustc.edu.cn
Liangcai Gao Peking University Beijing, China gaoliangcai@pku.edu.cn
Abstract—Remote work and online courses have become important methods of knowledge dissemination, leading to a large number of document-based instructional videos. Unlike traditional video datasets, these videos mainly feature rich-text images and audio that are densely packed with information closely tied to the visual content, requiring advanced multimodal understanding capabilities. However, this domain remains underexplored due to dataset availability and its inherent complexity. In this paper, we introduce the DocVideoQA task and dataset for the first time, comprising 1,454 videos across 23 categories with a total duration of about 828 hours. The dataset is annotated with 154K questionanswer pairs generated manually and via GPT, assessing models’ comprehension, temporal awareness, and modality integration capabilities. Initially, we establish a baseline using open-source MLLMs. Recognizing the challenges in modality comprehension for document-centric videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances unimodal feature extraction with diverse instruction-tuning data and employs contrastive learning to strengthen modality integration. Through fine-tuning, the LLM is equipped with audio-visual capabilities, leading to significant improvements in document-centric video understanding. Extensive testing on the DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing models. We’ll release the code and dataset to facilitate future research. Index Terms—Video Qustion Answering, Document Understanding, Multi-model Large Language Model, Dataset
I. INTRODUCTION
The task of question answering (QA) has long served as a fundamental way for assessing the capacity of machines to understand content across different types. As its core, QA systems aim to automatically answer questions posed by humans, by identifying and extracting relevant information from various media such as text, images, and videos. With advancements in technology, the field of QA has evolved from addressing questions in a single modality to tackling multi-modal questions, including Video Question Answering (VideoQA [1]). Additionally, the scope of QA has evolved from simple factoid-based questions [2] to more complex, context-dependent questions [3] that require deep understanding and reasoning over extensive documents. The rise of online education and remote work has led to a rapid increase in
*Corresponding Author
document-centric instructional videos on the internet, making these videos significant medium for knowledge dissemination. However, the investigation of this video category remains constrained due to the unavailability of specialized datasets. Consequently, as illustrated in Fig 1, building upon the foundational concept of DocVideoQA, we introduce a new dataset to facilitate the exploration of this novel QA paradigm. While QA systems have greatly diversified in terms of content input, ranging from short-text snippets [4] [5] [6] to long documents [7], and from purely textual data to mixed media such as images (VQA [8], ChartQA [9], TextVQA [10]), document images (DocVQA [11]) and videos (VideoQA [1]), current research has not adequately focused on the task of Document Video QA. This task, closely related to VideoQA and DocumentQA, encounters unique challenges. In the realm of VideoQA, most studies focus on aspects such as action recognition, sentiment analysis, or event detection, primarily analyzing dynamic visual elements in natural scene videos.In contrast, document videos have distinct characteristics. They contain rich text and intricate layout relationships, as well as audio content covering a wide range of topics and specialized terminology, especially when closely linked to visual information. Therefore, models need to possess advanced capabilities in text and layout analysis, as well as the ability to interpret complex audio-visual interactions. On the other hand, DocumentQA research has progressed from basic text extraction using OCR to the more complex DocVQA task, which combines document layout analysis with visual modalities. However, the field still lacks a task specifically focused on examining document content embedded within the intricate medium of video. The incorporation of document content in videos, along with the integration of temporal dimensions and speech modalities, presents novel challenges in effectively extracting and synthesizing features across different modalities for comprehensive understanding. In various scenarios, a pronounced need emerges for the capacity to interpret document videos. Within the educational domain, this facilitates the creation of customized review assistance systems tailored to online courses; in the realm of remote work, it enables the swift comprehension of meeting
arXiv:2503.15887v1 [cs.CV] 20 Mar 2025


A: Lindsay Wilson
Q: What's the speaker’s name
A: Power Hour and Year Over Year Data
Q: What's the topic of this video
A: Total position is $ 256,719.1
Q: What's the total position in 2022 Sunday 3:00 pm
A: They're embracing and it promotes intimacy
Q: What's the action and its benefits in the picture?
Single-Page KIE (Visual & Audio)
Multi-Page KIE (Visual)
Table Understanding (Visual & Audio)
Image Understanding (Visual & Audio)
Input
Output
Fig. 1. An illustrative example of DocVideoQA: a video on Fundraising Pushes challenges models with tasks such as information extraction, multi-page content comprehension, and visual-audio understanding, necessitating a multidimensional analysis of the video.
highlights, augmentation of omitted details, and summarization of discussions. To advance this field of research and tackle the previously mentioned challenges, we introduce a novel DocVideoQA dataset to alleviate the issue of data scarcity. Given that slide decks are one of the most efficient document types that arrange visual and textual elements for communication, our dataset comprises 1,454 explanatory videos primarily featuring slides, covering 23 distinct fields. We annotated these videos with 154K pairs of questions and answers, utilizing both human and GPT-generated methods. Our question design spans multiple dimensions (specific dimensions) to comprehensively assess models’ understanding of video content. Furthermore, to more effectively extract and integrate features from visual and speech modalities, we developed a robust multi-modal language model benchmark—DV-LLaMA. By employing a progressive training paradigm, our model effectively aligns video content with language, enhancing comprehension abilities by approximately 20%. Extensive experiments conducted on the DocVideoQA dataset demonstrate that our model achieves the best results in the proposed task. The main contributions of this paper can be summarized as follows:
• We are the first to present the Document Video Question Answering task, designed to answer questions derived from Document-Centric Video. • To support research, we introduce a new dataset, which consists of 1,454 document-centric videos across 23 domains, annotated with 154K question-answer pairs. We’ll release the code and dataset to facilitate future research. • We developed DV-LLaMA, a Multi-Model Large Language Model, enhancing understanding of DocumentCentric Video through improved feature extraction and integration from visual and audio modalities via progressive training.
TABLE I
COMPARISON OF VIDEOQA DATASETS
”MC/OE”: ”MULTICHOICE/OPEN-ENDED”. ”F/S”: ”FULL/SEGMENTAL”.
Dataset Topic Video n QA n Annotation Task Duration(s)
MSVD-QA [12] General 2K 51K Auto OE 10 NExt-QA [13] General 5.4K 52K Man OE&MC 44 TGIF-QA [14] Gif 72K 165K Auto&ManOE&MC 3 MovieQA [15] Movie 408 15K Man MC 203 Social-IQA [16] Social 1K 8K Man MC DramaQA [17] Drama 24K 16K Auto&Man MC 3.7 Sports-QA [18] Sports 6K 94K Auto&Man OE 21
DocVideoQA Document 1.5KF
154K Auto&Man OE 1,380F
75KS 35S
II. DOCVIDEOQA DATASET CONSTRUCTION
We built the DocVideoQA dataset from two main sources: the SlideSpeech dataset [20], a large-scale audio-visual corpus enriched with slides from multiple domains, and videos from SlidesLive1, which consist of academic conference presentations characterized by their high level of professionalism and extensive use of domain-specific vocabulary. From these sources, we selected videos in English that predominantly featured slides for our dataset. For supervised model training and generation of question-answer pairs, we meticulously annotated the video content. Visually, following the SlideSpeech paper, we annotated the timestamps for each slide in the video and utilized the TD [36] and OCR [37] models from the MMOCR [38] toolkit to extract OCR results and restore the correct reading sequence of the text for each slide. For audio, we utilized Whisper [21] and manual correction to transcribe the audio content of the video into high-confidence text.
1https://slideslive.com/


Large Language Model LoRA
DocVideoQA Training Data
Visual Encoder Audio Encoder
Visual Projection Audio Projection
Contrastive Learning
Text Prompt & Question
Stage 1 : Visual Data: DocVQA, OCR-VQA, WTQ, ChartQA Audio Data: Speech-to-text
Stage 2 :
Stage 3 :
Visual
Audio
Contrastive Learning
Training Data
Fig. 2. Overview of DV-LLaMA and stage-wise training
In the question-answer pair annotation process, we classified questions into three categories: Information extraction, Content comprehension, and Temporal awareness. This classification requires the model to comprehend visual content, including text, tables, and images, integrate audio information for multimodal understanding, and address crosspage temporal challenges. Some question-answer pairs were generated using GPT-4 [33] in conjunction with the annotated visual and audio text. Additionally, other pairs were manually annotated, particularly focusing on visual content within the video, such as images and tables, that OCR could not capture. As depicted in Table I, we annotated 1,454 videos and 154K question-answer pairs across 23 domains, with an average video duration of 23 minutes. Considering the limitations of existing MLLMs in processing long videos, we segmented the videos into sub-videos, each containing 1-3 slides. This resulted in a total of 74,000 sub-videos, with an average duration of 35 seconds each. Our dataset, compared to existing ones, offers a richer variety in the number and types of videos and QA pairs, features longer video lengths, and introduces previously untouched document scenarios.
III. METHOD
Document videos contain rich audio-visual information, offering detailed explanations of images with text. This requires strong unimodal feature extraction and an understanding of cross-modal relationships for effective integration. As shown in Fig 2, inspired by the Video-LLaMA model, we designed our model DV-LLaMA with two branches: Vision-Language and Audio-Language. We also propose a multi-stage training approach, using instruction data and contrastive learning to progressively tackle the challenges of feature extraction and integration in document videos.
A. Single-Branch Feature Enhancement Training
At this stage, we addressed the model’s limitations in processing document videos by constructing instruction data and training each branch separately to improve feature extraction. For the visual branch, given the rich textual content in document images, we leveraged open-source datasets such as OCR-Based DocVQA [11], OCR-VQA [22], SlideVQA [23], and TextVQA [24]. These datasets not only support visual question answering but also help restore the reading order of text, enhancing the model’s text comprehension. For the audio branch, as audio features are ultimately fed into the LLM for understanding, we focused on Speech-to-Text datasets [35] to improve transcription accuracy and ensure information completeness. During training, we froze the visual and audio encoders and trained a Q-Former [33] to compress and project tokens, which were then combined with text tokens for processing by the frozen LLM.
B. Contrastive Learning Alignment Training
In the second stage, our objective is to align content between visual and audio modalities by learning their interrelations. Given that audio segments in document-centric videos typically serve as explanations or supplements to the visual content, and both modalities share the same contextual semantic meanings, we leverage contrastive learning to facilitate the alignment of representation spaces across modalities. Assuming a batch size of B during the training process, a visual segment and its temporally aligned audio segment are considered as a positive pair, while pairs formed with other audio segments within the same batch are treated as negative examples. For each positive pair (v, a) from the distinct data modalities V and A, an image Vi is encoded into eiv by the vision encoder, and an audio Ai is encoded into
eia by the audio encoder. We select B − 1 audio embeddings from the same batch as negative examples. Drawing upon common practices in vision-language pretraining, we compute the image-to-audio contrastive learning loss as follows:
Lt
CL = −
B
X
i=1
1
B log
"
f (eia, eiv) f (eia, eiv) + P
k̸=i f (eia, evk),
#
where f (eia, evk) measures the cosine similarity between eia
and evk within a semantic space. During this phase, only the projection layers are trained. By utilizing contrastive learning in this phase, we aim to enable the model to understand the similarities in content that co-occur in the visual and audio modalities, thereby promoting cross-modal integration.
C. Multimodal Fusion Question-Answering Training
In the third phase, we focused on integrating information from visual and audio modalities and interacting with textual data, aiming to fuse the three modalities for multimodal question-answering tasks. For instruction fine-tuning, on one hand, we utilized the open-source dataset VideoInstruct100K [29], which contains approximately 100,000 video clips, each annotated with GPT-generated instruction-response pairs. This


dataset enhances models’ interactive capabilities through scenarios requiring complex comprehension and response generation from visual cues and verbal instructions. On the other hand, we constructed the training set for DocVideoQA, selecting 1.6K videos with QA pairs that were carefully designed and curated to improve the model’s understanding of interactions between text, visual, and audio modalities. During this process, features extracted from the three modalities were concatenated and fed into the LLM, while the video and audio encoders remained frozen and the projectors were optimized in our multimodal training approach. Unlike previous phases, this phase also involved fine-tuning the LLM using LoRA [25], transforming the LLM into a true MLLM. By effectively leveraging synchronized audio-visual data, DV-LLAMA achieved a deeper understanding of multimodal content, thereby improving its performance on the DocVideoQA task.
IV. EXPERIMENTS
A. Implementation details
Our model is based on Video-LLaMA, utilizing LLaMA-27B [34]. For the visual encoder, we consistently use the pretrained vision component from BLIP-2 [26] without any modifications. This includes a ViT-G/14 architecture from EVACLIP [27], alongside a pre-trained Q-Former. For the audiolanguage branch, we incorporate the pre-trained ImageBind [28] as the audio encoder. No hyperparameter tuning is applied during training. Instead, we empirically set the global batch size to 2,048 and the learning rate to 2e-5 for fine-tuning. The fine-tuning process is conducted for up to 3 epochs in each stage. We perform all fine-tuning on a workstation equipped with 4 NVIDIA Tesla A800 GPUs, with 80 GB of memory.
B. Evaluation Criteria
We evaluate performance using two common VideoQA metrics: Accuracy. For the test set of size N , given a question qi and its ground-truth answer yi, let the model’s predicted answer be ai, where both ai and yi are treated as sets of words. The evaluation criteria are as follows: Accuracy: This is redefined using the BERT score to account for semantic similarity between predicted and true answers. Accuracy is calculated as:
Accuracy = 1
N
N
X
i=1
1[BERT score(ai, yi) > T ]
where the indicator function outputs 1 if the BERT score exceeds threshold T , allowing for more flexibility than exact matching.
C. Experiment Result and Analysis
Table II presents a comparative analysis between our model DV-LLaMA and several state-of-the-art multi-modal large language models (MLLMs) across various domains within the DocVideoQA Dataset. As shown from row #1 to row #4, opensource models do not perform optimally, primarily due to their inability to handle the specific characteristics of documentcentric videos. We then applied a three-stage training process,
TABLE II ACCURACY COMPARISON ACROSS DIFFERENT DOMAINS AT T = 0.8
# Model AccT =0.8
Agriculture Education Fitness Total
Open-source MLLMs
1 Video-Chatgpt [29] 52.58 54.47 52.64 53.23 2 VideoChat [30] 54.72 51.69 55.71 54.62 3 Video-LLaMA [19] 60.48 62.58 64.65 61.88 4 Video-LLaVA [31] 62.37 68.46 62.12 63.92 Our Model
5 DV-LLaMA 68.64 72.69 71.76 70.51 6 - First stage 63.34 63.21 65.49 65.33 7 - Second stage 64.32 67.26 66.92 66.52 8 - Third stage 65.72 66.95 64.47 66.03
which resulted in DV-LLaMA, as shown in row #5. The model achieved a 20% improvement in performance, significantly surpassing existing open-source models. Specifically, row #6 indicates that the first training stage plays a crucial role in improving the model’s feature extraction capabilities, enabling it to adapt to the complex nature of document-centric videos and better interpret rich-text images and information-dense audio dialogues. Row #7 shows that contrastive learning aligns information from both visual and audio modalities, allowing the model to learn how audio complements and explains visual content. Finally, row #8 highlights that by leveraging LoRA to fine-tune both the LLM and the two modality branches, the model effectively integrates multi-modal information and interacts with text, resulting in better performance on QA tasks. Overall, the comparative analysis demonstrates that DVLLaMA, with our progressive training approach, significantly enhances the model’s ability to extract and fuse multi-modal information for complex QA tasks involving document videos.
V. CONCLUSIONS
In this paper, we introduce the task of Document Video Question Answering (DocVideoQA), filling the gap in videoQA for document-centric videos and extending DocumentQA to the most complex modality of video. To further advance research in this area, we provide a corresponding dataset. By developing and evaluating this dataset, we propose DV-LLaMA, which leverages multi-stage fine-tuning and contrastive learning to enhance the understanding and integration of visual and audio features required for document video scenarios in MLLMs. The experimental results powerfully demonstrate the effectiveness of our approach.
VI. ACKNOWLEDGEMENT
This work is supported by the project of Beijing Science and Technology Programme (Z231100007423011), which is also a research achievement of State Key Laboratory of Multimedia Information Processing and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology).


REFERENCES
[1] Zhong, Y., Ji, W., Xiao, J., Li, Y., Deng, W., Chua, T.S.: Video question answering: Datasets, algorithms and challenges. In: EMNLP. pp. 6439–6455 (2022) [2] Iyyer, M., Boyd-Graber, J., Claudino, L., Socher, R., Daum ́e III, H.: A neural network for factoid question answering over paragraphs. In: EMNLP. pp. 633–644 (2014) [3] Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.t., Choi, Y., Liang, P., Zettlemoyer, L.: QuAC: Question answering in context. In: EMNLP. pp. 2174–2184 (2018) [4] Bao, J., Tang, D., Duan, N., Yan, Z., Lv, Y., Zhou, M., Zhao, T.: Tableto-text: Describing table region with natural language. In: AAAI (2018) [5] Lelkes, A.D., Tran, V.Q., Yu, C.: Quiz-style question generation for news stories. In: WWW. p. 2501–2511 (2021) [6] Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t know: Unanswerable questions for SQuAD. In: ACL. pp. 784–789 (2018) [7] Koˇcisk ́y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.M., Melis, G., Grefenstette, E.: The NarrativeQA Reading Comprehension Challenge. Trans. Assoc. Comput. Linguist. 6, 317–328 (2018) [8] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: Vqa: Visual question answering. In: ICCV. pp. 2425–2433 (2015) [9] Ahmed, S., Jawade, B., Pandey, S., Setlur, S., Govindaraju, V.: Realcqa: Scientific chart question answering as a test-bed for first-order logic. In: ICDAR. pp. 66–83. Springer (2023) [10] Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: CVPR. pp. 8317–8326 (2019) [11] Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document images. In: WACV. pp. 2200–2209 (2021) [12] Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X., Zhuang, Y.: Video questionbanswering via gradually refined attention over appearance and motion. In: Proceedings of the 25th ACM International Conference on Multimedia. p. 1645–1653. MM ’17 (2017) [13] Xiao, J., Shang, X., Yao, A., Chua, T.S.: Next-qa: Next phase of question-answering to explaining temporal actions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9777–9786 (June 2021) [14] Jang, Y., Song, Y., Kim, C.D., Yu, Y., Kim, Y., Kim, G.: Video Question Answering with Spatio-Temporal Reasoning [15] Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.: Movieqa: Understanding stories in movies through questionanswering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016) [16] Sap, M., Rashkin, H., Chen, D., Le Bras, R., Choi, Y.: Social IQa: Commonsense reasoning about social interactions. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4463–4473 (Nov 2019) [17] Choi, S., On, K.-W., Heo, Y.-J., Seo, A., Jang, Y., Lee, M., & Zhang, B.-T. (2021). DramaQA: Character-Centered Video Story Understanding with Hierarchical QA. Proceedings of the AAAI Conference on Artificial Intelligence, 35(2), 1166-1174. https://doi.org/10.1609/aaai.v35i2.16203 [18] Li, H., Deng, A., Ke, Q., Liu, J., Rahmani, H., Guo, Y., Schiele, B., Chen, C.: Sports-qa: A large-scale video question answering benchmark for complex and professional sports (2024), https://arxiv.org/abs/2401.01505 [19] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023), https://arxiv.org/abs/2306.02858 [20] Wang, H., Yu, F., Shi, X., Wang, Y., Zhang, S., Li, M.: Slidespeech: A large-scale slide-enriched audio-visual corpus (2023) [21] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” arXiv preprint arXiv:2212.04356, 2022. [22] Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question answering by reading text in images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 947–952 (2019). [23] Tanaka, R., Nishida, K., Nishida, K., Hasegawa, T., Saito, I., Saito, K.: Slidevqa: A dataset for document visual question answering on multiple images. In: AAAI (2023)
[24] Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: CVPR. pp. 8317–8326 (2019) [25] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022) [26] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping languageimage pretraining with frozen image encoders and large language models (2023), https://arxiv.org/abs/2301.12597 [27] Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389 (2023) [28] Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.: Imagebind: One embedding space to bind them all. In: CVPR (2023) [29] Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards detailed video understanding via large vision and language models. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024) (2024) [30] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., Qiao, Y.: Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023) [31] Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122 (2023) [32] Xu, Y., Zhang, Q., Zhang, J., Tao, D.: Vitae: Vision transformer advanced by exploring intrinsic inductive bias. Advances in Neural Information Processing Systems 34 (2021) [33] OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) [34] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) [35] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly labelled audio captioning dataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395, 2023. [36] Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, and Xiang Bai, “Real-time scene text detection with differentiable binarization and adaptive scale fusion,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 [37] Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, and Yongdong Zhang, “Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition,” in Proc. CVPR, 2021, pp. 7098–7107. [38] Zhanghui Kuang, Hongbin Sun, Zhizhong Li, Xiaoyu Yue, Tsui Hin Lin, Jianyong Chen, Huaqiang Wei, Yiqin Zhu, Tong Gao, Wenwei Zhang, Kai Chen, Wayne Zhang, and Dahua Lin, “Mmocr: A comprehensive toolbox for text detection, recognition and understanding,” in Proc. MM, 2021, p. 3791–3794.