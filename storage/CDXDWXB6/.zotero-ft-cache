ROICtrl: Boosting Instance Control for Visual Generation
Yuchao Gu1,2, Yipin Zhou2, Yunfan Ye2, Yixin Nie2, Licheng Yu2, Pingchuan Ma3, Kevin Qinghong Lin1, Mike Zheng Shou1*
1Show Lab, National University of Singapore 2GenAI, Meta 3MIT https://roictrl.github.io/
Abstract
Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROIUnpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons (e.g., ControlNet, T2IAdapter) and embedding-based add-ons (e.g., IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.
1. Introduction
Recent text-based diffusion models have achieved remarkable success in generating images [7, 35, 36] and videos [13, 20, 42, 46] by scaling up data and computational resources. However, effectively controlling these text-based generative models continues to be a major challenge. The large information gap between natural language and the vi
*Corresponding Author.
Figure 1. Grid test for instance control. (a) We structure the region positions and instance captions into a single plain caption, then prompt DALL-E 3 to generate a nine-grid image. (b) We apply ROICtrl to generate a nine-grid image based on instance captions.
sual world complicates the precise description of spatial positions and attributes of multiple instances using language alone, often leading to linguistic ambiguity [11]. As a result, current text-based diffusion models are more effective at generating images of simple composition with a limited number of dominant instance. Inspired by the “chihuahua or muffin” grid test [10], which assesses the fine-grained visual recognition ability of multi-modal large language models, we use instance grids to evaluate the state-of-the-art text-to-image generation system DALL-E 3 [2]. As shown in Fig. 1, we structure region positions and their corresponding caption into a sentence. However, DALL-E 3 struggle to generate accurate nine-grid results, highlighting the challenge of using natural language alone to solve regional instance control in visual generation.
Much like the evolution in visual recognition, which has transitioned from concentrating on single dominant in
1
arXiv:2411.17949v1 [cs.CV] 27 Nov 2024


Figure 2. Applications of ROICtrl. A trained ROICtrl adapter can extend existing diffusion models (a) and their community-finetuned versions (b) to multi-instance generation. Additionally, it can collaborate with spatial-based add-ons (c) and embedding-based add-ons (d, e) to offer fine-grained control over spatial or identity information. ROICtrl can also be applied to continuous generation settings (f). Due to legal considerations, we do not display customized results involving human identity.
stances [8, 16, 23] (i.e., object classification) to recognizing objects within complex contexts [17, 25, 34] (i.e., object detection) through the use of bounding boxes to indicate spatial locations and distinguish instances, visual generation is also shifting towards using bounding boxes to anchor regions of interest (ROIs) for instance control. However, the main difference in ROI processing between visual recognition and visual generation is that visual generation requires handling variable-sized ROIs on high-resolution feature maps. For example, in Faster R-CNN [34], the ROI layer operates on lower-resolution features (e.g., 14×14) with a simple classification head. In contrast, the ROI layer in generative models is applied to higher-resolution features (e.g., 64×64 or 128×128) for finer detail, often using compu
tationally intensive operations like cross- or self-attention. This has led prior methods to compromise between spatial alignment and computational efficiency in ROI injection.
Prior methods for instance control can be broadly categorized into two approaches: 1) Implicit ROI injection via embedding: As shown in Fig. 3(a), GLIGEN [24] and subsequent works [12, 39] implicitly encode regional information by fusing box coordinate embeddings with instance caption embeddings. Self-attention mechanisms are then used to inject this ROI information into the global feature map. Although implicit ROI injection avoids directly handling variable-sized ROIs, it suffers from severe attribute leakage issues and lower spatial alignment. 2) Explicit ROI injection with attention mask: As shown in Fig. 3(b),
2


Figure 3. Illustration of different ROI injection designs. ⌊·⌉ denotes coordinate quantization to the nearest integer.
MIGC [48] and Instance Diffusion [40] use masked crossattention to isolate each ROI during instance caption injection, achieving better spatial alignment and reducing attribute leakage issues. However, despite the use of masked attention, the computations are still conducted on the fullsized high-resolution feature map, resulting in high computational costs. In this work, we introduce an effective strategy for instance control in visual generation. Inspired by ROIAlign [17] in object detection, we introduce a complementary operation named ROI-Unpool, which restores cropped ROI features to their original position on the high-resolution feature map. As shown in Fig. 3(c), combining ROI-Align and ROI-Unpool allows explicit extraction and processing of ROI features, with computational costs independent of the original feature size. Building on this operation, we introduce ROICtrl, an adapter that integrates instance control into existing diffusion models. ROICtrl is compatible with existing spatial-based add-ons (e.g., ControlNet [47] and T2I-Adapter [28]) and embedding-based add-ons (e.g., IP-Adapter [45] and ED-LoRA [14]), expanding their application for multi-instance generation (as shown in Fig. 2). In evaluating instance control, we find that previous benchmarks are limited to template-based captions, focusing on specific attributes like color, as summarized in Tab. 1. However, users often prefer free-form descriptions to capture broader attributes. To address this gap, we introduce ROICtrl-Bench, a benchmark specifically designed to evaluate both template-based and free-form instance captions. By leveraging the strong open-domain recognition abilities of multi-modal large language models, ROICtrl-Bench provides a more comprehensive assessment of instance control. Our contributions are summarized as follows: • We introduce ROI-Unpool, an operation that facilitates efficient and accurate ROI injection for visual generation. • We propose ROICtrl, an adapter that is compatible with existing diffusion models and their add-ons, expanding their applications in multi-instance generation.
Table 1. Comparison of existing instance control benchmarks. Previous benchmarks mainly focus on template-based instance captions, while ROICtrl-Bench covers both template-based and free-form instance captions for comprehensive evaluation.
Benchmarks In-Distribution Out-of-Distribution
Template Cap. Free-Form Cap. Template Cap. Free-Form Cap.
GLIGEN-Bench [24] ✓ MIG-Bench [48] ✓ InstDiff-Bench [40] ✓ ✓
ROICtrl-Bench ✓ ✓ ✓ ✓
• We introduce ROICtrl-Bench, a comprehensive benchmark for evaluating instance control capabilities. ROICtrl achieves state-of-the-art performance and improved efficiency on ROICtrl-Bench, as well as on two existing benchmarks (InstDiff-Bench [40] and MIG-Bench [48]).
2. Related Work
2.1. Controllable Visual Generation
While text-to-image and text-to-video diffusion models achieve high generation quality, they are limited by language alone in capturing fine-grained spatial(-temporal) and identity details. To address this, researchers have introduced visual conditions to enhance controllability: spatial control for precise layouts (e.g., ControlNet [47], T2IAdapter [28]), embedding control for detailed identity (e.g., ED-LoRA [14], IP-Adapter [45]), and trajectory control for fine-grained motion (e.g., VideoSwap [15], MotionCtrl [41]). However, these controls lack explicit instance separation, leading to severe attribute leakage issues in multiinstance generation, as shown in Fig. 2.
2.2. Instance Control in Visual Generation
Unlike the above controls that enable fine-grained visual alignment, instance control is designed to separate different instances, allowing for independent control of each instance while preventing attribute leakage between them. This approach is often associated with bounding-box, layout, or region control. We group all these types under the term “Instance Control” and outline the main methods below.
Training-Free Instance Control. Training-free instance control [5, 18, 22, 32, 43] primarily manipulates the attention map in diffusion models during inference, inspired by the finding that cross-attention conveys layout information [19]. The core idea is to enhance the influence of nouns on their corresponding regions using techniques such as attention modulation [22] or latent optimization [3, 43]. While this approach allows for some degree of instance control, it often involves a trade-off between image quality and spatial alignment, as well as increased computational costs and reduced flexibility during inference.
Training-Based Instance Adapter. Training-based instance adapters aim to learn instance control from data and can be categorized into implicit and explicit injection meth
3


(a) ROI-Align (b) ROI-Unpool
Figure 4. Illustration of ROI-Unpool. The dashed grid represents the spatial features, while the solid grid represents the ROI features. Similar to ROI-Align [17], ROI-Unpool avoids coordinate quantization during computation.
ods based on how they incorporate instance information. Implicit injection [12, 24, 39] encodes bounding box coordinates as positional embeddings, which are then fused with instance caption embeddings. A gated self-attention mechanism injects the instance embedding into spatial features. While this approach avoids directly handling variable-sized ROIs, it suffers from lower spatial alignment and significant attribute leakage issues. In contrast, explicit injection isolates the target region during instance caption injection, preventing attribute leakage between instances. Previous works [29, 40, 48] adopt attention masks to zero out unrelated regions; however, this approach results in substantial redundant computation and coordinate quantization errors.
To address these limitations, we introduce ROI-Unpool for explicit, efficient, and accurate ROI injection.
2.3. Instance Recognition in Visual Understanding
Early research in visual recognition focus on object classification tasks [8, 16, 23], where each image typically contains a single prominent object (e.g., ImageNet [8]). As the field progressed, attention shift to detecting multiple objects in context (e.g., MS-COCO [25]), with bounding boxes used to locate individual objects. The representative approach in object detection is the two-stage detector [17, 26, 34], which employs ROI-Pool/ROI-Align to parallelize feature extraction from varied-sized ROIs, enabling efficient ROI processing. Motivated by this line of research, we explore effective ROI operations for visual generation in this work.
3. Methodology
In this section, we first present the problem formulation of multi-instance generation in Sec. 3.1. Next, we introduce the basic operation, ROI-Unpool, in Sec. 3.2. Building on ROI-Unpool, we then describe the design of the ROICtrl adapter in Sec. 3.3. Finally, we discuss the applications of ROICtrl in Sec. 3.4.
3.1. Problem Formulation
Multi-instance generation is defined as using a global caption pg to describe the whole image, along with n instance caption pr = {pri }n
i=1 and their corresponding bounding
box coordinates cr = {cri }n
i=1 to describe each instance. Since the original diffusion model relies solely on the global caption pg for control, our goal is to develop an adapter that can incorporate region-specific information (pr, cr) into a pretrained diffusion model. An effective instance adapter should achieve strong spatial alignment and regional text alignment. Beyond these core requirements, the following additional criteria are crucial for real-world applications:
1) Free-Form Instance Captions. Instance captions should be in free-form text, providing flexibility similar to the global caption rather than relying solely on templatebased captions [24, 48].
2) Compatibility with Fine-Grained Controls. Since bounding boxes provide only coarse spatial cues for distinguishing instances, the instance adapter should be compatible with existing add-ons that offer fine-grained control, such as spatial-based add-ons (e.g., ControlNet [47], T2I-Adapter [28]) or embedding-based add-ons (e.g., EDLoRA [14], IP-Adapter [45]).
3.2. ROI-Unpool
In contrast to object detection, which feeds extracted ROI features into a simple classification head to category prediction, visual generation requires the processed ROI features to be “pasted back” at their original coordinates on the spatial feature map to allow further decoding of fine-grained details. To achieve this, existing methods [29, 40, 48] for instance control primarily use a masked attention mechanism that zeros out unrelated regions during instance caption injection. This approach keeps each ROI at its original spatial coordinates, bypassing the difficulties of “pasting back” varied-sized ROIs. However, the masked attention mechanism introduces significant redundant computation outside the ROI, which is costly on high-resolution feature maps in visual generation. Additionally, coordinate quantization errors during mask creation reduce spatial alignment. In this work, we address the challenges of “pasting back” varied-sized ROIs onto their original coordinates in the spatial feature map by introducing ROI-Unpool. ROI-Unpool complements ROI-Align [17], enabling explicit ROI operations for visual generation. Specifically, as shown in Fig. 4, ROI-Align computes ROI features by bilinearly resampling from the four nearest grid points in the original spatial feature map, whereas ROI-Unpool reconstructs the spatial features using the four nearest grid points from the ROI feature. For border regions without all four sample points, we compute partial values from available points. Positions that do not correspond to the ROI region are left empty.
4


Figure 5. Detailed structure of ROICtrl. In parallel with the pretrained global caption injection, we introduce an additional instance caption injection. The global attention output and instance attention output are then fused using learnable blending.
3.3. ROICtrl
Building on ROI-Unpool, we introduce ROICtrl, an adapter designed to integrate instance captions into diffusion models. As illustrated in Fig. 5, ROICtrl adds an instance caption injection parallel to the pretrained global caption injection. The global attention output and instance attention output are then fused through a learnable blending mechanism. Formally, when provided with the global caption pg, n region captions pr and their coordinates cr, the goal of ROICtrl is to inject the given conditions into the spatial feature H, resulting in H′ = ROICtrl H pg, {pri , cri }n
i=1 .
3.3.1 Instance Caption Injection
In ROICtrl, the global caption describes the overall composition and background of the image, while the instance caption provides specific details of each instance. As shown in Fig. 5, we use the pretrained cross-attention from the diffusion model to generate the global attention output Ag ∈ Rb×c×h×w. For instance caption injection, we first extract the ROI feature Hr ∈ Rb×n×c×r×r from the spatial feature H ∈ Rb×c×h×w using ROI-Align [17], where r × r represents the ROI feature size, n is the number of ROIs, and b, c, h, and w represent the batch size, channels, height, and width of the spatial feature, respectively. We then reuse the pretrained cross-attention to inject the instance captions into each ROI feature. Unlike previous methods in [24, 40, 48], we do not use any additional learnable modules for instance caption injection. This strategy preserves the pretrained model’s knowledge and ensures compatibility with embedding-based add-ons, such as EDLoRA [14] and IP-Adapter [45]. Since the pretrained crossattention is optimized for the original spatial resolution, directly applying it to ROI features may introduce artifacts and misalignment. To address this, we introduce ROI selfattention to refine ROI feature. Finally, ROI-Unpool places the refined ROI features back at their original positions in the feature map, producing the instance attention output
Ar ∈ Rb×n×c×h×w.
3.3.2 Learnable Attention Blending
Given the global attention output Ag ∈ Rb×1×c×h×w and the instance attention output Ar ∈ Rb×n×c×h×w, we first concatenate them along the ROI axis to form a combined attention output A ∈ Rb×(n+1)×c×h×w. The goal of learnable attention blending is to dynamically reweight the (n + 1) attention outputs at each spatial location (h, w). To achieve this, we compute the learnable fusion weight W ∈ Rb×n×1×h×w by applying a 1 × 1 convolution to reduce the channel dimension, followed by a softmax function applied across the ROI axis. We then use these weights to perform a weighted fusion of A, which produces the final attention output A′ ∈ Rb×c×h×w. After obtaining the final attention output A′, we incorporate instance box embeddings as guidance for occlusion scenario, inspired by GLIGEN [24]. Unlike GLIGEN, we use only box embeddings without instance caption embeddings to prevent attribute leakage. The explicit use of box coordinate conditioning enhances the objectness in the corresponding regions, leading to improved spatial alignment.
3.3.3 Training Objective
ROICtrl is optimized using the standard diffusion loss:
LLDM = Ez,ε∼N (0,I),t
h
|ε − εθ(zt, t, φ(pg, pr, cr))|2
2
i
, (1)
where φ denotes the learnable parameters of ROICtrl. An additional regularization is applied to the learnable fusion weight W to reduce the influence of the global attention output within the ROI and facilitate alignment with the instance caption. This regularization term is defined as:
Lreg = |M⊙W:,1,:,:|1
|M|1
, where M ∈ {0, 1}b×1×h×w is a
mask identifying the foreground area containing the ROI, and W:,1,:,: denotes the fusion weight of the global attention output. The final objective function combines the standard diffusion loss with the regularization term: L = LLDM+αLreg, where α = 0.01 throughout our experiments.
5


Table 2. Quantitative comparison with prior works on MIG-Bench [48], InstDiff-Bench [40], and the proposed ROICtrl-Bench.
Method mIoU Instance Success Rate (%)
Level (# Instances) L2 L3 L4 L5 L6 AVG L2 L3 L4 L5 L6 AVG
GLIGEN [24] 0.37 0.29 0.253 0.26 0.26 0.27 0.42 0.32 0.27 0.27 0.28 0.30 MIGC [48] 0.64 0.58 0.57 0.54 0.57 0.56 0.74 0.67 0.67 0.63 0.66 0.66 Instance Diffusion [40] 0.52 0.48 0.50 0.42 0.42 0.46 0.58 0.52 0.55 0.47 0.47 0.51 ROICtrl (Ours) 0.78 0.72 0.67 0.61 0.64 0.66 0.85 0.79 0.74 0.67 0.70 0.73
(a) Quantitative evaluation on MIG-Bench [48]. MIG-Bench uses GroundingDINO [27] mIoU to measure spatial alignment and assesses regional text alignment within the color space.
Location Attribute
Method AP AP50 APs APm APl AR Acccolor CLIPcolor Acctexture CLIPtexture Upper bound (real images) 48.4 65.2 30.9 53.3 64.8 67.8 - - - 
GLIGEN [24] 24.1 42.6 3.1 22.2 49.0 35.9 26.3 0.212 17.7 0.208 MIGC [48] 22.4 41.5 2.1 20.1 46.8 32.8 53.8 0.243 24.3 0.215 Instance Diffusion [40] 40.1 57.2 10.4 49.4 67.1 53.2 55.2 0.243 26.1 0.222 ROICtrl (Ours) 41.0 63.5 16.3 46.5 65.7 54.1 62.3 0.256 29.3 0.227
(b) Quantitative evaluation on InstDiff-Bench [40]. InstDiff-Bench evaluates spatial alignment using YOLO-Det [21] Average Precision (AP) and assesses regional text alignment based on color and texture using CLIP score [33].
T1: Subject T2: Subject* T3: Subject + Attribute T4: Subject + Attribute* AVG
Method mIoU Acc (%) mIoU Acc (%) mIoU Acc (%) mIoU Acc (%) mIoU Acc (%)
Upper Bound (real images) 0.797 72.5 - - 0.797 66.4 - - - 
GLIGEN [24] 0.579 59.1 0.474 43.3 0.546 16.3 0.548 1.90 0.537 30.2 MIGC [48] 0.521 61.9 0.442 47.6 0.498 33.7 0.498 12.3 0.490 38.9 Instance Diffusion [40] 0.673 66.5 0.562 53.5 0.634 39.4 0.559 23.0 0.607 45.6
ROICtrl (Ours) 0.692 68.9 0.557 50.9 0.688 47.3 0.669 27.8 0.652 48.7
(c) Quantitative evaluation on the proposed ROICtrl-Bench. We assess spatial alignment using YOLO-World [6] mIoU and evaluate regional text alignment with MiniCPM-V 2.6 [44]. Tracks 1 and 2 examine template-based instance caption, while tracks 3 and 4 evaluate free-form instance caption. * denote out-of-distribution caption rewritten by GPT-4 [1].
Figure 6. Qualitative comparison on ROICtrl-Bench. Track 1 and 2 examine template-based instance caption, while track 3 and 4 evaluate free-form instance caption. [ID] denotes in-distribution caption derived from real dataset, and [OOD] denotes out-of-distribution caption generated by GPT-4 [1].
3.4. Application
In this section, we discuss the applications of ROICtrl. Instance Control. Without any additional add-ons, ROICtrl alone can be used to control the instance in complex compositions, with each instance can be described with freeform text, as demonstrated in Fig. 2(a).
Compatible with Various Community Models. Once ROICtrl is trained on the base model, it can be directly adapted to various community models fine-tuned from the base model, as illustrated in Fig. 2(b).
Compatible with Spatial-based Add-ons. ROICtrl is used to separate various instances and can collaborate with fine-grained spatial controls (e.g., ControlNet [47], T2IAdapter [28]) during inference. As shown in Fig. 2(c), without ROICtrl, T2I-Adapter alone cannot control specific instance and suffers from severe attribute leakage.
Compatible with Embedding-based Add-ons. ROICtrl can work with embedding-based add-ons to control in
stance identity. As shown in Fig. 2(d, e), it supports both the tuning-based ED-LoRA [14] and the tuning-free IPAdapter [45]. This compatibility is achieved by reusing pretrained cross-attention without adding new learnable modules for instance captions, ensuring seamless integration with embedding-based add-ons that rely on pretrained cross-attention. Continuous Generation. ROICtrl enables continuous generation, allowing modification of local regions while preserving previously generated content, as shown in Fig. 2(f).
4. Experiments
4.1. Experimental Setup
We implement ROICtrl using the PyTorch [30] framework. For enhanced computational efficiency, we develop a custom CUDA kernel for the ROI-Unpool operation. We adopt multi-scale ROI, where the ROI size r × r is defined by the relation r = 6 × log2 R − 11, with R × R represent
6


Figure 7. Ablation study comparing ROICtrl and embedding-based injection (GLIGEN*). ROICtrl achieves faster convergence, improved spatial and regional text alignment, and flexible inference aspect ratios.
Table 3. Ablation study comparing ROICtrl with attention mask–based ROI injection. ROICtrl achieves similar regional text alignment but better spatial alignment, while significantly reducing memory and computational costs. The inference speed is tested by generating a 10242 resolution image with 25 valid ROIs, 50 DDIM [37] steps, and fp16 precision on an A100 GPU.
Models ROICtrl-Bench MIG-Bench Instdiff-Bench Training Inference Deployed Support
mIoU Acc mIoU Acc AP Color Acc Texture Acc Memory (G) Speed (s/img) Resolution Emb Addon
ROICtrl (Ours) 0.652 48.7 0.66 0.73 41.0 62.3 29.3 34.3 13.1 all ✓
Mask-Attn
ROICtrl (mask) 0.628 49.2 0.64 0.71 37.1 62.5 30.3 65.5 31.5 all ✓ Instance Diffusion 0.607 45.6 0.46 0.51 40.1 55.2 26.1 - 129.2 all ✗ MIGC 0.490 38.9 0.56 0.66 22.4 53.8 24.3 - 23.5 8×, 16× ✗
ing the spatial feature size. For example, if the diffusion model operates at a resolution of 512 × 512, with feature resolutions R = {64, 32, 16, 8}, the corresponding ROI sizes are r = {25, 19, 13, 7}. The model is trained on the MS-COCO [25] training set, where we recaptioning each instance with free-form text generated by CogVLM [2]. Training is performed with a batch size of 128 and a learning rate of 5e-5, over 60,000 steps on 8 NVIDIA A100 GPUs.
4.2. Evaluation Setup
ROICtrl-Bench. Existing benchmarks for instance control primarily evaluate template-based instance captions, as shown in Tab. 1. For example, MIG-Bench [48] primarily uses templates such as “[adj.]-colored-[noun.]” for evaluation. However, in real-world applications, users require free-form instance captions to capture a broader range of attributes. To bridge this gap, we construct ROICtrl-Bench, which consists of four tracks to evaluate various settings. Tracks 1 and 2 examine template-based instance captions, while Tracks 3 and 4 evaluate free-form instance captions. Additionally, Tracks 2 and 4 assess the model’s ability to generate out-of-distribution instance captions generated by GPT-4 [1]. Evaluation Metrics. Instance control is evaluated on two main aspects: spatial alignment and regional text alignment. For spatial alignment, we report the Mean Intersection over Union (mIoU). We calculate mIoU by first using Yolo-World [6] with a low confidence threshold to detect region proposals based on specified categories. We then apply
bipartite matching to pair detected boxes with ground truth boxes and compute the mIoU between them. For regional text alignment, we input cropped regions of the generated image and their instance captions into MiniCPM-V 2.6 [44] and using it to evaluate whether they successfully match. The match rate is reported as accuracy (Acc). For a comprehensive evaluation of ROICtrl, we also adopt two previous benchmarks, MIG-Bench [48] and InstDiff-Bench [40], following their evaluation settings.
4.3. Comparison to Prior Works
Quantitative Comparison. We present the quantitative comparison results in Tab. 2. Notably, ROICtrl outperforms both Instance Diffusion [40] and MIGC [48] in generating small objects, as illustrated in Tab. 2(b). This improvement is due to ROICtrl’s precise localization of small objects, which avoids the quantization errors commonly introduced by masked attention. On ROICtrl-Bench (Tab. 2(c)), ROICtrl performs slightly worse than Instance Diffusion on out-of-distribution subjects. This discrepancy may be due to Instance Diffusion being trained on an internal dataset containing 5 million recaptioned images, whereas our approach is trained solely on the publicly available MS-COCO dataset [25] with 118K samples. Despite this, Tab. 2 clearly shows that ROICtrl outperforms previous methods, achieving superior spatial alignment and regional text alignment. Qualitative Comparison. As qualitative comparison summarized in Fig. 6(a), ROICtrl effectively models occlusions when bounding boxes overlap. In Fig. 6(b), when encountering out-of-distribution instance captions, ROICtrl shows
7


Table 4. Ablation study of design choices in ROICtrl.
Models ROICtrl-Bench MIG-Bench Instdiff-Bench
mIoU Acc mIoU Acc AP Color Acc Texture Acc ROICtrl (Ours) 0.652 48.7 0.66 0.73 41.0 62.3 29.3
− ROI Self-Attn 0.540 48.6 0.66 0.72 32.7 60.5 32.9 − Lreg 0.658 47.2 0.66 0.72 41.1 58.2 21.9 global coord → local coord 0.655 49.5 0.68 0.74 42.1 63.3 30.3 multi-scale roi → single-scale roi 0.639 49.6 0.65 0.73 40.0 62.5 29.9
Figure 8. Effect of global attention regularization Lreg. Adding Lreg reduces the weight of the global attention output within the ROI, leading to improved regional text alignment.
fewer attribute leakage compared to GLIGEN [24] and MIGC [48], while maintaining better global consistency than Instance Diffusion [40]. Additionally, in Fig. 6(c) and (d), ROICtrl achieves superior regional text alignment with free-form instance caption compared to previous methods.
4.4. Ablation Study
4.4.1 Compare to ROI-Injection via Embedding
We compare ROICtrl with embedding-based ROI injection (see Fig. 3(a)). We adopt the architecture from GLIGEN [24], and label this variant as GLIGEN*. As summarized in Fig. 7(a), ROICtrl consistently outperforms GLIGEN* at same training steps. Notably, even after 500K training steps (10× more than ROICtrl), GLIGEN* still exhibits weaker spatial alignment. In terms of regional text alignment, GLIGEN* uses global self-attention to inject instance captions, leading to severe attribute leakage and much poorer regional text alignment than ROICtrl. Additionally, embedding-based ROI injection struggles to generalize when the inference aspect ratio deviates from the training aspect ratio, as shown in Fig. 7(b), which poses challenges for practical applications that require flexible inference aspect ratio.
4.4.2 Compare to ROI-Injection via Attention Mask
We compare ROICtrl with attention mask–based ROI injection (see Fig. 3(b)). Specifically, we modify the ROICtrl implementation by replacing ROI-Align and ROI-Unpool with masked attention. This variant, labeled as ROICtrl (Mask), achieves similar regional text alignment but much worse spatial alignment than ROICtrl, while also consuming more training memory and reducing inference speed, as summarized in Tab. 3. We also compare ROICtrl with previous attention maskbased methods, Instance Diffusion [40] and MIGC [48].
Figure 9. Comparison of regional and global coordinate conditioning. Regional coordinate conditioning leads to repetition issues when the inference size is doubled relative to the training size.
Instance Diffusion is more computationally intensive due to its complex design for supporting point and mask control, with ROICtrl achieving about 9.8× speedup. MIGC reduces computation by deploying adapters only on lowresolution feature maps (e.g., 8× or 16× downsampled feature), resulting in degraded spatial and regional text alignment performance, while still being slower than ROICtrl. Additionally, both Instance Diffusion and MIGC rely on learnable modules for regional caption injection, making them incompatible with embedding-based add-ons like EDLoRA [14] and IP-Adapter [45].
4.4.3 Key Design Choices of ROICtrl
Effect of ROI Self-Attn. In ROICtrl, we reuse pretrained cross-attention to inject instance captions. However, since this cross-attention is initially trained on full-resolution features, applying it directly to ROI features without ROI selfattention refinement leads to poorer spatial alignment. As shown in Tab. 4, removing ROI self-attention decreases the mIoU on ROICtrl-Bench from 0.652 to 0.540 and the AP on InstDiff-Bench from 41.0 to 32.7. Effect of Lreg. As discussed in Sec. 3.3.3, Lreg is used to decrease the fusion weight of the global attention output within the ROI. As shown in Tab. 4, omitting Lreg reduces the impact of the instance caption, results in poorer regional text alignment: Color Acc decreases from 62.3 to 58.2 and Texture Acc drops from 29.3 to 21.9 on InstDiff-Bench. As visualization shown in Fig. 8, applying Lreg enhances the instance textures of the ball and teddy bear.
Regional vs. Global Coordinate Conditioning. ROICtrl employs global coordinate conditioning, following GLIGEN [24], whereas recent works such as MIGC [48] and BlobGEN [29] utilize regional coordinate conditioning. Tab. 4 shows that local coordinate conditioning achieves slightly better quantitative performance. However, in realworld applications, we find that local coordinate conditioning does not generalize well to varying resolutions. As shown in Fig. 9, with an inference size of 512×1024 (double the training size of 512×512), regional coordinate conditioning suffers from subject repetition issues. Multi-Scale ROIs. In ROICtrl, we set multi-scale ROIs to
8


adapt to the multi-scale feature maps of U-Net. We compare the multi-scale ROIs with single-scale ROIs, where r = {7, 7, 7, 7}, as shown in Tab. 4. Multi-scale ROIs achieve better spatial alignment while maintaining similar text alignment.
5. Conclusion
In this paper, we introduce ROICtrl, a method designed to boost instance control in visual generation. ROICtrl is built upon ROI-Unpool, a foundational operation that enables efficient ROI modeling on high-resolution feature maps. By leveraging ROICtrl, we adapt existing diffusion models and their add-ons for multi-instance generation, showcasing a variety of applications enabled by our approach. ROICtrl demonstrates superior qualitative and quantitative results across various benchmarks while also achieving notable speedup, paving the way for controllable generation of multi-instance compositions.
References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6, 7 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1, 7
[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):1–10, 2023. 3
[4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73107320, 2024. 12 [5] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5343–5353, 2024. 3 [6] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16901–16911, 2024. 6, 7 [7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023. 1
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 2, 4 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mu ̈ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 13 [10] Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Yang Zhao, Xinze Guan, and Xin Wang. Muffin or chihuahua? challenging multimodal large language models with multipanel vqa. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6845–6863, 2024. 1 [11] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 1
[12] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion for accurate instruction following. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4744–4753, 2024. 2, 4 [13] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 1
[14] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 3, 4, 5, 6, 8 [15] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 76217630, 2024. 3 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 2, 4 [17] Kaiming He, Georgia Gkioxari, Piotr Dolla ́r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961–2969, 2017. 2, 3, 4, 5 [18] Yutong He, Ruslan Salakhutdinov, and J Zico Kolter. Localized text-to-image generation for free via cross attention control. arXiv preprint arXiv:2306.14636, 2023. 3
[19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im
9


age editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3
[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1
[21] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by Ultralytics, 2023. 6 [22] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7701–7711, 2023. 3 [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 2, 4 [24] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511–22521, 2023. 2, 3, 4, 5, 6, 8, 12 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla ́r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 2, 4, 7, 12 [26] Tsung-Yi Lin, Piotr Doll ́ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117–2125, 2017. 4 [27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 6
[28] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 4296–4304, 2024. 3, 4, 6 [29] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. arXiv preprint arXiv:2405.08246, 2024. 4, 8
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 6
[31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023. 13
[32] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7932–7942, 2024. 3
[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 6 [34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. 2, 4 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 1 [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022. 1 [37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 7
[38] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc N Tran, and Anh Tran. Anti-dreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2116–2127, 2023. 13 [39] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. 2, 4
[40] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instancelevel control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6232–6242, 2024. 3, 4, 5, 6, 7, 8, 12, 13 [41] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024. 3 [42] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7623–7633, 2023. 1 [43] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained
10


diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7452–7461, 2023. 3 [44] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6, 7
[45] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 4, 5, 6, 8 [46] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. 1
[47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847, 2023. 3, 4, 6 [48] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68186828, 2024. 3, 4, 5, 6, 7, 8, 12, 13
11


Global Caption: Two baseball players standing on a field, with one wearing baseball glove. ROI-1: A person. ROI-2: A person. ROI-3: A baseball glove.
Seed-1 Seed-2
Figure 10. Limitation of ROICtrl. ROICtrl prioritizes the use of instance captions to solve attribute binding but performs unstably when instance boxes with similar captions are heavily overlapped.
6. Detailed Evaluation Settings
6.1. ROICtrl-Bench
ROICtrl-Bench contains 200 samples, divided into groups {1, 2, 3, 4, 5, 6-10, 11-15, 16-20, 21-25, 26-30} based on instance counts. Each group includes 20 examples randomly selected from the MS-COCO 2017 evaluation set [25]. Half of the evaluation examples contain small-sized ROIs with spatial size smaller than 32 × 32. As discussed in Sec. 4.2, we create four types of instance captions for each example, corresponding to four tracks, resulting in a total of 800 evaluation examples. For template-based captions in tracks 1 and 2, we follow the GLIGEN [24] evaluation protocol, using only category labels as instance captions. For free-form instance captions, we leverage a multi-modal large language model to provide instance captions. We report the spatial alignment (mIoU) and regional text alignment (Acc) metrics for each track.
6.2. InstDiff-Bench
InstDiff-Bench [40] uses the entire MS-COCO 2017 evaluation set [25] as its benchmark. For spatial alignment evaluation, it calculates YOLOv8 detection metrics (AP) based on in-distribution instance captions (i.e., object categories). To assess the model’s ability to generate out-ofdistribution instance captions, it defines 8 common colors: black, white, red, green, yellow, blue, pink, purple, and 8 common textures: rubber, fluffy, metallic, wooden, plastic, fabric, leather, glass. For each instance, a texture or color adjective is randomly selected from that predefined adjective pool, and the caption is constructed using the template [adj.]-[noun.]. InstDiff-Bench inputs the cropped box into the CLIP model to predict attributes (colors and textures) and evaluates the accuracy of the predicted adjectives (i.e., Acccolor or Acctexture). Additionally, it reports the
regional CLIP score for each instance caption.
6.3. MIG-Bench
MIG-bench [48] mainly evaluates spatial alignment and regional text alignment on out-of-distribution instance captions. It selects 800 layouts from COCO, randomly assigns a color to each instance, and constructs the caption based on the template [adj.]-colored-[noun]. In their evaluation, they filter out small-sized ROIs and dense ROIs with more than 6 instances. MIG-bench primarily reports spatial alignment (mIoU) and regional text alignment (instance success rate).
7. Additional Experiments
7.1. Qualitative Comparison
We have demonstrated the qualitative comparison on ROICtrl-Bench in Sec. 4.3 of the main paper. Therefore, in this section, we primarily present the qualitative comparison on InstDiff-Bench [40] and MIG-Bench [48]. Small-Sized ROIs. As shown in Fig. 11(a), previous instance diffusion [40] tends to generate redundant instances beyond the box, while GLIGEN [24] and MIGC [48] do not accurately follow the box. In comparison, ROICtrl can accurately generate small-sized ROIs.
Out-of-Distribution Instance Caption. As shown in Fig. 11(b, c), previous methods do not accurately follow the instance caption when generating out-of-distribution attributes and exhibit attribute leakage. In comparison, ROICtrl follows the instance caption accurately.
8. Limitation and Future Works
8.1. Limitation Analysis
The attribution leakage problem is largely addressed in ROICtrl, as we prioritize using instance captions in the learnable blending process. However, generating the same instance for highly overlapping bounding boxes remains a challenge. As illustrated in Fig. 10, when the boxes exhibit significant overlap, the model need to rely on the global caption for additional information. However, ROICtrl tends to favor instance captions instead, making it unstable to solve this case. We believe that further improving the learnable blending strategy to dynamically reweight the global and instance captions could solve this issue.
8.2. Future Works
Apply ROICtrl to Video Instance Control. In our preliminary experiments on VideoCrafter2 [4], we find that with slight fine-tuning of the pretrained ROICtrl on a video dataset (about 2K iterations), ROICtrl can be used to control video instances, as shown in Fig. 12. However, improving the temporal consistency of video instances remains a
12


(a) Qualitative comparison of ROICtrl and previous methods on small-sized ROIs in Instdiff-Bench [40]. (Zoom in for details.)
(b) Qualitative comparison of ROICtrl and previous methods on out-of-distribution instance captions in Instdiff-Bench [40].
(c) Qualitative comparison of ROICtrl and previous methods on out-of-distribution instance captions in MIG-Bench [48].
Figure 11. Qualitative comparison of ROICtrl and previous methods on Instdiff-Bench [40] and MIG-Bench [48]. We provide examples for small-sized ROIs and out-of-distribution instance captions.
Figure 12. Applications of ROICtrl on Video Instance Control. We encourage readers to click and play the video clips in this figure using Adobe Acrobat.
challenge, presenting a potential direction for future development of ROICtrl.
Apply ROI-Unpool to Diffusion Transformers. ROICtrl is primarily designed for UNet-based diffusion models. Another future direction is to explore combining ROI-Unpool with transformer-based diffusion models [9, 31] to explicitly separate instance features and inject instance control.
8.3. Potential Negative Social Impact
This project aims to provide the community with an effective method for performing multi-instance control. However, a risk exists wherein malicious entities could exploit this framework, in combination with image customization, to generate deceptive images of multiple public figures, potentially misleading the public. This concern is not owing to our approach but rather a shared consideration in concept
customization. One potential solution to mitigate such risks involves adopting methods similar to anti-dreambooth [38], which introduce subtle noise perturbations to the published images to mislead the customization process. Additionally, applying unseen watermarking to the generated image could deter misuse and prevent them from being used without proper recognition.
13