ECHOVIDEO: IDENTITY-PRESERVING HUMAN VIDEO
GENERATION BY MULTIMODAL FEATURE FUSION
Jiangchuan Wei ByteDance
weijiangchuan@bytedance.com
Shiyue Yan ByteDance
yanshiyue@bytedance.com
Wenfeng Lin ByteDance
linwenfeng.1008@bytedance.com
Boyuan Liu ByteDance
liuboyuan@bytedance.com
Renjie Chen ByteDance
chenrenjie.1998@bytedance.com
Mingyu Guo ByteDance
guomingyu.313@bytedance.com
Figure 1: Sampling results of EchoVideo. (a) Facial feature preservation. (b) Full-body feature preservation. EchoVideo is capable of not only extracting human features but also resolving semantic conflicts between these features and the prompt, thereby generating coherent and consistent videos.
ABSTRACT
Recent advancements in video generation have significantly impacted various downstream applications, particularly in identity-preserving video generation (IPT2V). However, existing methods struggle with "copy-paste" artifacts and low similarity issues, primarily due to their reliance on lowlevel facial image information. This dependence can result in rigid facial appearances and artifacts reflecting irrelevant details. To address these challenges, we propose EchoVideo, which employs two key strategies: (1) an Identity Image-Text Fusion Module (IITF) that integrates high-level semantic features from text, capturing clean facial identity representations while discarding occlusions, poses, and lighting variations to avoid the introduction of artifacts; (2) a two-stage training strategy, incorporating a stochastic method in the second phase to randomly utilize shallow facial information. The objective is to balance the enhancements in fidelity provided by shallow features while mitigating excessive reliance on them. This strategy encourages the model to utilize high-level features during training, ultimately fostering a more robust representation of facial identities. EchoVideo effectively preserves facial identities and maintains full-body integrity. Extensive experiments demonstrate that it achieves excellent results in generating high-quality, controllability and fidelity videos. The code and model are available at: https://github.com/bytedance/EchoVideo.
arXiv:2501.13452v2 [cs.CV] 27 Feb 2025


Running Title for Header
Figure 2: Issues in IP character generation. (a) Semantic conflict. The input image depicts a child’s face, while the prompt specifies an adult male. Insufficient information interaction leads to inconsistent character traits in the model’s output. (b) Copy-paste. During training, the model overly relies on visual information from facial images, directly using the Variational Autoencoder(VAE)-encoded [1] face as the output for the generated face.
1 Introduction
In the realm of text-to-video generation tasks, large-scale pre-trained models based on Diffusion Transformers (DiT) [2] have exhibited exceptional performance, driving significant advancements across various downstream applications [3, 4, 5, 6], particularly in identity-preserving text-to-video (IPT2V) generation. Users can input images containing facial portraits, which are subsequently utilized by text-to-image models to produce videos that feature their likeness. Recently, several methods have emerged within the open-source community [7, 8, 9, 10] targeting this domain. Notably, ConsisID [10] has achieved state-of-the-art (SOTA) verifiable results; However, it relies heavily on shallow information extracted from facial images, leading to artifacts such as rigid facial expressions and misalignment between the face and body in the generated outputs. Additionally, the generated faces are vulnerable to interference from extraneous information present in the input facial images, including occlusions. This issue is often associated with the "copy-paste" phenomenon. In contrast, closed-source methods such as Vidu [11], Pika [12], and HailuoAI [13] frequently encounter challenges related to low similarity between the generated faces and the input facial images.
In light of the observed phenomena, we note that the process of decoupling facial identity information from portrait images often incorporates excessive irrelevant information inherent to the images themselves. For instance, the ConsisID [10] utilizes the original portrait images as strong supervisory signals, concatenating them with the initial noisy latent representations. While this approach facilitates the model’s rapid acquisition of high-fidelity facial information, it fails to effectively filter out other irrelevant details. This issue is illustrated in Figure 2.
To address this issue, we propose EchoVideo, which integrates the IITF module specifically designed to capture high-level semantic information, including refined facial identity features. Unlike the dual facial guidance mechanisms [7, 10, 8], this structural approach facilitates a pre-fusion integration that significantly simplifies the complexity of multimodal information fusion learning within the pre-trained DiT. Consequently, this enables the DiT to efficiently generate videos that reflect the target identity characteristics, guided by the pre-fusion features of IITF. Notably, this multimodal information fusion module is designed to be pluggable, facilitating seamless adaptation to other tasks and effectively addressing the challenges associated with information fusion learning in similar applications. Another existing challenge is that users often desire not only to preserve their facial identity but also to retain additional attributes such as clothing and hairstyle from the portrait images. Existing methods [14, 15, 16, 17, 18, 19] within the open-source community, require supplementary pose information for control, which considerably increases the usability barrier for users. To tackle this, we conducted experiments with EchoVideo aimed at this task, which validated that human control can be achieved solely through text prompts, demonstrating promising performance. This is illustrated in Figure 1.
Our contributions can be summarized as follows:
2


Running Title for Header
Figure 3: Overall architecture of EchoVideo. By employing a meticulously designed IITF module and mitigating the over-reliance on input images, our model effectively unifies the semantic information between the input facial image and the textual prompt. This integration enables the generation of consistent characters with multi-view facial coherence, ensuring that the synthesized outputs maintain both visual and semantic fidelity across diverse perspectives.
• We introduce EchoVideo, an identity-preserving model based on DiT that effectively maintains a high degree of identity similarity while addressing common "copy-paste" problem encountered in such contexts. Furthermore, EchoVideo not only preserves facial identity but also ensures consistency in full-body representations, including attributes such as clothing and hairstyle.
• We propose a multi-modal fusion module, termed IITF, which integrates textual semantics, image semantics, and facial image identity. This module effectively extracts clean identity information and resolves semantic conflicts between modalities. To the best of our knowledge, this is the first approach that combines these three modalities for identity preservation in video generation. Additionally, this architecture is designed to be plug-and-play, allowing it to be applied to other pre-trained generative models.
2 Related Work
2.1 Diffusion for Video Generation
The introduction of diffusion models has rapidly supplanted GANs [20, 21, 22] and auto-regressive models [23, 24, 25] in the fields of image and video generation due to their superior performance. Initially, video generation based on U-net models [26, 27, 28, 29, 30] primarily built upon text-to-image models [31]. These models incorporated temporal attention blocks to facilitate learning along the temporal dimension, achieving subtle video motion. However, they still fall short in terms of duration, realism, and dynamic quality compared to real videos. The emergence of SORA [32] has underscored the potential of DiT-based video generation models to significantly enhance video quality. Notably, Latte [33] marks the first application of DiT in the video generation domain, integrating temporal learning modules into text-to-video models like PixArt [34] to enable video-level transfer. Following this, there has been an explosive proliferation of DiT-based video generation works [35, 36, 37, 38, 39]. These emerging works, particularly the open-source initiatives, have accelerated the development of applications related to video generation, including image-to-video translation, video continuation, motion control [40, 41], and identity-preserving video generation.
2.2 Identity-preserving Video Generation
Identity-preserving generation aims to retain distinct identity attributes in generated images or videos. Building on the remarkable success of diffusion models in text-to-image synthesis, numerous methods [42, 43, 44, 45, 46] have emerged that leverage these models for identity-preserving image generation. However, achieving identity preservation at the video level introduces additional complexity, as it requires maintaining identity consistency across frames while ensuring the naturalness of facial movements. Among the U-Net-based approaches, MagicMe [7] relies on fine-tuning with reference images, while ID-Animator [8] introduces a face adapter to encode facial features, allowing for tune-free operation. However, both methods have limitations regarding facial similarity and overall video quality. In contrast,
3


Running Title for Header
Figure 4: Illustration of facial information injection methods. (a) Dual branch. Facial and textual information are independently injected through Cross Attention mechanisms, providing separate guidance for the generation process. (b) IITF. Facial and textual information are fused to ensure consistent guidance throughout the generation process.
several open-source DiT-based methods, such as ConsisID [10] and Magic Mirror [9], leverage the robust CogVideoX [37] model as a foundation, integrating facial information through cross-attention mechanisms to achieve identity preservation. FantasyID [47] incorporates a 3D facial geometry prior to ensure plausible facial structures during video synthesis. Despite generating high-quality videos and maintaining strong facial similarity, these methods suffer from significant “copy-paste” and “semantic conflicts” artifacts. To address these challenges, we propose EchoVideo, which incorporates a multimodal feature fusion module named IITF. This model effectively decouples facial features, enabling the generation of more stable and coherent portrait videos.
3 Preliminary: Diffusion Model
The forward process of the diffusion model is a process of progressively adding Gaussian noise to perturb the video through a Markov chain, which can be expressed as
zt = √α ̄tzvideo + √1 − α ̄tε, ε ∼ N (0, I), (1)
where zvideo is the VAE-encoded video, α ̄t = Qt
i=1(1 − βi) determines the variance of the noise and controls the
noise-to-signal ratio (NSR) of the noisy image. βt ∈ (0, 1) is a parameter that increases monotonically over time defined in advance. When t is very large, the noisy image in the forward process tends towards standard Gaussian noise.
The inference (reverse) process starts from a standard Gaussian noise and gradually transfers it to the target data distribution according to the condition through the Gaussian transition qθ (zt−1 | zt, y) = N (zt−1; μθ(zt, t; y), σt). [48] claims that this process can be non-Markovian, and gives the sampling formula in the reverse process as
zt−1 = 1
√1 − βt
zt − √1 − α ̄tεθ(zt, t; y) +
q
1 − α ̄t−1 − σt2εθ(zt, t; y) + σtε, (2)
where y represents the input image, σt = η
q (1−α ̄t−1) (1−α ̄ t )
√βt; η ∈ [0, 1]; when η = 0, the sampling process is
deterministic, known as Denosing Diffusion Implicit Model (DDIM), whereas at η = 1, the sampling process aligns with that of DDPM [49]; εθ(zt, t; y) is the noise estimated by using zt, t, y through the denoising network, and its training loss function is L1 loss between the estimated noise and the real noise, shown as
L = Et,ε[∥ε − εθ(zt, t; y)∥2]. (3)
4


Running Title for Header
Figure 5: Qualitative results. (a) Ours. (b) ConsisID [10]. (c) ID-Animator [8]. Our model can effectively overcome semantic conflicts and copy-paste phenomena while maintaining the face IP.
4 Methodology
4.1 Overall Architecture
The overall structure of EchoVideo is shown in the Figure 3. The model architecture is based on a conditional DiT video generation model. Through innovative fusion of the target person’s facial image and text prompt, we achieve precise preservation of personal identity characteristics to generate high-quality personalized video content.
For image input with faces Iin ∈ RH×W ×3, we first use a face extractor to locate the facial region Iface ∈ RH′ ×W ′ ×3, Iface = F ACE_EXT RACT OR(Iin) (4)
To comprehensively capture facial feature information, we design a dual-branch feature extraction architecture to obtain facial visual features Fvision and identity information Fid, considering both overall semantic information and local fine-grained features of the face to provide more complete identity information for subsequent video generation.
Specifically, Fvision is obtained using the high-performance SigLip [50] encoder:
Fvision = SigLip(Iface) (5)
This feature contains high-level semantic information such as overall facial structure. Fid contains local features of facial details, ensuring precise modeling of key facial regions, extracted through arcface [51]:
Fid = arcf ace(Iface) (6)
For the obtained facial features, we use the Identity Image-Text Fusion (IITF) module to fuse them with textual description information, achieving alignment and integration between different modalities. This module can correct potential inconsistencies between text descriptions and actual facial attributes (such as age, gender, etc.), ensuring identity consistency in the final generated content through feature-level correction.
Additionally, similar to previous works, we use VAE to encode the obtained facial region as conditional input for DiT, ensuring that the model can obtain correct low-dimensional visual information while maintaining facial semantic information.
5


Running Title for Header
Figure 6: Effect of the IITF module. (a) Without IITF. (b) With IITF. IITF can effectively extract facial semantic information and resolve conflicts with text information, generating consistent characters while maintaining the face IP.
4.2 Identity Image-Text Fusion Module
In existing IP video generation methods, there are notable limitations in how textual and facial modality information is utilized. Specifically, these two modalities are injected through separate Cross Attention modules, a design that prevents the model from effectively coordinating and integrating character feature information contained in different modalities. This problem becomes particularly evident when there are discrepancies between the input facial image and text description. As shown in the Figure 2(a), the facial image shows a child while the text description refers to an adult male. Due to the lack of deep inter-modal interaction, the model often produces a simple "face swap" effect rather than truly understanding and fusing feature information from both modalities.
Therefore, in this paper, we propose IITF to fuse text and facial information, establishing a semantic bridge between facial and textual information, coordinating the influence of different information on character features, thereby ensuring the consistency of generated characters. IITF consists of two core components: facial feature alignment and conditional feature alignment.
Facial Feature Alignment. Since SigLip [50] is a pretrained image-language model, Fvision can naturally align with the semantic space of text. However, Fid is obtained through a pure vision model. Directly fusing it with Fvision would make it difficult for the model to effectively utilize both types of information due to feature space misalignment. Therefore, in IITF, we introduce a learnable lightweight mapping module Prj [46] to map Fid, aligning it with Fvision in feature space:
F′
id = P rj(Fid) (7)
Since Fid focuses more on detailed facial features while Fvision focuses more on overall facial information and is less affected by external changes like lighting and occlusion, fusing both types of information ensures accuracy of facial features in generated results:
Fface = M LP (concat(Fvision, F ′
id)) (8)
Conditional Feature Alignment. Text descriptions Fprompt typically contain key attribute information of the target person (such as gender, age, etc.), while facial features Fface also implicitly contain related visual features. Inconsistencies between these two types of information often lead to "copy-paste" phenomena in generated results. To avoid this, in IITF, we fuse facial features Fface with text features Fprompt. Through interaction between both types of information, we unify the guidance direction for character features in the generation process, ensuring consistency of character features in output results:
Fcond = M LP (concat(Fprompt, Fface)) (9)
4.3 Data and Training
Although Fface can effectively extract high-level semantic information of faces, low-dimensional visual information about faces is often insufficient, leading to loss of high-frequency facial details in generated videos, producing blurry effects. Therefore, similar to previous works, we use VAE-encoded features zface of padded Iface as conditional input to the model. Since zface is a strong condition, this would cause the model to attempt using zface’s face directly for all
6


Running Title for Header
Figure 7: Effect of using facial visual features encoded by VAE. (a) Without face visual features. (b) With face visual features. By using the facial visual information , the facial details in the generated video can be effectively supplemented.
video frames, producing stiff expressions and fixed viewpoints, as shown in the figure. Therefore, to avoid excessive model dependence on zface, we adjusted our training data and strategies.
Data. In the training data, we try to select faces outside of the training videos, avoiding facial information with the same lighting and angles as in the videos, driving the model to generate faces from different points of view based on the Fface information. Additionally, we randomly drop zface during training to ensure correct perception of Fface.
Loss function. Besides using the L2 loss function shown in equation 2 to supervise noise prediction in the diffusion process, we added extra loss for facial regions to ensure model perception of faces. From equation 1, the original features can be calculated from model-predicted noise as:
zpred = √1α ̄t
(zt − √1 − α ̄tεθ) (10)
where εθ is the predicted noise by model. In previous work, supervision of latent-space faces was often based on downsampled facial masks in pixel space, ensuring computational efficiency during loss calculation. However, due to spatiotemporal interactions during video VAE encoding, obtaining latent space masks directly through resizing may lead to misalignment issues. Therefore, we propose using MTCNN-extracted face detection boxes to supervise facial regions:
Lbox = M askbox∥zvideo − zpred∥2 (11)
The final loss function used is:
L = Lddpm + λLbox (12)
5 Experiments
5.1 Settings
Dataset Preparation. We utilized a text-to-video dataset collected from the Internet as our primary dataset and filtered it to select segments containing only a single individual for training purposes. To establish image-video pairs, we first applied a face detector on the raw text-to-video content, extracting one frame per second for face detection. We retained only those detected faces that exhibited high quality and moderate face sizes across the entire video. Subsequently, we employed a face recognition model to extract facial features from all detected faces within the video. If the extracted facial features clustered into a single identity, the video was preserved; otherwise, it was discarded due to the presence of multiple identities. In constructing the image-video pair dataset, we generated three distinct types of data.
• Paired Data. For each text-to-video pair, we uniformly sampled five frames containing faces to serve as the corresponding images for that video. This approach resulted in the construction of a dataset comprising 3M paired data samples.
• Cross-Paired Data. To mitigate the risk of the model learning a trivial copy-paste shortcut solution, we created cross-paired data by interleaving images of the same individual from different video. This strategy yielded an additional 0.3M cross-paired data samples.
7


Running Title for Header
Model Identity Average ↑ Identity Variation ↓ Inception Distance ↓ Dynamic Degree ↑
ID-Animator [8] 0.349 0.032 159.11 0.280 ConsisID [10] 0.414 0.094 200.40 0.871 pika† [12] 0.329 0.091 268.35 0.954 Ours 0.516 0.075 176.53 0.955
Table 1: Quantitative comparison with SOTA IPT2V methods. The best is in bold; the second best is underlined.
.
• Generated Cross-Paired Data. To further enhance the diversity of non-identity information in the imagevideo pairs, we employed an identity-preserving text-to-image model [43], to generate faces with varying poses, lighting conditions, and expressions based on the faces detected in the video. This process resulted in the creation of another 0.3M cross-paired data samples.
Training Details. We utilized CogVideoX-5B [37] as the foundational model, employing Siglip [50] to extract facial visual features and a face recognition network [51] to obtain facial embeddings. The input consisted of video data at a resolution of 480p, comprising 49 frames per video. The training process was divided into two phases: the pre-training phase of the IITF module and the subsequent training of the entire EchoVideo model, which incorporated facial visual features encoded by VAE. During the pre-training phase of the IITF module, we set the batch size to 256 and the learning rate for IITF to 2 × 10−4, with a total of 20K iterations. In the complete training phase of the EchoVideo, we increased the batch size to 320 and adjusted the learning rate to 2 × 10−5, conducting a total of 50K iterations. Additionally, the drop ratio for image latent, which serve as shallow facial information, was set to 0.1, while the weight for the mask loss was configured to 1.0.
Evaluation. To mitigate the influence of visual feature extractors on celebrity faces, we carefully select 23 images of ordinary individuals from the FFHQ dataset [21] as test subjects, ensuring that these individuals are not part of the training set. The selection aims for balanced representation across gender, age, and ethnicity, while also considering diversity in facial poses, lighting conditions, and occlusions. We design 23 prompts based on the methodology from VBench [52], corresponding to various scenarios, activities, and facial poses. The evaluation of the IPT2V model emphasizes human-annotated results. We select a baseline model and employ the Good Same Bad (GSB) metric for quantification. Evaluation metrics are categorized into two dimensions: video quality and identity preservation. The video quality dimension follows the established text-to-video evaluation framework, subdivided into sensory quality, adherence to instructions, physical simulation, and coverage quality; The identity preservation dimension involves annotators comparing which video maintains closer identity consistency with the provided inference face image. Additionally, we utilize various tools, including VBench [52], to assess video generation quality and facial recognition embedding similarity for evaluating the effectiveness of identity preservation.
5.2 Results
Qualitative Results. We present a comparative analysis of EchoVideo against other methods in Figure 5. The IDAnimator [8] model, limited by parameter scale, exhibits noticeably inferior visual quality and realism compared to the other two approaches, and it shows a lack of responsiveness to text prompts. Meanwhile, ConsisID [10] suffers from an over-reliance on shallow information, resulting in a pronounced copy-paste effect in the generated faces. For instance, in case 1, the "coffee cup" from the reference image appears unchanged in the generated video. Additionally, ConsisID [10] fails to effectively integrate multimodal information, leading to semantic conflicts that manifest as "head-body coordination" issues, as shown in case 2. In contrast, our method not only achieves video quality and identity preservation comparable to ConsisID [10] but also effectively addresses the aforementioned challenges.
Quantitative Results. We present the quantitative results of various methods across five dimensions in Table 1. The evaluation of generated videos is conducted using the dynamic degree and aesthetic quality metrics from VBench [52] to assess motion amplitude and visual appeal, respectively. Additionally, the Fréchet Inception Distance [53] is utilized to quantify the visual quality of the generated videos in the facial region. For identity preservation, we calculate the mean and standard deviation of the cosine similarity [54] between the facial features of the reference image and the video frame. In terms of facial similarity and video dynamics, EchoVideo demonstrates a superior advantage, while the performance in other dimensions is comparable to other methods.
5.3 Ablation Studies
In this section, we conduct ablation studies to demonstrate the effectiveness of our IITF module and training strategy.
8


Running Title for Header
Model Identity Average ↑ Identity Variation ↓ Inception Distance ↓ Dynamic Degree ↑
w/o IITF 0.025 0.017 268.09 0.898 w/o Two-Stage 0.495 0.073 183.46 0.951 w/o Mask Loss 0.386 0.077 180.80 0.941 EchoVideo 0.516 0.075 176.53 0.955
Table 2: Ablation studies. “w/o IITF” denotes the exclusion of IITF module. “w/o Two-Stage” indicates the omission of the second training stage. “w/o Mask Loss” refers to the absence of the box-based face mask loss.
IITF. To demonstrate that the IITF module effectively integrates identity information for controlling the face generation in videos while decoupling irrelevant factors from the reference image, such as occlusion and lighting, we conducted an ablation study by replacing the IITF module with a unimodal text encoder. As illustrated in Figure 6, the absence of the IITF module leads to a loss of identity control in the generated videos. This finding is further supported by the significant decline in facial similarity observed in Table 2, where the FaceSim scores indicate a marked decrease in identity preservation.
Training Strategy. We compared the necessity of incorporating shallow facial information training in the second phase of our training strategy. As shown in Figure 6, the absence of VAE-encoded image latent leads to a degradation of facial detail in the generated outputs compared to the complete EchoVideo results, particularly in the regions of the eyes and eyebrows. This observation is further supported by the FaceSim scores presented in Table 2, which indicate that the training conducted in the second phase effectively enhances facial similarity.
Mask Loss. We evaluated the impact of using face detection bounding boxes as mask regions for loss calculation. The quantitative results in Table 2 demonstrate that the incorporation of strong supervision for facial regions significantly enhances the similarity and stability of face generation.
6 Conclusion
In this paper, we introduce EchoVideo, a unified framework for achieving identity consistency in human video generation through multimodal feature fusion. This framework not only maintains consistency in facial features but also extends to full-body representations. The multimodal integration module, IITF, is specifically developed to effectively capture clean facial identity information while aligning semantic information across different modalities in a pre-fusion manner. By leveraging IITF, we effectively address the copy-paste issue commonly encountered in video generation. Furthermore, EchoVideo can be seamlessly integrated into existing DiT-based or U-Net-based text-to-video or text-to-image models. Extensive experiments demonstrate that EchoVideo achieves state-of-the-art performance in identity-preserving tasks.
References
[1] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Structured causal disentanglement in variational autoencoder. arXiv preprint arXiv:2004.08697, 2020.
[2] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.
[3] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847, 2023.
[4] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted gaussian splatting from blurry images. arXiv preprint arXiv:2405.20224, 2024.
[5] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cycle3d: High-quality and consistent image-to-3d generation via generation-reconstruction cycle. arXiv preprint arXiv:2407.19548, 2024.
[6] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Instadrag: Lightning fast and accurate drag-based image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024.
[7] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024.
[8] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024.
9


Running Title for Header
[9] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magic mirror: Id-preserved video generation in video diffusion transformers. arXiv preprint arXiv:2501.03931, 2025.
[10] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024.
[11] Shengshu Technology. Vidu. https://www.vidu.com/, 2024.
[12] Pika Labs. Pika. https://pika.art/, 2025.
[13] Minimax. Hailuo. https://hailuoai.video/, 2025.
[14] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9326–9336, 2024.
[15] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1481–1490, 2024.
[16] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8153–8163, 2024.
[17] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024.
[18] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024.
[19] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024.
[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.
[21] Tero Karras. A style-based generator architecture for generative adversarial networks. arXiv preprint arXiv:1812.04948, 2019.
[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110–8119, 2020.
[23] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873–12883, 2021.
[24] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.
[25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821–8831. Pmlr, 2021.
[26] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.
[27] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.
[28] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8850–8860, 2024.
[29] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836, 2024.
[30] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399–417. Springer, 2025.
10


Running Title for Header
[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.
[32] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024.
[33] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.
[34] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.
[35] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024.
[36] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024.
[37] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.
[38] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: A high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024.
[39] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: A powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024.
[40] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024.
[41] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024.
[42] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8640–8650, 2024.
[43] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.
[44] Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, and Yi Yang. Caphuman: Capture your moments in parallel universes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6400–6409, 2024.
[45] Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, et al. Consistentid: Portrait generation with multimodal fine-grained identity preserving. arXiv preprint arXiv:2404.16771, 2024.
[46] Cheng Yu, Haoyu Xie, Lei Shang, Yang Liu, Jun Dan, Liefeng Bo, and Baigui Sun. Facechain-fact: Face adapter with decoupled training for identity-preserved personalization. arXiv preprint arXiv:2410.12312, 2024.
[47] Yunpeng Zhang, Qiang Wang, Fan Jiang, Yaqi Fan, Mu Xu, and Yonggang Qi. Fantasyid: Face knowledge enhanced id-preserving video generation. arXiv preprint arXiv:2502.13995, 2025.
[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.
[49] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.
11


Running Title for Header
[50] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975–11986, 2023.
[51] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690–4699, 2019.
[52] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21807–21818, 2024.
[53] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.
[54] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5901–5910, 2020.
12