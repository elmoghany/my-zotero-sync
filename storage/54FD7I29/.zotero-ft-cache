arXiv:2506.10540v1 [cs.MA] 12 Jun 2025
AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation
HAOYUAN SHI, Harbin Institute of Technology, Shenzhen, China YUNXIN LI, Harbin Institute of Technology, Shenzhen, China XINYU CHEN, Harbin Institute of Technology, Shenzhen, China LONGYUE WANG, Alibaba International Digital Commerce, Hangzhou, China BAOTIAN HU∗, Harbin Institute of Technology, Shenzhen, China MIN ZHANG, Harbin Institute of Technology, Shenzhen, China
Fig. 1. A visual example of AniMaker generating compelling storytelling animation from narrative text. Our framework maintains consistent character appearance across scenes while delivering high-quality action representation for complex sequences. AniMaker seamlessly integrates adaptive shot scheduling with smooth transitions, ensuring narrative coherence throughout the animation.
Despite rapid advancements in video generation models, generating coherent, long-form storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation’s logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover, collectively realizing multicharacter, multi-scene animation. Central to AniMaker’s approach are two
∗Corresponding author: Baotian Hu.
key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animationspecific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.
Additional Key Words and Phrases: storytelling animation, multi agents, MCTS, storyboard generation
1


Haoyuan Shi, Yunxin Li, Xinyu Chen, et al
1 INTRODUCTION
The emergence of large-scale multimodal models [Kong et al. 2024; Li et al. 2025; Liu et al. 2024c; Yang et al. 2024] has ignited significant interest in storytelling videos. In response, a new wave of methods [Dalal et al. 2025; Guo et al. 2025; Kim et al. 2024; Wu et al. 2024] including MINT Video and TTT-Video has emerged, aiming to generate the entire video in a single pass. However, these methods still face significant challenges when generating long videos spanning multiple scenes and characters, particularly in maintaining visual continuity, narrative coherence, and non-repetitive content. In contrast, recent advancements in single-clip video generation, exemplified by models like Wan [Wan et al. 2025], Vidu[Bao et al. 2024], and Pika [Pika Labs 2025], have pushed generated video clips closer to cinematic quality. Due to shorter clip duration, these models utilize textual and visual prompts directly and efficiently, improving motion quality, semantic fidelity, and consistency with reference images. Consequently, utilizing these models to generate a storytelling video with multiple clips appears highly promising. Existing storytelling video frameworks [He et al. 2024; Li et al. 2024; Lin et al. 2023; Wu et al. 2025; Xie et al. 2024; Xu et al. 2025], which implement the composition of multiple video clips, generally follow a fixed pipeline: script → scene images → video clips → final video composition. While this modular format can effectively generate multi-shot long videos, it also introduces several limitations: Firstly, existing methods typically map scene images to fixed-length clips. It forms a rigid and fragmented video construction, leading to disjointed transitions and unnatural pacing, severely hindering the expressive continuity crucial for complex or extended actions. Secondly, due to the inherent instability of video generation models, a single flawed clip can noticeably degrade the overall video quality. To mitigate fragmentation, an intuitive approach is generating continuous clips conditioned on prior frames, but this compounds error propagation and quality degradation. Drawing parallels with professional filmmaking, we identify that existing methods overlook Best-of-N Sampling—generating multiple clip candidates and selecting the best ones. However, implementing this faces two challenges: prohibitive computational costs and inadequate automated evaluation mechanisms. Generating and evaluating multiple candidate clips per shot is computationally intensive, often relying on expensive commercial APIs or prolonged GPU inference. Current evaluation metrics like VBench [Xing et al. 2024] only assess individual clips and their internal consistency, neglecting critical elements such as cross-clip coherence, sequential motion quality, and animation-specific qualities in storytelling animation. To address these challenges, we introduce AniMaker, a multiagent framework with MCTS-driven clip generation. This multiagent framework mirrors professional production, including the Director Agent for storyboards, Photography Agent for clip generation, Reviewer Agent for evaluating the quality of clip candidates, and Post-Production Agent for editing the entire videos. These agents collaborate to enable automated multi-character, multiscene storytelling without manual pre- or post-processing. The core Photography and Reviewer agents interact with each other under an MCTS-Gen scheme during clip generation. Specifically, based on Monte Carlo Tree Search (MCTS), MCTS-Gen offers an efficient
strategy for navigating the vast candidate space of video generation. It strikes a balance between broad exploration and computational efficiency by intelligently allocating more generation opportunities to promising clips while encouraging the exploration of unexplored regions. For Reviewer Agent, we introduce a comprehensive evaluation framework AniEval, specifically designed for multi-shot storytelling animation. AniEval advances beyond metrics like VBench by implementing retrospective evaluation with cross-clip contextual references. It evaluates critical dimensions—story consistency, action completion, and animation-specific attributes—by analyzing each clip in the context of its preceding and succeeding clips. This contextual assessment serves as the quality evaluation mechanism for video-clip nodes within the MCTS-Gen framework. We conduct extensive experiments on the dataset constructed from TinyStories [Eldan and Li 2023], featuring complex interaction with multiple characters across diverse backgrounds. Experimental results demonstrate AniMaker’s superior performance across VBench, AniEval, and human evaluation, with significantly improved multi-candidate generation efficiency. This validates AniMaker’s effectiveness in bringing AI-generated storytelling videos closer to production-grade quality (as shown in Figure 1). In summary, our main contributions are in three key aspects:
• We propose AniMaker, a fully automated multi-agent framework for generating coherent, multi-character, multi-scene animation from textual stories. The framework features MCTS-Gen, a novel search-based generation strategy that efficiently balances exploration and resource usage. • We develop AniEval, the first comprehensive evaluation framework specifically designed for multi-shot storytelling animation. It provides context-aware assessment by analyzing clips in relation to their surrounding content. • Experimental results demonstrate AniMaker’s superior performance across all evaluation frameworks—achieving best scores across all scene image metrics, ranking 1st in VBench, 14.6% higher scores in our AniEval, and better human ratings (3.22 versus 2.07).
2 RELATED WORK
Storyboard Visualization. Storyboard visualization bridges script to video by generating coherent image sequences, demanding flexible, reusable character-background modules for multiple scenes. While adapter-based techniques like IP-Adapter [Ye et al. 2023], Mixof-Show [Gu et al. 2023], T2I-Adapter [Mou et al. 2024], ROICtrl [Gu et al. 2024], and StoryAdapter [Mao et al. 2024] realize character consistency, they often overlook background continuity. Consistencyaware methods try to address this challenge. StoryGen [Liu et al. 2024b] iteratively generates images using previous visual-language context, while StoryDiffusion [Zhou et al. 2024] employs a trainingfree Consistent Self-Attention module to improve feature alignment. Despite these advancements, the precise preservation of visual details, especially with multiple characters or changing backgrounds, remains a challenge.
Video Generation. Driven by the success of diffusion models, VDM [Ho et al. 2022] pioneers their application to video generation, leading to improved models like Stable Video Diffusion [Blattmann et al.
2


AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation
Fig. 2. The overall architecture of our AniMaker framework. Given a story input, Director Agent creates detailed scripts and storyboards with reference images. Photography Agent generates candidate video clips using MCTS-Gen, which optimizes exploration-exploitation balance. Reviewer Agent evaluates clips with our AniEval assessment system. Post-production Agent assembles selected clips, adds voiceovers, and synchronizes audio with subtitles. This multi-agent system enables fully automated, high-quality animated storytelling.
2023], ModelScope [Wang et al. 2023], VideoCrafter1 [Chen et al. 2023], and VideoCrafter2 [Chen et al. 2024]. While these models excel at animating single text or image, they struggle with maintaining consistency across multiple video clips. Following Sora[Liu et al. 2024c], research has been increasingly focusing on coherent storytelling videos, with several works[Dalal et al. 2025; Guo et al. 2025; Kim et al. 2024; Wu et al. 2024] enhancing temporal modeling and contextual understanding. Despite these advancements, challenges persist as single-pass long video generation often suffers from inconsistencies and repetitive content over time. Considering this, models like HunyuanVideo [Kong et al. 2024], Wan [Wan et al. 2025], and CogVideoX [Yang et al. 2024] still prioritize the effective utilization of text prompts and reference images, resulting in more faithful generation and improved motion depiction. Industry solutions [Bao et al. 2024; Pika Labs 2025; Pixverse AI 2025; Runway AI, Inc. 2025] extend these advancements, enabling features like start/end frame-specific control.
Storytelling Video Agent. Storytelling video generation typically follows a pipeline: script → scene images → video clips → final video composition. Large multimodal models (LMMs) often act as high-level planners, coordinating each stage. Models such as VideoDirectorGPT [Lin et al. 2023], DreamStory [He et al. 2024], AnimDirector [Li et al. 2024], MM-StoryAgent [Xu et al. 2025], MovieAgent [Wu et al. 2025], and DreamFactory [Xie et al. 2024] adhere to this conventional pipeline. Notably, certain tasks like scriptwriting, character design, voiceovers, and sound effects are typically handled manually during pre- or post-processing.
3 METHODOLOGY
Our proposed framework, AniMaker, automates the creation of storytelling animation from text. It uses a multi-agent system that mirrors professional animation pipelines, incorporating novel components for efficient candidate generation and comprehensive evaluation. The overall architecture is detailed in Figure 2.
3.1 Task Formulation
Automated generation of storytelling animation Vfinal, from text input Tprompt, can be represented as:
F : Tprompt → Vfinal
This transformation is realized by mapping through several crucial intermediate representations:
• Script (Pscript): Derived from Tprompt, the script
Pscript = ( (shotk )Nclips
k=1 , Kcut )
defines: – An ordered sequence of Nclips shots, where each shotk = (dk, Ck, Bk ) specifies its textual description dk , involved characters Ck , and background Bk . – A set of indices Kcut ⊆ {1, . . . , Nclips}, marking shot transitions, indicating where KeyFrame guidance is required for the new shot. • Storyboard(Sboard): Storyboard is established from Pscript, containing:
3


Haoyuan Shi, Yunxin Li, Xinyu Chen, et al
– Character Bank: Bchar = {(c, Ichar
c ) | c ∈ Ctotal}, with
Ichar
c as the reference image for character c, where Ctotal represents all available characters in the story.
– Background Bank: Bbg = {(b, Ibg
b ) | b ∈ Btotal}, with Ibg
b as the reference image for background b, where Btotal represents all available backgrounds in the story.
– KeyFrames: for each shotk where k ∈ Kcut , a KeyFrame Fkey
k
is defined: Fkey
k = Visualize({Ichar
c | c ∈ Ck }, Ibg
B
k
, dk )
where Visualize(·) generates a keyframe by integrating the specified characters, background, and text description into a multimodal prompt. • Video Clip Sequence (v): v = (v1, . . . , vNclips ) is generated, cor
responding to (shotk )Nclips
k=1 . Let GK and GC be abstract generative
processes of each clip vk :
– If k ∈ Kcut (after shot transition): vk ∼ GK (Fkey
k
, dk ). – If k ∉ Kcut : vk ∼ GC (last_frame(vk −1), dk ). • Final Video (Vfinal): The assembled clip sequence Assemble(v) then undergoes post-processing, utilizing information from Pscript to produce the polished Vfinal. This may include additions like voiceovers and subtitles.
Vfinal = PostProcess(Assemble(v), Pscript)
3.2 Pipeline Overview
AniMaker transforms textual input (Tprompt) into compelling storytelling animation (Vfinal) through four specialized agents working collaboratively. The Director Agent (Section 3.3.1) creates a detailed storyboard (Sboard), the Photography Agent (Section 3.3.2) generates candidate video clips (vk ) using MCTS-Gen (efficiently exploring generation space conserving computational resources), the Reviewer Agent (Section 3.3.3) evaluates clips with AniEval (our context-aware evaluation framework), and the Post-production Agent (Section 3.3.4) assembles clips with voiceover.
3.3 Multi-Agent Framework: AniMaker
3.3.1 Director Agent. The Director Agent orchestrates storyboard generation through a two-stage process. First, Gemini 2.0 Flash [Google Cloud 2025] creates a raw script with shot descriptions (shotk ), followed by automated validation for consistency and narrative flow. Second, in the Storyboard (Sboard) realization phase, a Visual Bank is built: Character Bank is built (Bchar) with Hunyuan3D [Zhao et al. 2025] and Background Bank (Bbg) is built with FLUX1-dev [Flux AI 2025]. Then, GPT-4o [Hurst et al. 2024] gen
erates keyframes (Fkey
k ) combining validated shot descriptions (dk ) with Visual Bank imagery. This ensures visual consistency, with the resulting Sboard serving as the animation production blueprint in the following phases. 3.3.2 Photography Agent. Converting storyboards to clips in multi-shot AI video generation presents challenges including distorted appearances, inconsistent motion, and object inconsistencies. Inspired by filmmaking’s "NG" concept and "gacha" methods prevalent in AIGC, we recognize the necessity of generating multiple candidate clips to select the optimal one. For optimal selection, each
clip must not only possess high individual quality but also ensure consistency and coherence with both preceding and succeeding clips. However, naively generating k candidates per clip creates a combinatorial explosion (e.g., k2 for two-clip sequences). Notably, poor-quality current clips allow for pruning the search space, as further exploration down such paths is unlikely to be satisfactory. Thus we propose MCTS-Gen, a Monte Carlo Tree Search (MCTS)inspired method for multi-clip video generation (Figure 3). MCTS naturally fits this task: multi-clip sequences correspond to tree paths, with each clip as a node. Crucially, clip evaluation considers both intrinsic quality and inter-clip consistency, aligning with MCTS’s backpropagation where child’s scores update the parent’s evaluation results. MCTS-Gen iteratively constructs a chosen path of selected video clips, extending this path by one clip per iteration. This process is controlled by the following parameters: w1 (initial candidate count per node), w2 (UCT-guided expansion times per iteration), and α (exploration-exploitation balancing factor, default 1). Specifically, we use Wan 2.1 [Wan et al. 2025] for video clip generation, and the algorithm proceeds as follows: 1. Expansion: w1 (3 in Figure 3) initial child clips are generated from the chosen path’s terminal node (node 1 in Figure 3). These clips (node 3, 4, 5 in Figure 3) are then scored using AniEval (detailed in Section 3.3.3) and ranked. 2. Simulation: Further w2 (3 in Figure 3) expansions are generated guided by the UCT score:
U CT (nodej) = 2.0
rank (nodej) + 1 + α ·
√︄
2.0
child_count (nodej) + 1
where rank is from the initial AniEval scoring, child_count is dynamically updated, and α balances exploitation and exploration. The node with the highest UCT score generates a new child clip. 3. Backpropagation: AniEval scores of child clips (nodes 6, 7, 8 in Figure 3) propagate upwards. A parent node’s score is updated by adding the average score of children to its own. 4. Selection: The node with the highest AniEval score (node 3 in Figure 3) is added to the chosen path, then generating new child clips until reaching a total of w1 children, allowing the iterative generation process to continue. This MCTS-Gen process systematically balances the exploration of diverse video generation space with the exploitation of promising paths, aiming to efficiently construct a high-quality, coherent video sequence while managing computational resources.
3.3.3 Reviewer Agent. Existing objective metrics, such as CLIP Score and Inception Score, may identify superior video generation models, but often struggle to differentiate among candidates from the same model using the same prompt. Similarly, the widely adopted VBench has significant limitations. For instance, some of its metrics, like "dynamic degree," are overly simplistic, merely measuring pixel changes rather than accurately reflecting character action. More critically, VBench’s "Consistency" metrics, based on single-clip segmentation, prove unsuitable for multi-shot animation, which inherently involves frequent character and scene changes. To address evaluation challenges, we introduce AniEval, a comprehensive evaluation framework built on EvalCrafter [Liu et al. 2024a]. AniEval refines EvalCrafter’s metrics for fully automated
4


AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation
Fig. 3. Illustration of our MCTS-Gen strategy for efficient Best-of-N Sampling.
evaluation, e.g., by automating action assessment through comparing prompted character actions with those identified in video clips. Furthermore, responding to the specific demands of evaluating multi-shot animation characterized by multiple characters and diverse scenes, AniEval introduces several additional metrics: DreamSim [Fu et al. 2023] assesses overall frame consistency; CountScore [Cheng et al. 2024] aims at the issue of objects appearing or disappearing between shots; Face Consistency evaluates animated character facial consistency by training an InceptionNext [Yu et al. 2024] model on the Anime Face Dataset [splcher 2019], overcoming the limitations of conventional face recognition methods like MTCNN [Ku and Dong 2020] in anime face detection and tracking. In conclusion, AniEval comprises 4 primary domains with 14 fine-grained metrics for comprehensive assessment (Table 1). Additionally, AniEval supports contextual scoring by evaluating clips based on preceding and succeeding content, providing robust evaluation for multi-shot animation generation. 3.3.4 Post-production Agent. The Post-production Agent transforms video clip sequences into a polished animation film through three stages. Firstly, it leverages Gemini 2.0 Flash to generate a detailed voiceover script specifying narration, dialogue, emotional tones, and desired voice timbres. The agent then selects appropriate voice profiles based on character attributes (age, gender) and assesses text length for audio-visual synchronization. Secondly, the script is processed through CosyVoice2 [Du et al. 2024] to generate
Table 1. AniEval Metrics.
Domain Metric Brief Description
Overall Video Quality VQA_A Aesthetic video quality VQA_T Technical video quality MusIQ Frame quality score
Text-Video Alignment Text-Video Consistency Measured by CLIP Text-Story Consistency Measured by BLIP-BLEU Detection-Score Object generation accuracy Count-Score Key object count accuracy
Video Consistency DreamSim Perceptual frame-frame similarity Face Consistency Animated character facial consistency Warping Error Temporal inconsistency via pixel differences Semantic Consistency Temporal semantic coherence via CLIP
Motion Quality Action Recognition Actions-prompt consistency Action Strength Motion intensity via Flow-Score Motion AC-Score Motion amplitude-prompt consistency
audio tracks, which undergo verification for duration consistency and content accuracy. Finally, the agent employs the MoviePy library for film assembly, integrating validated subtitles and performing comprehensive editing to ensure precise synchronization between visuals, voiceover, and subtitles.
5


Haoyuan Shi, Yunxin Li, Xinyu Chen, et al
Table 2. Scene Image evaluation on Contextual Coherence (Coherence), Image-Image Similarity (I-I Sim), and Text-Image Similarity (T-I Sim).
Model Coherence↑ I-I Sim↑ T-I Sim↑
StoryGen 0.54 0.77 0.22 StoryDiffusion 0.70 0.80 0.25 StoryAdapter 0.78 0.83 0.25 MovieAgent 0.59 0.65 0.23 MMStoryAgent 0.78 0.83 0.26 VideoGen-of-Thought 0.71 0.77 0.23 AniMaker(Ours) 0.81 0.83 0.31
4 EXPERIMENTS 4.1 Settings
4.1.1 Datasets. To evaluate our AniMaker, we sample 10 narratives from TinyStories [Eldan and Li 2023]. These narratives feature complex multi-character interactions across diverse backgrounds, providing an ideal testbed for multi-shot animation generation. 4.1.2 Baseline. We evaluate several state-of-the-art storytelling models: StoryGen, StoryDiffusion, and StoryAdapter (visual narrative specialists), alongside MovieAgent, MMStoryAgent, and VideoGenof-Thought (video generators). The latter group utilizes their built-in video modules, while StoryDiffusion and StoryAdapter are paired with external image-to-video models (CogVideoX and Wan 2.1). 4.1.3 Evaluation Metrics. For scene image generation, we evaluate text-to-image alignment and cross-image consistency. Metrics include Text-to-Image CLIP (Coherence) [Radford et al. 2021], Imageto-Image Similarity (I-I Sim) [Gal et al. 2022], and Text-Image Similarity (T-I Sim) [Hessel et al. 2021]. Video generation is assessed using VBench and our AniEval for comprehensive evaluation.
4.2 Qualitative Analysis
Based on samples in Figures 5 and 6, we qualitatively analyze AniMaker’s output, focusing on visual fidelity and narrative coherence in multi-character, multi-scene animated storytellings. Enhanced Consistency. AniMaker achieves superior visual consistency across scenes. While baselines struggle with character and background consistency during transitions, AniMaker maintains visual characteristics even when switching between different scenes. This stems from the Director Agent’s storyboard creation, providing consistent reference images for characters and backgrounds. Additionally, AniEval ensures coherent clip selection by evaluating cross-clip consistency metrics.
Improved Action Representation. AniMaker excels at depicting complex and extended character actions. Baseline methods often produce incomplete movements, particularly for multi-step action sequences (e.g., squatting, picking up an object, standing up, and walking away in Figure 5). Our MCTS-Gen strategy enables the Photography Agent to explore and select clip sequences that concatenate into coherent, complete long actions.
Seamless Transitions Between Clips. AniMaker delivers smoother video transitions through effective generation and selection mechanisms. The Reviewer Agent uses AniEval, integrating cross-clip consistency metrics like DreamSim and applying contextual scoring across adjacent clips, minimizing visual disruptions.
Table 3. VBench evaluation results, presenting scores for Image Quality (I.Q.), Semantic Consistency (S.C.), Background Consistency (B.C.), Animation Quality (A.Q.), Motion Smoothness (M.S.), Dynamic Degree (D.D.), and Average Rank (Rk. Avg. - the average ranking position across all models).
Model I.Q.↑ S.C.↑ B.C.↑ A.Q.↑ M.S.↑ D.D.↑ Rk. Avg.↓
StableDiffusion+Cog. 75.52 78.05 85.27 59.61 97.63 33.58 5.83 StableDiffusion+Wan. 76.93 78.54 87.43 69.75 96.67 60.30 4.33 StoryAdapter+Cog. 76.17 72.17 88.03 63.55 98.16 26.71 5.33 StoryAdapter+Wan. 75.96 75.04 88.64 73.38 97.02 84.73 4.17 MovieAgent 72.09 68.61 79.84 55.40 99.01 35.44 6.33 MMStoryAgent 76.41 87.27 90.74 73.84 99.80 0.00 2.67 VoT 63.85 75.11 85.78 74.91 99.25 3.50 4.83 AniMaker(Ours) 76.96 84.27 89.06 69.79 98.50 66.97 2.50
In summary, AniMaker outperforms existing methods through superior visual consistency across scenes, effective depiction of complex action sequences, seamless inter-clip transitions, and robust handling of multi-character, multi-scene narratives.
4.3 Quantitative Comparisons
Scene Image Generation Analysis. Scene images (Keyframes) are the core output of a storyboard. Table 2 demonstrates that AniMaker outperforms all competing methods across metrics. Notably, AniMaker achieves a Text-to-Image Similarity (T-I Sim) score of 0.31, representing a 19.2% improvement over the best-performing baseline. This advantage stems from our multimodal storyboard generation approach that incorporates character references, background references, and text prompts for keyframe generation, rather than relying solely on text.
VBench Evaluation Analysis. Table 3 shows that AniMaker achieves the best average rank (2.50), demonstrating consistent top-tier performance across metrics. While MMStoryAgent excels in Semantic Consistency (S.C.) and Background Consistency (B.C.), its 0.00 score in Dynamic Degree (D.D.) reveals a critical limitation—it produces static image sequences resembling comic strips rather than true animation. This exposes VBench’s limitations: it favors static consistency (where even brief static images can achieve high scores) and focuses on individual clips rather than multi-scene, multi-character animation. These findings highlight the need for a new evaluation framework specifically designed for storytelling animation quality.
AniEval Evaluation Analysis. We introduce AniEval for better evaluation of storytelling Animation. Table 4 presents results from our AniEval framework. AniMaker outperforms all competitors with a total score of 76.72, representing a 14.6% improvement over the second-best model (VideoGen-of-Thought at 66.93). Particularly noteworthy is AniMaker’s exceptional performance in Video Consistency (V.C.), surpassing the best-performing baseline by 15.5%. AniMaker’s relatively lower performance in Text-Video Alignment (T.V.A.) can be attributed to our agent framework’s creative adaptation of stories into scripts, where additional narrative elements are introduced. Compared to VBench, AniEval’s assessment results align remarkably better with human evaluations (Table 5), confirming that AniEval provides a more comprehensive and accurate assessment of multi-shot animation quality than previous metrics.
6


AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation
Fig. 4. AniEval Score of Different w1 (initial candidate count) and w2 (expansion iterations) Combinations.
MCTS-Gen Parameter Analysis. Figure 4 shows how MCTS-Gen parameters w1 (initial candidate count) and w2 (expansion iterations) affect generation quality. Two patterns emerge: higher w1 values improve AniEval scores by providing more initial candidates, while higher w2 values enhance performance by better evaluating each clip’s suitability for continuous generation. Importantly, once certain thresholds are reached, configurations with fewer total generations (e.g., w1 = 3, w2 = 3 with 4.37 generations per node) perform comparably to those requiring more (e.g., w1 = 3, w2 = 5 with 5.76 generations per node). This demonstrates MCTS-Gen’s efficiency—compressing the search space by over 50% compared to exhaustive search strategies (which require 9 generations per node for the same candidates) while maintaining quality.
Table 4. AniEval evaluation results, presenting scores for Overall Video Quality (O.V.Q.), Text-Video Alignment (T.V.A.), Video Consistency (V.C.), Motion Quality (M.Q.), and Total Performance.
Model O.V.Q.↑ T.V.A.↑ V.C.↑ M.Q.↑ Total↑
StoryDiffusion+CogVideoX 46.54 86.05 47.14 70.35 56.75 StoryDiffusion+Wan 2.1 47.07 84.99 47.13 71.00 56.55 StoryAdapter+CogVideoX 56.76 87.38 55.89 69.73 63.95 StoryAdapter+Wan 2.1 60.39 86.99 51.41 72.11 62.37 MovieAgent 41.17 68.50 68.68 70.16 61.95 MMStoryAgent 47.93 75.27 63.54 61.39 62.79 VideoGen-of-Thought 66.17 72.95 65.42 66.72 66.93 AniMaker(Ours) 81.87 74.30 79.35 72.66 76.72
4.4 Human Rating
Following MovieAgent’s evaluation framework [Wu et al. 2025], we conduct human evaluation with 10 participants on 90 storytelling videos from 9 models across 10 stories. Each video was rated on a 1-5 scale across five dimensions: Visual Appeal, Script Faithfulness, Narrative Coherence, Character Consistency, and Physical Law Adherence. Note that evaluating complete storytelling videos (rather than individual clips) typically yields lower scores due to the increased complexity and length. As shown in Table 5, our model achieves superior performance across all metrics, with particularly strong results in Character Consistency.
Table 5. Human rating results, presenting scores for Character Consistency (C.C.), Narrative Coherence (N.C.), Physical Law Adherence (P.L.), Script Faithfulness (S.F.), and Visual Appeal (V.A.) using a 1-5 scale.
Model C.C.↑ N.C.↑ P.L.↑ S.F.↑ V.A.↑ Avg.↑
StoryDiffusion+CogVideoX 1.37 1.48 1.37 1.67 1.56 1.49 StoryDiffusion+Wan 2.1 2.00 1.82 1.82 2.00 2.11 1.95 StoryAdapter+CogVideoX 1.64 1.39 1.46 1.68 1.71 1.58 StoryAdapter+Wan 2.1 2.04 1.82 1.89 2.04 2.57 2.07 MovieAgent 1.19 1.26 1.44 1.26 1.48 1.33 MM_StoryAgent 1.62 1.62 1.72 1.83 2.24 1.81 VideoGenoT 1.67 1.74 1.78 1.74 2.26 1.84 AniMaker(Ours) 3.44 3.24 3.04 3.08 3.28 3.22
4.5 Ablation Studies
We conduct two key ablation experiments. First, we ablate MCTSGen by setting w1 = 1, w2 = 1, essentially generating only one candidate per clip. This change results in a 7.1% reduction in performance (the green triangle in Figure 4) on AniEval, confirming the importance of our MCTS-driven generation strategy. Notably, even this ablated version still outperforms the best-performing baseline method by 6.6%, further demonstrating that our multi-agent framework remains highly competitive even without clip candidate selection. Next, we ablate AniEval by generating five candidates per clip with VBench for selection. This yields a score of 73.18, a 4.6% decrease compared to our full method. Qualitative assessment of the resulting videos also reveals noticeable degradation in action expressiveness and cross-clip consistency, highlighting the importance of AniEval for storytelling animation.
5 LIMITATIONS
Despite employing state-of-the-art models for storyboard and video clip generation, a gap remains between current model capabilities and commercial film production quality. Key limitations include poor adherence to physical laws and unrealistic interactions between scene elements, creating bottlenecks in production-ready output. However, our framework adopts a modular, plug-and-play architecture that enables seamless model integration. As more advanced models emerge, we will continuously update our framework and share these workflow improvements with the community, especially mitigating the hallucination of models [Wang et al. 2024].
6 CONCLUSION
We present AniMaker, a comprehensive multi-agent framework that transforms text input into coherent storytelling animation by emulating professional workflows. Our system introduces two key innovations: MCTS-Gen, which optimizes exploration-exploitation balance during clip generation, and AniEval, the first evaluation framework specifically designed for multi-shot storytelling animation. AniMaker orchestrates specialized agents that seamlessly collaborate across storyboarding, generation, evaluation, and postproduction stages. Our quantitative results validate the effectiveness of this approach, with substantial gains in both technical metrics and perceived quality. These advances mark an important step toward bridging the gap between AI-generated content and professional animation standards, paving the way for more accessible and highquality animated storytelling production.
7


Haoyuan Shi, Yunxin Li, Xinyu Chen, et al
REFERENCES
Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. 2024. Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233 (2024).
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023).
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. 2023. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 (2023). Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. 2024. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7310–7320.
Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, et al. 2024. Theatergen: Character management with llm for consistent multi-turn image generation. arXiv preprint arXiv:2404.18919 (2024). Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. 2025. One-minute video generation with test-time training. arXiv preprint arXiv:2504.05298 (2025). Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. 2024. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117 (2024). Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759 (2023). Flux AI. 2025. Flux AI Official Website. https://flux1ai.com/ Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. 2023. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344 (2023). Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). Google Cloud. 2025. Gemini 2.0 Flash. https://cloud.google.com/vertex-ai/generativeai/docs/models/gemini/2- 0- flash Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. 2023. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems 36 (2023), 15890–15902. Yuchao Gu, Yipin Zhou, Yunfan Ye, Yixin Nie, Licheng Yu, Pingchuan Ma, Kevin Qinghong Lin, and Mike Zheng Shou. 2024. ROICtrl: Boosting Instance Control for Visual Generation. arXiv preprint arXiv:2411.17949 (2024). Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. 2025. Long context tuning for video generation. arXiv preprint arXiv:2503.10589 (2025).
Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang, Zeyu Liu, Wenhao Huang, Hongyang Chao, and Jian Yin. 2024. Dreamstory: Open-domain story visualization by llm-guided multi-subject consistent diffusion. arXiv preprint arXiv:2407.12899 (2024).
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718 (2021).
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. 2022. Video diffusion models. Advances in Neural Information Processing Systems 35 (2022), 8633–8646. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).
Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. 2024. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473 (2024). Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. 2024. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024). Hongchang Ku and Wei Dong. 2020. Face recognition based on mtcnn and convolutional neural network. Frontiers in Signal Processing 4, 1 (2020), 37–42. Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, and Min Zhang. 2025. Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models. arXiv preprint arXiv:2505.04921 (2025).
Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, and Min Zhang. 2024. Anim-director: A large multimodal model powered agent for
controllable animation video generation. In SIGGRAPH Asia 2024 Conference Papers. 1–11. Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. 2023. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091 (2023).
Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. 2024b. Intelligent grimm-open-ended visual storytelling via latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6190–6200.
Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. 2024a. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 22139–22149.
Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. 2024c. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177 (2024).
Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, and Yuyin Zhou. 2024. Story-adapter: A training-free iterative framework for long story visualization. arXiv preprint arXiv:2410.06244 (2024).
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. 2024. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, Vol. 38. 4296–4304. Pika Labs. 2025. Pika Labs Official Website. https://pika.art/ Pixverse AI. 2025. Pixverse AI: Official Website. https://pixverse.ai/ Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748–8763.
Runway AI, Inc. 2025. Runway AI: Official Website. https://runwayml.com/ splcher. 2019. Anime Face Dataset. https://www.kaggle.com/datasets/splcher/ animefacedataset Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. 2025. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 (2025).
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. 2023. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571 (2023). Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. 2024. Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding. In Findings of the Association for Computational Linguistics: ACL 2024.
Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. 2025. Automated Movie Generation via Multi-Agent CoT Planning. arXiv preprint arXiv:2503.07314 (2025). Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, and Sergey Tulyakov. 2024. Mind the Time: TemporallyControlled Multi-Event Video Generation. arXiv preprint arXiv:2412.05263 (2024). Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F Bissyand, and Saad Ezzini. 2024. Dreamfactory: Pioneering multi-scene long video generation with a multi-agent framework. arXiv preprint arXiv:2408.11788 (2024).
Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. 2024. A survey on video diffusion models. Comput. Surveys 57, 2 (2024), 1–42. Xuenan Xu, Jiahao Mei, Chenliang Li, Yuning Wu, Ming Yan, Shaopeng Lai, Ji Zhang, and Mengyue Wu. 2025. MM-StoryAgent: Immersive Narrated Storybook Video Generation with a Multi-Agent Paradigm across Text, Image and Audio. arXiv preprint arXiv:2503.05242 (2025).
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. Cogvideox: Textto-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024). Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023).
Weihao Yu, Pan Zhou, Shuicheng Yan, and Xinchao Wang. 2024. Inceptionnext: When inception meets convnext. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. 5672–5683.
Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. 2025. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202 (2025).
Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. 2024. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems 37 (2024), 110315–110340.
8


AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation
Fig. 5. A comparative case showcasing AniMaker and models specialized in visual narratives. This figure illustrates the visualization of the short story of Tom and Lily. In the story, Tom brings a sack of toys to the town square, where he meets a sad girl named Lily who has no toys. Tom offers to share his toys, and the two children happily play together. Three models—StoryDiffusion, StoryAdapter, and AniMaker (ours)—are compared. AniMaker demonstrates superior narrative consistency, emotional expression, and character continuity across frames. It coherently depicts the extended action sequence of Tom picking up the sack, leaving his house, and arriving at the square. In contrast, while StoryDiffusion and StoryAdapter capture key moments from the story, they suffer from inconsistencies in visual coherence and character alignment, with mismatched character appearances highlighted by red boxes in the figure.
9


Haoyuan Shi, Yunxin Li, Xinyu Chen, et al
Fig. 6. A comparative case showcasing AniMaker and models capable of generating storytelling videos. This figure visualizes the story of Sue, a little girl who tries to climb a big tree in the park but gets scared. Her friend Tom warns her to be careful, and she climbs down safely. Grateful, Sue hugs Tom, and they play on the swings together. The comparison includes MovieAgent, MMStoryAgent, VideoGen-of-Thought, and AniMaker (ours). AniMaker stands out with coherent scene progression, expressive character interactions, and consistent character identities. It clearly captures Sue’s emotional journey and key events—from climbing the tree and feeling afraid, to receiving help and having fun—demonstrating strong temporal and narrative alignment. In contrast, MovieAgent shows limited relevance to the input story, with inconsistent visuals and abstract content. VideoGen-of-Thought and MMStoryAgent follow the narrative more closely but still suffer from visual continuity issues, with character mismatches highlighted in red boxes.
10