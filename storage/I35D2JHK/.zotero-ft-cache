1
From Sora What We Can See:
A Survey of Text-to-Video Generation
Rui Sun*, Yumin Zhang*†, Tejal Shah, Jiahao Sun, Shuoying Zhang, Wenqi Li, Haoran Duan, Bo Wei, Rajiv Ranjan Fellow, IEEE
Abstract—With impressive achievements made, artificial intelligence is on the path forward to artificial general intelligence. Sora, developed by OpenAI, which is capable of minute-level world-simulative abilities can be considered as a milestone on this developmental path. However, despite its notable successes, Sora still encounters various obstacles that need to be resolved. In this survey, we embark from the perspective of disassembling Sora in text-to-video generation, and conducting a comprehensive review of literature, trying to answer the question, From Sora What We Can See. Specifically, after basic preliminaries regarding the general algorithms are introduced, the literature is categorized from three mutually perpendicular dimensions: evolutionary generators, excellent pursuit, and realistic panorama. Subsequently, the widely used datasets and metrics are organized in detail. Last but more importantly, we identify several challenges and open problems in this domain and propose potential future directions for research and development. A comprehensive list of text-to-video generation studies in this survey is available at https://github.com/soraw-ai/Awesome-Text-to-Video-Generation
Index Terms—Diffusion Transformer Model, Video Generation, Text-to-Video, AIGC
✦
1 INTRODUCTION
Recent rapid advancements in the field of AI-generated content (AIGC) mark a pivotal step toward achieving Artificial General Intelligence (AGI), particularly following OpenAI’s introduction of their Large Language Model (LLM), GPT-4, in early 2023. AIGC has attracted significant attention from both the academic community and the industry, highlighted by developments such as the LLM-based conversational agent ChatGPT [1], and text-to-image (T2I) models like DALL·E [2], Midjourney [3] and Stable Diffusion [4]. These achievements have significantly influenced the text-to-video (T2V) domain, culminating in the remarkable capabilities showcased by OpenAI’s Sora [5], as shown in Fig. 1. As elucidated in [5], Sora is engineered to operate as a sophisticated world simulator, crafting videos that render realistic and imaginative scenes derived from textual instructions. Its remarkable scaling capabilities enable efficient learning from internet-scale data, achieved through the integration of the DiT model [6], which supersedes the traditional U-Net architecture [7]. This strategic incorporation aligns Sora with advancements similar to those in GenTron [8], W.A.L.T [9], and Latte [10], enhancing its generative prowess. Uniquely, Sora has the capacity to produce minutelong videos of high quality, a feat not yet accomplished by existing T2V studies [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26]. It also excels in generating videos with superior resolution and seamless
• Rui Sun, Yumin Zhang, Tejal Shah, Wenqi Li, Haoran Duan, Bo Wei,Rajiv Ranjan are with Newcastle University, UK. E-mail: {ruisun, haoran.duan}@ieee.org, {Tejal.Shah, W.Li31, Bo.Wei, Raj.Ranjan}@newcastle.ac.uk • Jiahao Sun and Shuoying Zhang are with FLock.io, UK. E-mail: {sun, shuoying}@flock.io • * These authors contributed equally to the work. • † The corresponding author (email: Y.Zhang361@newcastle.ac.uk).
quality, paralleling advancements in existing T2V approaches [27], [28], [29], [30], [31], [32], [33], [34], [35]. While Sora significantly enhances the generation of complex objects, surpassing previous works [36], [37], [38], [39], it encounters challenges with ensuring coherent motion among these objects. Nonetheless, it is paramount to recognize Sora’s superior capability in rendering complex scenes with intricate details, both in subjects and backgrounds, outperforming prior studies focused on complex scene [24], [40], [41], [42], [43] and rational layout generation [42], [44], [45]. Based on our best knowledge, there are two surveys that are relevant to ours: [46] and [47]. [46] encompasses a broad spectrum of topics from video generation to editing, offering a general overview, but focusing on only limited diffusion-based text-to-video (T2V) techniques. Concurrently, [47] presents a detailed technical analysis of Sora, providing an elementary survey of related techniques, yet it lacks depth and breadth specifically in the T2V domain. In response, our work seeks to bridge this gap by delivering an exhaustive review of T2V methodologies, benchmark datasets, pertinent challenges and unresolved issues, along with prospective future directions, thereby contributing a more nuanced and comprehensive perspective to the field. Contributions: In this survey, we conduct an exhaustive review that focused on the text-to-video generation domain, through an in-depth examination of OpenAI’s Sora. We systematically track and summarize the latest literature, distilling the core elements of Sora. This paper also elucidates the foundational concepts, including the generative models and algorithms pivotal to this field. We delve into the specifics of the surveyed literature, from the algorithms and models employed to the techniques employed for producing high-quality videos. Additionally, this survey provides an extensive survey of T2V datasets and relevant evaluation metrics. Significantly, we illuminate the current
arXiv:2405.10674v1 [cs.CV] 17 May 2024


2
The late afternoon sun peeking through the window of a New York City loft.
runway
stability ai An exploding cheese house
LED chinese dragon hovering over chinese town
male wizards in wizard hats and wizard cloaks casting magic spell at McDonalds
Morph Studio
TGANs-C
a cook puts noodles into some boiling water.
Text2Filter
Swimming in swimming pool
NUWA
running on the sea
Text2Video Generation
A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.
Sora
CHALLENGES OPEN PROBLEMS FUTURE DIRECTIONS
Fig. 1: Text-to-video (T2V) generation is a flourishing research area, which has gone through several iterations in recent years. Early works are limited to simple scenes (low-resolution, single-object, and short-duration). Subsequently, benefiting from the success achieved by the diffusion model in the generative area, current works are generating more complex videos and various tools have been commercially successful. Sora, with longer prompts processing capacity and minute-level world-simulative video generation, is an extremely promising T2V tool but it also faces challenges and open problems.
challenges and open problems in T2V research, proposing future directions based on our insights. Sections Structure: The paper is organized as follows: In Section 2, we provide a foundational overview, including the objectives of T2V generation, along with the core models and algorithms underpinning this technology. In Section 3, we primarily offer an extensive overview of all pertinent fields based on our observations of Sora. In Section 4, we conduct a detailed analysis to underscore the challenges and unresolved questions in T2V research, drawing particular attention to insights gleaned from Sora. Section 5 is dedicated to outlining prospective future directions, informed by our analysis of existing research and the pivotal aspects of Sora. The paper culminates in Section 6, where we present our concluding observations, synthesizing the insights and implications drawn from our comprehensive review.
2 PRELIMINARIES
2.1 Notations
Given a collection of n videos and associated text description {Vi, Ti}n
i=1, each video Vi ∈ RT ×C×H×W contains k frames
Vi = {f 1
i ,f2
i ,··· ,fk
i }, where C is the number of color bands, H and W are the number of pixels in the height and width of one frame, and T reflects the time dimension. Guided by the input prompts {T ∗
j }m
j=1, the goal of text
to-video (T2V) is to generate synthetic videos {V∗
j }m
j=1 via the designed generator.
2.2 Foundational Models and Algorithms
2.2.1 Generative Adversarial Networks (GAN)
GAN is an unsupervised machine learning model implemented by a system of two neural networks contesting with each other in a zero-sum game framework [48]. GAN
is made up of a generator and a discriminator where the generator’s role is to produce data (such as images) that are indistinguishable from real data, while the discriminator’s role is to distinguish between the generator’s fake and real data [49], [50]. The optimization objective of a GAN involves a minmax game between two entities: the generator G and the discriminator D. The overall target optimization function is formulated as:
mDax mGin V(G, D), (1)
where the value function V(D, G) is defined as:
Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 − D(G(z)))]. (2)
In this framework, the generator G aims to minimize the probability that the discriminator D makes a correct decision. Conversely, the discriminator endeavors to maximize its probability of correctly identifying the generator’s output. The real data x is sampled from the data distribution pdata(x), and the input to the generator, z, is sampled from a prior distribution pz(z), which is typically a uniform or Gaussian distribution. The expectation operator E(·) represents the expected value over the respective probability distributions. This adversarial process involves alternating between optimizing the discriminator to distinguish real data from fake, and optimizing the generator to produce data that the discriminator will classify as real. This training process continues until the generator produces data so close to the real data that the discriminator can no longer distinguish between the two.


3
x
x!
1/0
z x!
Discriminator Generator Encoder Decoder
x x!
GAN/VAE
Layer Norm
Linear and Reshape
Noise Variance
...
x
patchify
condition embedded
Layer Norm
MH Attention
C
Layer Norm
MH Attention
C
Layer Norm
MLP
C
embedding tokens
Layer Norm
DiT-blocks (Cross Attention) ×N
...
Diffusion-(U-Net-based)
Autoregressive-(Transformer-based)
...
denoising
feature embedding
Q
K V
MM
embedding
feature
KV
MM
Q
embedding
feature
KV
MM
Q
feature embedding
Q
K V
MM
x
condition embedded
Encoding
x
...
patchify
Encoder
Decoder
x!
Noise
Diffusion-(Transformer-based)
(Cross Attention blocks)
Tokenizer
,
....
Input text
Transformer
Positioned
...
Transformer
Fig. 2: Illustrations of different generators.
2.2.2 Variational Autoencoders (VAE)
VAE represents a class of deep generative models that are rooted, anchoring their design in the principles of Bayesian inference, which facilitates a structured approach for learning latent representations of data [51]. A VAE comprises of two primary components: an encoder and a decoder. The encoder, often realized through a neural network, functions by mapping input data to a probabilistic distribution in a latent space. Conversely, the decoder, also implemented as a neural network, aims to reconstruct the input data from this latent representation, thereby enabling the model to capture the underlying data distribution. Consider a dataset D = {x(1), x(2), ..., x(N)} consists of N independent and identically distributed (i.i.d.) samples drawn from a distribution p(x). The objective of a VAE is to approximate this distribution with a parametric distribution model pθ(x), where θ denotes the parameters of the model. This approximation is achieved by introducing a latent variable z, leading to the joint distribution expressed as:
pθ(x, z) = pθ(x|z)p(z). (3)
Here, p(z) is the prior over the latent variables, typically chosen to be a standard Gaussian. The term pθ(x|z) represents the likelihood, which is modeled by the decoder network. The encoder facilitates this process by approximating the posterior pθ(z|x) with a variational distribution qω(z|x), where ω symbolizes the encoder’s parameters. Training VAE is fundamentally about maximizing the evidence lower bound (ELBO) on the marginal likelihood of the observed data, which is formulated as:
L(θ, ω; x) = Eqω(z|x)[log pθ(x|z)] − KL[qω(z|x)∥p(z)], (4)
where the first term is the reconstruction loss, encouraging the decoded samples to match the original inputs, and the second term is the Kullback-Leibler divergence between the approximate posterior and the prior, acting as a regularize. By
optimizing the ELBO, VAEs learn to balance the reconstruction fidelity with the complexity of the latent representation, enabling them to generate new samples that are consistent with the observed data.
2.2.3 Diffusion Model
Diffusion models [52] are sophisticated generative models that are trained to create data by inverting a diffusion process, as described by Ho et al. in their 2020 work on denoising. These models incrementally introduce noise to data, eventually converting it into a Gaussian distribution through a specified series of steps. This transformation is represented by a Markov chain comprising latent variables {xt}tT=0, where x0 denotes the initial data and xT represents the fully noised data. The forward diffusion is characterized by a series of transition probabilities q(xt|xt−1) that systematically incorporate noise into the data:
q(xt|xt−1) = N (xt; p1 − βtxt−1, βtI), (5)
in which {βt}tT=1 are predefined small variances that gradually intensify the noise.
Denoising Diffusion Probabilistic Models (DDPM) [52], highlighted in [52], are a prominent subset of diffusion models. They interpret the generation process as a backward diffusion that sequentially purifies the data, converting random noise back into a sample from the intended distribution. DDPMs ascertain a denoising function εθ(xt, t) to estimate the noise added at each step, with the reverse mechanism defined by:
pθ(xt−1|xt) = N (xt−1; μθ(xt, t), Σθ(xt, t)), (6)
where μθ(xt, t) and Σθ(xt, t) are functions determined by the neural network that incrementally purify the data. The main objective in training involves enhancing the neural network to diminish the disparity between the noisy data and its denoised version, usually by optimizing the variational lower bound.


4
The key insight of DDPM is the inversion of the diffusion process, necessitating the estimation of the reverse conditional probabilities. By training the neural network to eliminate noise at each phase, the model is capable of producing high-quality samples from mere noise through the repetitive application of the learned denoising function.
2.2.4 Autoregressive Models
Autoregressive (AR) models represent a category of statistical models used for understanding and predicting future values in a time series [53]. The fundamental assumption of an AR model is that the current observation is a linear combination of the previous observations plus some noise term. The general form of an AR model of order o, denoted as AR(o), can be expressed as:
vt =
o
X
i=1
θivt−i + εt, (7)
where vt is the current value of the time series. θ1, θ2, . . . , θo are the parameters of the model. vt−1, vt−2, . . . , vt−o are the previous o values of the time series. εt is white noise, which is a sequence of uncorrelated random variables with mean zero and a constant finite variance. In the context of neural networks, autoregressive models have been extended to model complex patterns in data. They are fundamental to various architectures, particularly in sequence modeling and generative modeling tasks. By capturing the dependencies between elements in a sequence, autoregressive models can generate or predict subsequent elements in a sequence, making them crucial for applications such as language modeling, time series forecasting, and generative image modeling.
2.2.5 Transformers
The Transformer [54] model operates on the principle of self-attention, multi-head attention, and position-wise feedforward networks. Self-attention mechanism enables the model to prioritize different parts of the input sequence. It calculates attention scores using the equation:
Attention(Q, K, V ) = softmax QKT
√dk
V. (8)
Here, Q, K, and V correspond to the query, key, and value matrices, which are derived from the input embeddings, with dk being the key’s dimension.
Multi-head attention mechanism enhances the model’s capability to attend to various positions by applying different learned projections to the queries, keys, and values h times, executing the attention function concurrently. The results are concatenated and linearly transformed as shown:
MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO, (9)
where
headi = Attention(QWQ
i , KWK
i ,V WV
i ). (10)
WQ
i , WK
i , WV
i , and WO are the learned parameter matrices.
Position-Wise Feed-Forward Networks are included in each layer of the Transformer, applying two linear transformations with a ReLU activation in the middle to each position identically:
FFN(x) = max(0, xW1 + b1)W2 + b2. (11)
The weights and biases of the linear transformations are denoted by W1, b1, W2, and b2.
Layer Normalization and Residual Connection are utilized, with the Transformer adding residual connections around each sub-layer, followed by layer normalization. The output is given by:
LayerNorm(x + Sublayer(x)), (12)
where Sublayer(x) is the function implemented by the sublayer itself. Positional Encoding is incorporated into the input embeddings to provide position information, as the model lacks recurrence or convolution. These encodings are summed with the embeddings and are defined as:
P E(pos,2i) = sin(pos/100002i/dmodel ), (13)
P E(pos,2i+1) = cos(pos/100002i/dmodel ). (14)
Here, pos represents the position and i is the dimension.
3 FROM SORA WHAT WE CAN SEE
Following significant breakthroughs in text-to-image technology, humans have ventured into the more challenging domain of text-to-video generation, capable of conveying and encapsulating a richer array of visual information. Even though the pace of research in this realm has been gradual in recent years, the launch of Sora has dramatically reignited optimism, marking a momentous shift and breathing new life into the momentum of the field. Therefore, in this section, we systematically classify the key insights we saw from Sora especially T2V generation area into three main categories and offer detailed reviews for each: Evolutionary Generators (see Section 3.1), Excellent Pursuit (see Section 3.2), Realistic Panorama (see Section 3.3) and Dataset and Metrics (see Section 3.4). The comprehensive structure is illustrated in Fig. 3.
3.1 Evolutionary Generators
The technical report of the cutting-edge Sora [5] demonstrates that while the video generated from Sora represents a significant leap forward compared to existing works, the advancement in the algorithmic design of text-to-video (T2V) generators has been cautiously incremental. Sora primarily advances the field by refining existing works through intricate splicing and sophisticated optimization techniques. This observation underscores the importance of a comprehensive review of the currently available T2V algorithms. By examining the foundational algorithms underpinning contemporary T2V models, we categorize them into three principal frameworks: GAN/VAE-based, Diffusionbased, and Autoregressive-based.


5
B. Excellent Pursuit
From Sora
WHAT WE CAN SEE
1) Extended Duration:
2) Superior Resolution: Video LDM, Show-1, STUNet, MoCoGAN-HD, Text2Performer
3) Seamless Quality: DAIN, CyclicGen, Softmax-Splatting, FLAVR
A. Evolutionary Generators
C. Realistic Panorama
D. Datasets & Metrics
1) Datasets
2) Metrics
1) Dynamic Motion:
2) Complex Scene:
3) Multiple Objects:
4) Rational Layout:
1) GAN/VAE-Based:
2) Diffusion-based:
3) Autoregressive-based:
Open
Face Movie
Action
Instruct Cooking UCF-101
MSR-VTT
DideMo
YT-Tem-180M
WebVid2M
HD-VILA-100M
InternVid
HD-VG-130M
Youku-mPLUG
VAST-27M
Panda-70M
ActNet-200
Charades
Kinetics
ActivityNet
Charades-Ego
SS-V2
How2
HowTo100M
LSMDC MAD
YouCook2
Epic-Kitchens
CV-Text
Image-level Video-level
PSNR/SSIM
IS FID
CLIP Score Video IS
FVD/KVD
FCS
LTVR, TATS, Phenaki, LVDM, Make-a-Video, StyleInv, Vlogger, MoonShot, ControlNet, DIGAN, StyleGAN-V, Gen-L-Video, SEINE, SceneScape, NUWA-XL, MCVD
Sync-DRAW, VQ-VAE, TGANs-C, IRC-GAN, GODIVA, Text2Filter
DDPM, DiT, MotionDiffuse, LVDM, ImagenVideo, VDM, Make-A-Video, Tune-A-Video, MagicVideo, VideoCrafter1, SVD, ModelScopeT2V, Show-1, VideoLDM, NUWA-XL, Text2Videeo-Zero, PixelDance, GenTron, W.A.L.T, VideoCrafter2, MagicVideo-V2, Latte, Sora
VideoGPT, NUWA, NUWA-Infinity, TATS, CogVideo, Phenaki, LWM, Genie
LAMP, AnimateDiff, MotionLoRA, Lumiere, Dysen-VDM, ART•V, DynamiCrafter, PixelDance, MoVideo, MicroCinema, ConditionVideo, DreamVideo, TF-T2V, GPT4Motion, Text2Performer
VideoDirectorGPT, FlowZero, VideoDrafter, SenceScape
DeterctorGuidance, MOVGAN, VideoDreamer, UniVG
Craft, FlowZero, LVD
Fig. 3: The structure of section From Sora What We Can See.
2018
2017 2019 2020 2021 2022 2023 2024
VAE GAN
Sync-DRAW
GAN/VAE-based
VQ-VAE GODIVA
Text2Filter
TGANs-C IRC-GAN
Transformers
Key Model/Algorithm
DDPM
VDM
Diffusion-based
Make-A-Video
VideoCrafter1 VideoCrafter2
ModelScopeT2V
Video LDM
GenTron
DiT-based
W.A.L.T
DiT
Autoregressive-based
Sora
MotionDiffuse
Imagen Video
LVDM
MagicVideo
Tune-A-Video Text2Video-Zero
NUWA-XL
Show-1
SVD
PixelDance
MagicVideo-V2
Latte
VideoGPT
NUWA
NUWA-Infinity
CogVideo
TATS Phenaki
LWM Genie
Legends
Fig. 4: T2V Generators Evolutionary timeline based on foundational algorithms.
3.1.1 GAN/VAE-based
In the initial phases of exploration within the text-to-video domain, researchers primarily focused on devising generative models based on neural networks (NN), such as Variational Autoencoders (VAE) [55], [56], [57], [58] and Generative Adversarial Networks (GAN) [58], [59], [60]. These pioneering efforts laid the groundwork for understanding and developing complex video content directly from textual descriptions. A pioneering work in automatic video generation from textual descriptions is presented in [55], where the authors introduced an innovative method that integrates VAE with a recurrent attention mechanism. This approach generates sequences of frames that evolve over time, guided by textual inputs, which uniquely focus on each frame while learning a latent distribution across the entire video. Despite its innovation, the VAE framework faces a notable challenge termed ‘posterior collapse’. To mitigate this, [56] introduced the VQ-VAE model, an innovative model that combines the advantages of discrete representation with the efficacy of continuous models. Utilizing vector quantization, VQVAE excels in generating high-quality images, videos, and speech. Similarly, GODIWA [57] is designed based on VQVAE architecture pre-trained on HowTo100M [61] dataset, showcasing its ability to be fine-tuned on downstream video generation tasks and its remarkable zero-shot capabilities.
Rather than generating videos directly from textual input, the authors in [60] proposed TGANs-C, a method that creates videos from textual captions. This approach integrates a novel methodology that includes 3D convolutions and a multi-component loss function, ensuring the videos are both temporally coherent and semantically consistent with the captions. Expanding upon these innovations, [58] proposed an advanced hybrid model that combines a VAE and GAN, proficiently capturing both the static attributes (e.g., background settings and object layout) and dynamic elements (e.g., movements of objects or characters) from textual descriptions. This model elevates the process of video generation from mere textual inputs to a more complex and nuanced creation of video content based on textual narratives. In a complementary advancement, [59] innovatively combines GAN with Long Short-Term Memory (LSTM) [62] networks, significantly improving the visual quality and semantic coherence of text-generated videos, ensuring a closer alignment between the generated content and its textual descriptions.
3.1.2 Diffusion-based
In the groundbreaking paper by Ho et al. [52], the introduction of diffusion models marked a significant milestone in the text-to-image (T2I) generation domain, leading to the development of pioneering models such as DALL·E [2],


6
Midjourney [3], and Stable Diffusion [63]. Recognizing that videos fundamentally comprise sequences of images considered with spatial and temporal information, researchers have begun to investigate the potential of diffusion architecturebased models for generating high fidelity videos from textual descriptions. Video Diffusion Models (VDM) [64], a pioneering advancement in text-to-video generation, significantly enhance the standard image diffusion approach to cater to video data. VDM addresses the critical challenge of temporal coherence in generating high-fidelity videos. By innovating with a 3D U-Net [65] architecture, the model is tailored for video applications, enhancing traditional 2D convolutions to 3D while integrating temporal attention mechanisms to ensure the generation of temporally coherent video sequences. This design maintains spatial attention and incorporates temporal dynamics, enabling the model to generate coherent video sequences. In a similar vein, MagicVideo [66] and its successor, MagicVideo-V2 [66], are significant innovations in text-to-video generation, leveraging latent diffusion models to tackle challenges like data scarcity, complex temporal dynamics, and high computational costs. MagicVideo introduces a 3D U-Net-based architecture with enhancements like a video distribution adaptor and directed temporal attention, enabling efficient, high-quality video generation that maintains temporal coherence and realism. It operates in a latent space, focusing on keyframe generation and efficient video synthesis. MagicVideo-V2 builds upon this, incorporating a multi-stage pipeline with modules for Textto-Image, Image-to-Video, Video-to-Video, and Video Frame Interpolation. Latent space exploitation is a common theme among several models. LVDM [14] proposed a hierarchical latent video diffusion model that compresses videos into a lower-dimensional latent space, enabling efficient long video generation and reducing computational demands. The conventional framework is restricted to producing short videos, the length of which is predetermined by the number of input frames provided during the training phase. To overcome this limitation, the LVDM introduces a conditional latent diffusion approach, enabling the generation of future latent codes based on previous ones in an autoregressive fashion. Show-1 [28], PixelDance [67], and SVD [63] leverage both pixel-based and latent-based techniques for generating high-resolution videos. Show-1 starts with pixel-based VDMs for generating accurate, low-resolution keyframes and then uses latent-based VDMs for efficient high-resolution video enhancement, capitalizing on the strengths of both approaches to ensure high-quality, computationally efficient video outputs. PixelDance is built on a latent diffusion framework trained to denoise perturbed inputs within the latent space of a pre-trained VAE, aiming to minimize computational demands. The underlying structure is a 2D UNet diffusion model, enhanced to a 3D variant by incorporating temporal layers, including 1D convolutions and attentions along the temporal dimension, adapting it for video content while maintaining its efficacy with image inputs. This model, capable of joint training with images and videos, ensures highfidelity outputs by effectively integrating spatial and temporal resolutions. SVD incorporates temporal convolution and attention layers into a pre-trained diffusion architecture,
allowing it to capture dynamic changes over time efficiently. Tune-A-Video [68] extends these concepts by incorporating temporal self-attention to capture consistencies across frames, optimizing computational resources. Specifically, Tune-AVideo aims to solve the computational expense problem and extends a 2D Latent Diffusion Model (LDM) to the spatiotemporal domain to facilitate T2V generation. Researchers in [68] innovated by adding a temporal self-attention layer to each transformer block within the network, enabling the model to capture temporal consistencies across video frames. This design is complemented by a sparse spatio-temporal attention mechanism and a selective tuning strategy that updates only the projection matrices in attention blocks, optimizing for computational efficiency and preserving the pre-learned features of the T2I model while ensuring temporal coherence in the generated video. In the realm of video enhancement, VideoLCM [69] model is designed as a latent consistency model optimized with a consistency distillation strategy, focusing on reducing the computational burden and accelerating training. It leverages large-scale pre-trained video diffusion models to enhance training efficiency. The model applies the DDIM [70] solver for estimating the video output and incorporates classifierfree guidance to synthesize high-quality content, allowing it to achieve fast and efficient video synthesis with minimal sampling steps. VideoCrafter2 [71] model is distinctively engineered to enhance the spatial-temporal coherence in video diffusion models, employing an innovative datalevel disentanglement strategy that meticulously separates motion aspects from appearance characteristics. This strategic design facilitates a targeted fine-tuning process with highquality images, aiming to substantially elevate the visual fidelity of the generated content without compromising the precision of motion dynamics. This approach builds upon and significantly refines the groundwork laid by VideoCrafter1 [72], which is structured as a Latent Video Diffusion Model (LVDM), incorporating a video Variational Autoencoder (VAE) and a video latent diffusion process. The video VAE compresses the video data into a lowerdimensional latent representation, which is then processed by the diffusion model to generate videos. Models like Make-A-Video [73] and Imagen Video [74] extend text-to-image technologies to the video domain. MakeA-Video model leverages advancements in T2I technology and extends them into the video domain without needing paired text-video data. It is designed around three core components: a T2I model, spatiotemporal convolution and attention layers, and a frame interpolation network. Initially, it leverages a T2I model trained on text-image pairs, then extends this with novel spatiotemporal layers that incorporate temporal dynamics, and finally, employs a frame interpolation network to enhance the frame rate and smoothness of the generated videos. Imagen Video employs a sophisticated cascading architecture of video diffusion models specifically tailored for T2V synthesis. This design intricately combines base video diffusion models with subsequent stages of spatial and temporal super-resolution models, all conditioned on textual prompts, to progressively enhance the quality and resolution of generated videos. The model’s innovative structure enables it to produce videos that are not only high in fidelity and resolution but also


7
exhibit strong temporal coherence and alignment with the descriptive text. MotionDiffuse [75]: text-driven human motion generation, particularly the need for diversity and finegrained control in generated motions. utilizing a diffusion model, specifically tailored with a Cross-Modality Linear Transformer for integrating textual descriptions with motion generation. It enables fine-grained control over the generated motions, allowing for independent control of body parts and time-varied sequences, ensuring diverse, realistic outputs. Text2Video-Zero [76] is built upon the Stable Diffusion T2I model, tailored for zero-shot T2V synthesis. The core enhancements include introducing motion dynamics to the latent codes for temporal consistency and employing a crossframe attention mechanism that ensures the preservation of object appearance and identity across frames. These modifications enable the generation of high-quality, temporally consistent video sequences from textual descriptions without additional training or fine-tuning, leveraging the existing capabilities of pre-trained T2I models. NUWA-XL [25] introduces a novel “Diffusion over Diffusion” architecture designed for generating extremely long videos. The primary challenge it addresses is the inefficiency and quality gap in long video generation from existing method, NUWA-XL’s architecture innovatively employs a “coarse-to-fine” strategy. It starts with a global diffusion model that generates keyframes outlining the video’s coarse structure. Subsequently, local diffusion models refine these keyframes, filling in detailed content between them, enabling the system to generate videos with both global coherence and fine-grained details efficiently. Differing from the approach of fine-tuning pre-trained models, Sora aims at the more ambitious task of training a diffusion model from scratch, presenting a significantly greater challenge. Drawing inspiration from the scalability of transformer architectures, OpenAI has integrated the DiT [6] framework into their foundational model architecture, OpenAI shifts the diffusion model from the conventional U-Net [7] to a transformer-based structure, harnessing the Transformer’s scalable capabilities to efficiently train massive amounts of data and tackle complex video generative tasks. Similarly, aiming to enhance training efficiency through the integration of transformers and diffusion models, GenTron [8] builds upon the DiT-XL/2 structure, transforming latent dimensions into non-overlapping tokens processed through transformer blocks. The model’s innovation lies in its text conditioning, employing both adaptive layernorm and cross-attention mechanisms for integrating text embeddings and enhancing interaction with image features. A significant aspect of GenTron is its scalability; the GenTron-G/2 variant expands the model to over 3 billion parameters, focusing on the depth, width, and MLP width of transformer blocks. Concurrently, W.A.L.T [9] is based on a two-stage process incorporating an autoencoder and a novel transformer architecture. Initially, the autoencoder compresses both images and videos into a lower-dimensional latent space, enabling efficient training on combined datasets. The transformer employs window-restricted self-attention layers, alternating between spatial and spatiotemporal attention, which significantly reduces computational demands while supporting joint image-video processing. This structure facilitates the generation of high-resolution, temporally consistent
videos from textual descriptions, showcasing an innovative approach in T2V synthesis. Latte [10] further extends these innovations by employing a series of Transformer blocks to process latent space representations of video data, which are obtained using a pre-trained variational autoencoder. This approach allows for the effective modeling of the complex distributions inherent in video data, handling both spatial and temporal dimensions innovatively.
3.1.3 Autoregressive-based
Recent advancements in T2V generation have prominently featured autoregressive-based transformers as well, recognized for their superior efficiency in handling sequential data and scalability. These attributes are pivotal for modeling the intricate temporal dynamics and high-dimensional data characteristics of video generation tasks. Transformers [77] are particularly advantageous due to their seamless integration with existing language models, which fosters the creation of coherent and contextually enriched multimodal outputs. A notable development in this area is NUWA [78], which integrates a 3D transformer encoder-decoder framework with a specialized 3D Nearby Attention mechanism, enabling efficient and high-quality synthesis of images and videos by processing data across 1D, 2D, and 3D dimensions, showcasing remarkable zero-shot capabilities. Building on this, NUWA-Infinity [79] introduces an innovative autoregressive over autoregressive framework, adept at generating variablesized, high-resolution visuals. It combines a global patchlevel with a local token-level model, enhanced by a Nearby Context Pool and an Arbitrary Direction Controller, to ensure seamless, flexible, and efficient visual content generation. Further extending these capabilities, Phenaki [13] extends the paradigm with its unique C-ViViT encoder-decoder structure, focusing on the generation of variable-length videos from textual inputs. This model efficiently compresses video data into a compact tokenized representation, facilitating the production of coherent, detailed, and temporally consistent videos. Similarly, VideoGPT [77] is a model that innovatively combines VQ-VAE and Transformer architectures to tackle video generation challenges. It employs VQ-VAE to learn downsampled discrete latent representations of videos through 3D convolutions and axial attention, creating a compact and efficient representation of video content. These learned latents are subsequently modeled autoregressively with a Transformer, enabling the model to capture the complex temporal and spatial dynamics of video sequences. The Large World Model (LWM) [80] represents another stride forward, designed as an autoregressive transformer to process long-context sequences, blending video and language data for multimodal understanding and generation. Key to its design is the RingAttention mechanism, which addresses the computational challenges of handling up to 1 million tokens efficiently, minimizing memory costs while maximizing context awareness. The model incorporates VQGAN for video frame tokenization, integrating these tokens with text for comprehensive sequence processing. On the other hand, Genie [81] model is crafted as a generative interactive environment tool, which incorporates spatiotemporal (ST) transformers across all its components, utilizing a novel video tokenizer and a causal action model to extract latent actions, which are then passed to a dynamics model. This


8
dynamics model autoregressively predicts the next frame, employing an ST-transformer architecture that balances model capacity with computational efficiency. The model’s design leverages the strengths of transformers, particularly in handling the sequential and spatial-temporal aspects of video data, to generate controllable and interactive video environments. TATS [12] is specifically designed for generating longduration videos, addressing the challenge of maintaining high quality and coherence over extended sequences. The model architecture innovatively combines a time-agnostic VQGAN, which ensures the quality of video frames without temporal dependence, and a time-sensitive transformer, which captures long-range temporal dependencies. This dual approach enables the generation of high-quality, coherent long videos, setting a new standard in the field of video synthesis. CogVideo [82] integrates a multi-frame-rate hierarchical training approach, adapting from a pretrained T2I model, CogView2 [83], to enhance T2V synthesis. This design inherits the text-image alignment knowledge from CogView2, using it to generate key frames from text and then interpolating intermediate frames to create coherent videos. The model’s dual-channel attention mechanism and recursive interpolation process allow for detailed and semantically consistent video generation.
3.2 Excellent Pursuit
Sora has shown its excellent video generation abilities from the presented demos with long-term duration, high resolution, and smoothness [84]. Based on such merits, we categorized current works into three streams: extended duration, superior resolution, and seamless quality.
3.2.1 Extended Duration
Compared with short video generation or image generation tasks, long-term video generation is more challenging as the latter requires the ability to model long-range temporal dependence and maintain temporal consistency with many more frames [5]. Specifically, with the duration extended of generative video, one obstacle is that the prediction error will accumulate. To address such a challenge, the retrospection mechanism (LTVR) [11] is introduced to push the retrospection frames to be consistent with the observed frames, and thus alleviate the accumulated prediction error. TATS [12] achieves the goal of long video generation by combining a time-agnostic VQGAN for high-quality frame generation and a time-sensitive transformer for capturing long-range temporal dependencies. Phenaki [13] is presented for generating videos from open-domain textual descriptions. By incorporating the designed casual attention, it is capable of working with variable-length videos and generating longer sequences by extending the video with new prompts. LVDM [14] proposed a hierarchical framework for generating longer videos, enabling the production of videos with over a thousand frames. Through leveraging T2I models for visual learning and unsupervised video data for motion understanding, Make-A-Video [73] can generate long videos with high fidelity and diversity without the need for paired
text-video data. StyleInV [15] is capable of generating long videos by leveraging the sparse training approach and the efficient design of the motion generator. By constraining the generation space with the initial frame and employing temporal style codes, the method achieves high single-frame resolution and quality, as well as temporal consistency over long durations. More recently, through enhancing spatialtemporal coherence in the snippet, Vlogger [16] successfully generates over 5-minute vlogs from open-world descriptions without losing video coherence regarding the script and actors. MoonShot [17] leverages the Multimodal Video Block that integrates spatial-temporal layers with decoupled crossattention for multimodal conditioning, and the direct use of pre-trained Image ControlNet [18] for precise geometry control, and thus to generate long videos with high visual quality and temporal consistency, efficiently handling diverse generative tasks. Additionally, inspired by the success of Implicit Neural Representations (INR) [19] in model complex signals, various works have introduced INR in video generation. For example, DIGAN [20] synthesizes long videos of high resolution without demanding extensive resources for training by leveraging the compactness and continuity of Implicit Neural Representations (INRs), allowing for efficient training on long videos. Similarly, StyleGAN-V [21] can efficiently produce videos of any length and at any frame rate by treating videos as continuous signals. Besides, various works from the perspective of decomposing to address the long-range consistency during the process of video generation. In GenL-Video [22], the long videos are first perceived as short clips. The bidirectional cross-frame attention is developed to mutual the influence among different video clips and thus facilitate finding a compatible denoising path for the long video generation. SEINE [23] adopts a similar approach that views long videos as compositions of various scenes and shot-level videos of different lengths. By synthesizing longterm videos of various scenes, SceneScape [24] is proposed to generate long-term videos from text descriptions, addressing the challenge of 3D consistency in video generation. NUWAXL [25] offers a novel solution based on a coarse-to-fine strategy, developing a “Diffusion over Diffusion” architecture that enables efficient and coherent generation of extremely long videos. MCVD [26] generates videos of arbitrary lengths by autoregressively generating blocks of frames, allowing for the production of high-quality frames for diverse types of videos, including long-duration content.
3.2.2 Superior Resolution
Compared to low-quality videos, high-resolution videos undoubtedly have a broader potential application such as a simulation engine in the context of autonomous techniques. On the other hand, high-resolution video generation also poses a challenge to the computational resources. Considering such a challenge, Video Latent Diffusion Models (LDM) [27] introduce the off-the-shelf pre-trained image LDM into the video generation while avoiding excessive computing demands. By training a temporal alignment model, Video LDMs can generate videos up to 1280×2048 resolution. However, LDMs struggle to generate a precise text-video alignment. In Show-1 [28], the advantages of pixel-based and


9
latent-based Video Diffusion Models (VDM) are combined as a hybrid model. Show-1 utilizes the pixel-based VDMs to init the low-resolution video generation and then uses latentbased VDMs for upscaling to get high-resolution videos (up to 572×320). Recently, STUNet [29] adopts a Spatial SupperResolution (SSR) model to upsample the output from the base model. Specifically, to avoid temporal boundary artifacts and ensure smooth transitions between temporal segments, Multi-Diffusion is employed along the temporal axis, which allows the SSR network to operate on short segments of the video, averaging predictions of overlapping pixels to produce a high-resolution video. From another perspective, video generation is regarded as a trajectory of discovering the problem in MoCoGAN-HD [30] which presents a framework that utilizes contemporary image generators to render highresolution videos (up to 1024×1024). In the text-driven human video generation tasks, a subfield of video generation, Text2Performer [31] achieves the goal of generating highresolution human videos (up to 512×256) by decomposing the latent space for separate handling of human appearance and motion and utilizing continuous pose embeddings for motion modeling. Such a method ensures the appearance is consistently maintained across frames while producing temporally coherent and flexible human motions from text descriptions.
3.2.3 Seamless Quality
In terms of viewability, the high-frame-rate video is much more appealing to viewers compared to the unsmooth, as it avoids common artifacts, such as temporal jittering and motion blurriness [32]. Depth-aware video frame interpolation method (DAIN) [32] is introduced to exploit depth information to detect occlusion and prioritize closer objects during interpolation. By involving a depth-aware flow projection layer that synthesizes intermediate flows by sampling closer objects preferentially over farther ones, the method can address the challenge of occlusion and motion in the video frames. Through leveraging cycle consistency loss along with motion linearity loss and edge-guided training, CyclicGen [33] achieves superior performance in generating high-quality interpolated frames, which is essential for high frame-rate video generation. Softmax splatting is introduced in Softmax-Splatting [34] to interpolate frames at any desired temporal position, effectively contributing to the generation of high frame-rate videos. Currently, FLAVR [35] addresses the challenge of generating high frame rate videos through a designed architecture that leverages 3D spatio-temporal convolutions for motion modeling. Besides, to alleviate the computational limitation, FLAVR directly learns motion properties from video data, which simplifies the training and deployment process.
3.3 Realistic Panorama
A key challenge in T2V generation is achieving realistic video output. Addressing this involves focusing on the integration of elements essential for realism. Breaking down a realistic panorama T2V generation, we identify the following key components that should be considered: 1. Dynamic Motion 2. Complex Scene 3. Multiple Objects 4. Rational Layout.
3.3.1 Dynamic Motion
In recent years, although there has been significant advancement in T2I generation, numerous researchers have begun exploring the extension of T2I models to T2V generation, such as LAMP [85] and AnimateDiff [86]. Motion represents one of the critical distinctions between videos and images, and it is a key focus in this evolving field of study [87]. LAMP focuses on learning motion patterns from a limited dataset. It employs a first-frame-conditioned pipeline that leverages a pre-trained T2I model to create the initial frame, enabling the video diffusion model to concentrate on learning the motion for subsequent frames. This process is enhanced with temporal-spatial motion learning layers that capture both temporal and spatial features, streamlining the motion generation in videos. AnimateDiff integrates a pre-trained motion module into personalized T2I models, enabling smooth, content-aligned animation production. This module is optimized using a novel strategy that learns motion priors from video data, focusing on dynamics rather than pixel details. A key innovation is MotionLoRA, a fine-tuning technique that adapts this module to new motion patterns, enhancing its versatility for varied camera movements. Motion consistency and coherence are also key challenges in motion generation. Unlike traditional models that generate keyframes and then fill in gaps, often leading to inconsistencies, Lumiere [29] uses a Space-Time U-Net architecture to generate the entire video in one pass. This method ensures global temporal consistency by incorporating spatial and temporal down- and up-sampling, significantly improving motion generation performance. Dysen-VDM [88] integrates a Dynamic Scene Manager (Dysen) that operates in three orchestrated steps: extracting key actions from text, converting these into a dynamic scene graph (DSG), and enriching the DSG with contextual scene details. This methodology enables the generation of temporally coherent and contextually enriched video scenes, aligning closely with the intended actions described in the input text. ART•V [89] focuses on an innovative approach that addresses the challenges of modeling complex long-range motions by sequentially generating video frames, each conditioned on its predecessors. This strategy ensures the production of continuous and simple motions, maintaining coherence between adjacent frames. DynamiCrafter [90] focuses on a dual-stream image injection mechanism that integrates both a text-aligned context representation and visual detail guidance. This approach ensures that the animated content remains visually coherent with the input image while being dynamically consistent with the textual description. Several works have concentrated on improving motion generation in T2V. PixelDance [67] focuses on enriching video dynamics by integrating image instructions for the video’s first and last frames alongside text instructions. This method allows the model to capture complex scene transitions and actions, enhancing the motion richness and temporal coherence in the generated videos. MoVideo [87] using depth and optical flow information derived from a key frame to guide the video generation process. Initially, an image is generated from the text, and then depth and optical flows are extracted from this image. MicroCinema [91] employs a two-stage process, initially generating a key image


10
from text, then using both the image and text to guide the video creation, focusing on capturing motion dynamics. ConditionVideo [92] separates video motion into background and foreground components, enhancing clarity and control over the generated video content. DreamVideo [93] decouples the video generation task into subject learning and motion learning. The motion learning aspect is specifically targeted to adapt the model to new motion patterns effectively. They introduce a motion adapter, which, when combined with appearance guidance, enables the model to concentrate solely on learning the motion without being influenced by the subject’s appearance. TF-T2V [94] enables the learning of intricate motion dynamics without relying on textual annotations, employing an image-conditioned model to capture diverse motion patterns effectively, which includes a temporal coherence loss that strengthens the continuity across frames. GPT4Motion [95] employs GPT-4 to generate Blender scripts, which are then used to drive Blender’s physics engine, simulating realistic physical scenes that correspond to the given textual descriptions. The integration of these scripts with Blender’s simulation capabilities ensures the production of videos that are not only visually coherent with the text prompts but also adhere to physical realism. However, challenges remain, particularly in human motion generation. Text2Performer [31] diverges from traditional discrete VQ-based models by generating continuous pose embeddings, enhancing motion realism and temporal coherence in the generated videos. MotionDiffuse [75] focuses on a probabilistic strategy that facilitates the creation of varied and lifelike motion sequences from text descriptions. Integrating a Denoising Diffusion Probabilistic Model (DDPM) with a cross-modality transformer architecture allows the system to produce intricate, ongoing motion sequences aligned with textual inputs, ensuring both high fidelity and precise controllability in the resulting animations.
3.3.2 Complex Scene
Generating videos with complex scenes is exceptionally challenging due to the intricate interplay of elements, requiring sophisticated understanding and replication of detailed environments, dynamic interactions, and variable lighting conditions with high fidelity. In the initial phases of research, authors in [40] proposed a GAN network that incorporates a novel spatio-temporal convolutional architecture. This architecture effectively captures the dynamics of both the foreground and background elements in the scene. By training the model on large datasets of unlabeled video, it learns to predict plausible future frames from static images, creating videos with realistic scene dynamics. This method allows the model to handle complex scenes by understanding and generating the temporal evolution of different components within a scene. Subsequently, as LLMs evolved, researchers delved into leveraging their capabilities to enhance generative models in producing complex scenes. VideoDirectorGPT [41] leverages LLM for video content planning, producing detailed scene descriptions, and entities with their layouts, and ensuring visual consistency across scenes. By employing a novel Layout2Vid generation technique to ensure spatial and temporal consistency across scenes it produces rich, narrativedriven video content. Similarly, FlowZero [42] enhances
alignment between spatio-temporal layouts and text prompts through a novel framework for zero-shot T2V synthesis. Integrating LLM and image diffusion models, FlowZero initially translates textual prompts into detailed Dynamic Scene Syntax (DSS), outlining scene descriptions, object layouts, and motion patterns. It then iteratively refines these layouts in line with the text prompts via self-refinement processes, thereby synthesizing temporally coherent videos with intricate motions and transformations. VideoDrafter [43] converts input prompts into comprehensive scripts by utilizing LLM, identifies common entities, and generates reference images for each entity. VideoDrafter then produces videos by considering the reference images, descriptive prompts, and camera movements through a diffusion process, ensuring visual consistency across scenes. SceneScape [24] emphasizes the generation of videos for more complex scenarios within 3D scene synthesis. Using pretrained text-to-image and depth prediction models ensures the generation of videos that maintain 3D consistency via a test-time optimization process. It employs a progressive strategy where a unified mesh representation of the scene is continuously built and updated with each frame, thereby guaranteeing geometric plausibility.
3.3.3 Multiple Objects
As for every frame of a video, generating multiple objects presents several challenges such as attribute mixing, object mixing, and object disappearance. Attribute mixing occurs when objects inadvertently and mistakenly adopt characteristics from other objects. Object mixing and disappearance involve the blending of distinct objects, which results in the creation of peculiar hybrids and inaccurate object counts. To solve these problems, Detector Guidance (DG) [36] integrates a latent object detection model to enhance the separation and clarity of different objects within generated images. Their approach manipulates cross-attention maps to refine object representation, demonstrating significant improvements in generating distinct objects without human intervention. The complexity of video synthesis necessitates capturing the dynamic spatio-temporal relationships between objects. MOVGAN [37], drawing inspiration from layout-to-image generation advancements, innovates by employing implicit neural representations alongside a self-inferred layout motion technique. This approach enables the generation of videos that not only depicts individual objects but also accurately represents their interactions and movements over time, enhancing the realism and depth of the synthesized video content. In the realm of T2V generation, addressing the visual features of multiple subjects within a single video frame is crucial due to the attribute binding problem. VideoDreamer [38] leverages Stable Diffusion with latent-code motion dynamics and temporal cross-frame attention, further customized through Disen-Mix Finetuning and optional Human-in-the-Loop Re-finetuning strategies. It successfully generates high-resolution videos by maintaining temporal consistency and subject identity without artifacts. UniVG [39] tackles multi-object generation challenges by enhancing its base model with Multi-condition CrossAttention for tasks requiring high freedom, enabling effective management of complex scenarios involving multiple objects


11
derived from text and image inputs. For tasks with lower freedom, it introduces Biased Gaussian Noise to more effectively preserve content, aiding in tasks such as image animation and super-resolution. These innovations allow UniVG to produce semantically aligned, high-quality videos across a variety of generative tasks, effectively addressing the intricacies of multi-object scenarios.
3.3.4 Rational Layout
Ensuring high-quality video output in T2V conversion hinges on the ability to generate rational layouts based on textual instructions. Craft [44] is designed for generating videos from textual descriptions, learning from video-caption data to predict the temporal layout of entities within a scene, retrieve spatio-temporal segments from a video database, and fuse them to generate scene videos. It incorporates the Layout Composer, a model that generates plausible scene layouts by understanding spatial relationships among entities. Utilizing a sequential approach that combines text embeddings and scene context, Craft accurately predicts the locations and scales of characters and objects, facilitating the creation of visually coherent and contextually accurate videos. FlowZero [42] generate layouts using LLMs to transform textual prompts into structured syntaxes for guiding the generation of temporally-coherent videos. This process includes frame-by-frame scene descriptions, foreground object layouts, and background motion patterns. Specifically, for foreground layouts, LLMs generate a sequence of frame-specific layouts that outline the spatial arrangement of foreground entities in each frame. These layouts consist of bounding boxes defining the position and size of the prompt-referenced objects. This structured approach ensures that foreground objects adhere to the visual and spatio-temporal cues provided in the text, enhancing the video’s coherence and fidelity. Nonetheless, authors in [45] highlight that existing models face challenges with sophisticated spatiotemporal prompts, frequently resulting in limited or erroneous movements, for instance, their inability to accurately represent objects transitioning from left to right. Addressing these shortcomings, they propose LLM-grounded Video Diffusion (LVD), a novel method that enhances neural video generation from text prompts by first generating dynamic scene layouts (DSLs) with a Large Language Model (LLM), then using these DSLs to guide a diffusion model for video generation. This approach addresses the limitations of current models in generating videos with complex spatiotemporal dynamics and achieves significantly better performance in generating videos that closely align with the desired attributes and motion patterns.
3.4 Datasets and Metrics
3.4.1 Datasets
We comprehensively review the T2V datasets and mainly categorize them into six streams based on the collected domain: Face, Open, Movie, Action, Instruct, and Cooking. Table 1 summarizes datasets in 12 dimensions, and the details of each dataset are described as follows: CV-Text [96] is a high-quality dataset of facial text-video pairs, comprising 70,000 in-the-wild face video clips with a
resolution of at least 512×512. Each clip is paired with 20 generated descriptions whose average length is around 67.2. MSR-VTT [97] provides 10K web video clips with 40 hours and 200K clip-sentence pairs in total. Each clip is accompanied by approximately 20 natural sentences for description. The number of videos is around 7.2K, and the average of each clip and sentence is 15.0s and 9.3 words respectively. DideMo [98] consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. YT-Tem-180M [99] was collected from 6 million public YouTube videos, 180 million clips, and annotated by ASR. WebVid2M [100] consists of 2.5M video-text pairs. The average length of each video and sentence is 18.0s and 12.0 words respectively. The raw descriptions of each video are collected from the Alt-text HTML attribute associated with web images. HD-VILA-100M [101] is a large-scale text-video dataset collected from YouTube consisting of 100M high-resolution (720P) video clips and sentence pairs from 3.3 million videos with 371.5K hours and 15 popular categories in total. InternVid [102] is a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InterVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of a total of 4.1B words. HD-VG-130M [103] comprises 130 million text-video pairs from the open domain with a high resolution (1376×768). Most descriptions in the dataset are around 10 words. Youku-mPLUG [104] is the first released Chinese videolanguage pretraining dataset collected from Youku [119]. It contains 10 million Chinese video-text pairs filtered from 400 million raw videos across a wide range of 45 diverse categories. The average time of each video is around 54.2s. VAST-27M [105] consists of a total of 27M video clips covering diverse categories, and each clip paired with 11 captions (including 5 vision, 5 audio, and 1 omni-modality caption). The average lengths of vision, audio, and omnimodality captions are 12.5, 7.2, and 32.4 respectively. Panda-70M [106] is a high-quality video-text dataset originally curated from HD-VILA-100M. It consists of a total duration of 166.8Khr of 70.8M videos with paired highquality text captions. The time of each video is 8.5s and the length of each sentence is 13.2 words on average. LSMDC [107] consists of around 118K video clips aligned to sentences sourced from 200 movies. The total time of videos is around 158 hours, and the average of each clip and sentence is 4.8s and 7.0 words respectively. MAD [108] is derived from movies, and contains more than 384K sentences anchored on more than 1.2K hours of 650 videos. The length of each sentence is 13.2 words on average. UCF-101 [109] is a dataset of human actions, collected samples from YouTube [120] that encompass 101 action categories, including human sports, musical instrument playing, and interactive actions. It consists of over 13K clips and 27 hours of video data. The resolution and frame rate of UCF-101 is 320×240 and 25 Frame-Per-Second (FPS).


12
TABLE 1: Comparison of existing text-to-video datasets.
Dataset Domain Annotated #Clips #Sent LenC(s) LenS #Videos Resolution FPS Dur(h) Year Source CV-Text [96] Face Generated 70K 1400K - 67.2 - 480P - - 2023 Online MSR-VTT [97] Open Manual 10K 200K 15.0s 9.3 7.2K 240P 30 40 2016 YouTube DideMo [98] Open Manual 27K 41K 6.9s 8.0 10.5K - 87 2017 Flickr Y-T-180M [99] Open ASR 180M - - - 6M - - - 2021 YouTube WVid2M [100] Open Alt-text 2.5M 2.5M 18.0 12.0 2.5M 360P - 13K 2021 Web H-100M [101] Open ASR 103M - 13.4 32.5 3.3M 720P - 371.5K 2022 YouTube InternVid [102] Open Generated 234M - 11.7 17.6 7.1M *720P - 760.3K 2023 YouTube H-130M [103] Open Generated 130M 130M - 10.0 - 720P - - 2023 YouTube Y-mP [104] Open Manual 10M 10M 54.2 - - - - 150K 2023 Youku V-27M [105] Open Generated 27M 135M 12.5 - - - - - 2024 YouTube P-70M [106] Open Generated - 70.8M 8.5 13.2 70.8M 720P - 166.8K 2024 YouTube LSMDC [107] Movie Manual 118K 118K 4.8s 7.0 200 1080P - 158 2017 Movie MAD [108] Movie Manual - 384K - 12.7 650 - - 1.2K 2022 Movie UCF-101 [109] Action Manual 13K - 7.2s - - 240P 25 27 2012 YouTube ANet-200 [110] Action Manual 100K - - 13.5 2K *720P 30 849 2015 YouTube Charades [111] Action Manual 10K 16K - - 10K - - 82 2016 Home Kinetics [112] Action Manual 306K - 10.0s - 306K - - - 2017 YouTube ActNet [113] Action Manual 100K 100K 36.0s 13.5 20K - - 849 2017 YouTube C-Ego [114] Action Manual - - - - 8K 240P - 69 2018 Home SS-V2 [115] Action Manual - - - - 220.1K - 12 - 2018 Daily How2 [116] Instruct Manual 80K 80K 90.0 20.0 13.1K - - 2000 2018 YouTube HT100M [61] Instruct ASR 136M 136M 3.6 4.0 1.2M 240P - 134.5K 2019 YouTube YCook2 [117] Cooking Manual 14K 14K 19.6 8.8 2K - - 176 2018 YouTube E-Kit [118] Cooking Manual 40K 40K - - 432 *1080P 60 55 2018 Home
ActNet-200 [110] provides a total of 849 hours of video, where 68.8 hours of video contain 203 human-centric activities. Around 50% of the videos are in HD resolution (1280×720), while the majority have a frame rate of 30 FPS. Charades [111] is collected from household activities of 267 people, which consists of around 10K videos covering 157 daily actions with an average length of 30.1s. Kinetics [112] contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. SS-V2 [115] is a large collection of labeled video clips that show humans performing pre-defined basic 174 actions with everyday objects. The dataset was created by a large number of crowd workers, containing around 220.1K videos. ActivityNet [113] contains 20K videos amounting to 849 video hours with 100k total descriptions, each with its unique start and end time. For each sentence, the length is 13.5 words and for descriptions, is 36s on average. Charades-Ego [114] consists of 8K videos (4K pairs of third and first-person videos) total. In these videos, with over 364 pairs involving more than one person in the video. The average time of each video is around 31.2s. How2 [116] covers a wide variety of instructional topics. It consists of 80K clips with 2K hours in total, and the length of each video is around 90s on average. HowTo100M [61] consists of 136 million video clips collected from 1.22M narrated instructional web videos depicting humans performing and describing over 23K different visual tasks. Each clip is paired with a text annotation in the form of automatic speech recognition (ASR). YouCook2 [117] contains 2000 videos that are nearly equally distributed over 89 recipes with a total length of 176 hours. The recipes are from four major cuisines (i.e., African, American, Asian and European) and have a large variety of cooking styles, methods, ingredients, and cookware. Epic-Kitchens [118] is an egocentric video recorded by 32 participants in the kitchen environments. It contains 55 hours
of video and a total of around 40.0K action segments. Major videos were recorded in a full HD resolution of 1920×1080.
3.4.2 Metrics
The metrics used to evaluate the quality of video generated from T2V models can be mainly categorized into two aspects: quantitative and qualitative. In the former, the evaluators are typically presented with two or more generated videos to compare against videos synthesized by other competitive models. Observers generally engage in voting-based assessments regarding the realism, natural coherence, and text alignment of the videos. Although human evaluation has been utilized in various works [68], [73], [103], [121], the time-consuming and labor-intensive aspects restrict its wide application. Besides, qualitative measurements fail to give a comprehensive evaluation of the model at times [46]. Therefore, we restrict our review to the quantitative metrics. These reviewed metrics are categorized into image-level and video-level. Generally, the former is utilized to evaluate generated videos frame-wise, and the latter focuses on the comprehensive evaluation of the video quality:
Peak Signal-to-Noise Ration (PSNR) [122]. Commonly known as PSNR, was designed to quantify reconstruction quality for the frame of generated videos. Given an original image Io with M × N pixels and the generated image Ig, the PSNR can be calculated as:
PSNR = 10 · log10( MAX2
Io
MSE ), (15)
where MSE = 1
MN
P
i
P
j[Io(i, j) − Ig(i, j)]2. MAXIo is the possible maximal value of the original image. Io(i, j) and Ig(i, j) denote the pixel value on the position (i, j).
Structural Similarity Index (SSIM) [122]. SSIM is usually utilized to measure the similarity between two images, and also as a method to quantify the quality from a perceptive view. Specifically, the definition of SSIM is:
SSIM = [L(Io, Ig)]α · [C(Io, Ig)]β · [S(Io, Ig)]γ, (16)


13
where L(Io, Ig) is utilized to compare the luminance, C(Io, Ig) reflects the contrast difference, S(Io, Ig) measures the structure similarity between the original image Io and the generated image Ig. Practically, the parameters are set as α = 1, β = 1, and γ = 1. Then, the SSIM can be simply represented as:
SSIM = (2I ̄oI ̄g + δ1)(2ΣIoIg + δ2)
(I ̄o2 + I ̄g2 + δ1)(Σ2
Io + Σ2
Ig + δ2) , (17)
where I ̄ denotes the average value of the image I. Σ2
Io and
Σ2
Ig are the variance of Io and Ig respectively. We use ΣIoIg to denote the covariance. Lastly, L is the maximum intensity value in δi = (KiL)2 and Ki ≪ 1.
Inception Score (IS) [123]. IS is developed to measure both image-generated quality and diversity. Specifically, the pre-trained Inception network [124] is applied to get the conditional label distribution p(y|Ig) of each generated image. IS can be formulated as:
IS = exp(EIg [KL(p(y|Ig)||p(y)]), (18)
where KL denotes the KL-divergence [125].
Fre ́chet Inception Distance (FID) [126]. Compared with IS, FID provides a more comprehensive and accurate evaluation as it directly considers the similarity between the generated images and original images.
FID = ||I ̄o − I ̄g||2 + Tr(ΣIg + ΣIr − 2(ΣIg ΣIr )1/2). (19)
Here, Tr is the trace of a matrix. CLIP Score [127]. CLIP Score has been widely used to measure the alignment between images and sentences. Based on the pre-trained CLIP embedding, it can be calculated as:
CLIPscore = E[max(cos(EI , ES), 0)], (20)
where EI and ES denote the embedded features from the image and the sentence respectively, cos(EI , ES) is the cosine similarity.
Video Inception Score (Video IS) [128]. Typically, Video IS calculates the IS of the generated videos based on the features extracted from C3D [129].
Fr ́echet Video Distance (FVD). Based on the extracted features from pre-trained Inflated-3D Convnets (I3D) [130] Fre ́chet Video Distance (FVD) [131] scores can be computed by combining the means and covariance matrices:
FVD = ||V ̄ − V ̄∗||2 + Tr(ΣV + ΣV∗ − 2(ΣV ΣV∗ )1/2). (21)
Here, we use the V ̄ and V ̄∗ to denote the exception of the realistic videos V and the generated videos V∗ respectively. Kernel Video Distance (KVD) [132]. KVD adopts the kernel method to evaluate the performance of generated model. Given the kernel function Φ, and sets {F , F ∗} that are features extracted from the realistic videos and generated videos via the pre-trained I3D and combining Maximum Mean Discrepancy (MMD), KVD can be formulated as:
Ef,f′ [Φ(f, f ′)]+Ef∗,f∗′ [Φ(f ∗, f ∗′)]−2Ef,f∗ [Φ(f, f ∗)]. (22)
Concretely, f and f ∗ are sampled from F and F ∗ respectively.
Frame Consis Score (FCS) [68]. FCS calculates the cosine similarity of the CLIP image embeddings for all pairs of video frames to measure the consistency of edited videos.
4 CHALLENGES AND OPEN PROBLEMS
At beginning of this section, we will generally go through all existing common problems still not being addressed, even for SOTA work Sora.
4.1 Unsolved Problems from Sora
In Fig 5, there are five weaknesses based on the video demonstrated by OpenAI on their website [84].
Unrealistic and incoherent motion: In Fig 5(a), we observe a striking example of unrealistic motion: a person appears to be running backwards on a treadmill but paradoxically, the running action is forward, a physically implausible scenario. Typically, treadmills are designed for forwardfacing use, as depicted on the left side of the figure. This discrepancy highlights a prevalent issue in T2V synthesis, where current LLMs excel at understanding and explaining the physical laws of motion but struggle to accurately render these laws in visual or video formats. Additionally, the video exhibits incoherent motions; for instance, a person’s running pattern should display a consistent sequence of leg movements. Instead, there’s an abrupt shift in the positioning of the legs, disrupting the natural flow of motion. Such frame-to-frame inconsistencies underscore another significant challenge in T2V conversion: maintaining motion coherence throughout the video sequence.
Intermittent object appearances and disappearances: In Fig 5(b), we see a multi-object scene characterized by intermittent appearances and disappearances of objects, which detracts from the generation accuracy of the video from text. Despite the prompt specifying “five wolf pups” initially, only three are visible. Subsequently, an anomaly occurs where one wolf inexplicably exhibits two pairs of ears. Progressing through the frames, a new wolf unexpectedly appears in front of the middle wolf, followed by another appearing in front of the rightmost wolf. Ultimately, the first wolf, which is displayed from the middle of the screen, disappears from the scene. Such spontaneous manifestations of animals or people, particularly in scenes with numerous entities, pose a significant challenge. When a video necessitates a precise count of objects or characters, the occurrence of such unplanned elements can disrupt the narrative, leading to outputs that are not only inaccurate but also inconsistent with the specified prompt. The introduction of unexpected objects, especially if they contradict the intended storyline or content, risks misrepresenting the intended message, undermining the video’s integrity and coherence.
Unrealistic phenomena: Fig 5(c) showcases two sequences of snapshot frames, illustrating the issues of inaccurate physical modeling and unnatural object morphing. Initially, the first sequence of four frames depicts a basketball passing through the hoop and igniting into flames, as specified by the prompt. However, contrary to the expected explosive interaction, the basketball passes through the hoop unscathed. Subsequently, the next set of four frames reveals another basketball traversing the hoop, but this time it directly passes through the rim which unexpectedly morphs during the sequence. Additionally, in this snapshot, the basketball fails to explode as dictated by the prompt.


14
(a) Prompt: Step-printing scene of a person running, cinematic film shot in 35mm.
leading with the right foot leading with the right foot leading with the left foot leading with the left foot
extraneous object anomalous object 3 -> 4
3 -> 4 4 -> 5 5 -> 4
morphing morphing
inaccurate physical modeling
inaccurate physical modeling
inconsistency deformation deformation deformation deformation
(b) Prompt: Five gray wolf pups frolicking and chasing each other around a remote gravel road, surrounded by grass. The pups run and leap, chasing each other, and nipping at each other, playing.
(c) Prompt: Basketball through hoop then explodes.
(d) Prompt: Archeologists discover a generic plastic chair in the desert, excavating and dusting it with great care.
(e) Prompt: A grandmother with neatly combed grey hair stands behind a colorful birthday cake with numerous candles at a wood dining room table, expression is one of pure joy and happiness, with a happy glow in her eye. She leans forward and blows out the candles with a gentle puff, the cake has pink frosting and sprinkles and the candles cease to flicker, the grandmother wears a light blue blouse adorned with floral patterns, several happy friends and family sitting at the table can be seen celebrating, out of focus. The scene is beautifully captured, cinematic, showing a 3/4 view of the grandmother and the dining room. Warm color tones and soft lighting enhance the mood.
incoherent
running backwards on a treadmill
Fig. 5: Screenshots of Sora generated video with its prompts from [84]
This scenario exemplifies the challenges of inaccurate physical modeling and unnatural object morphing. In the depicted case, Sora seems to induce some unnatural changes to objects, which can significantly detract from the realism of the videos.
Limited understanding of objects and characteristics: Fig 5(d) illustrates the limitations of Sora in accurately comprehending objects and their inherent characteristics, with a focus on texture. The sequence reveals a plastic chair that initially appears stable but then undergoes bending and shows inconsistent shapes between the initial frames, alongside an unnatural floating without any visible support. Subsequently, the chair is depicted undergoing continuous, extreme bending. This represents a failure to correctly model the chair as a rigid, stable object, leading to implausible physical interactions. Such inaccuracies can result in videos that appear surreal and are therefore not suitable for practical use. Although minor flaws might be corrected with additional design tools, significant errors, particularly with prominent objects displaying unrealistic behavior across multiple frames, could render the video ineffective. Consequently, achieving the desired outcome might require numerous iterations to correct these pronounced inconsistencies
Incorrect interactions between multi-objects: Fig 5(d) illustrates the model’s inability to accurately simulate complex interactions involving multiple objects. The sequence intended to show a ‘grandmother’ character blowing out candles. Ideally, the candle flames should react to the airflow, either by flickering or being extinguished. However, the video fails to depict any interaction with the external environment, displaying the flames as unnaturally static throughout the scene. This highlights Sora’s challenges in rendering realistic interactions, particularly in scenes with multiple active elements or complex dynamics. The difficulty is amplified
in scenarios involving numerous moving subjects, intricate backgrounds, or interactions involving various textures. Such shortcomings can lead to unrealistic and sometimes unintentionally humorous outcomes, undermining the effectiveness of the video, especially in contexts requiring a high degree of realism or accurate representation of physical interactions.
4.2 Data access privacy
Inspired by advancements in large language models (LLMs), Sora is trained using internet-scale datasets [5]. However, the vast expanse of the Internet’s public data constitutes only a fraction of the total available information; the bulk is private data from individuals, companies, and institutions. Compared to public datasets, private ones are richer in diversity and contain fewer duplicates. However, they also harbor a significant amount of sensitive personal information, particularly in image and video formats, which tends to have more personalized content than plain text. This is a critical distinction, as the latter is predominantly used for training LLMs. Therefore, when the public data resources are fully utilized and there is an aim to further enhance model capabilities, especially in terms of generalizability, it becomes crucial to devise strategies for leveraging private, non-sensitive data in a manner that rigorously protects privacy. Federated Learning (FL), as introduced in [133], offers a promising solution that enables thousands of clients to collaboratively contribute to a global model using their own data, without the need to transfer any raw data at any stage. Recent studies have validated the efficacy of FL in fine-tuning LLMs [134] and enhancing diffusion models [135]. While FL can effectively address the challenges of distributed private data accessibility, it still faces significant problems, including network bottlenecks, heterogeneous data, the intermittent availability of devices and so on.


15
4.3 Simultaneous Multi-shot Video Generation
Multi-shot video generation stands as a significant challenge within the T2V sector, a realm where video generation itself remains largely uncharted. This scenario persisted until Sora showcased its proficiency in this domain. With its advanced linguistic comprehension, Sora is capable of generating videos that incorporate multiple shots, consistently maintaining the characters and visual style throughout the sequence. Such capability signifies a significant stride in video generation, offering a coherent and continuous visual narrative. Despite these advancements, Sora has yet to master the creation of simultaneous multi-shot videos featuring identical characters and consistent visual styles. This capability is crucial for specific fields, such as robotics, where learning from demonstration is essential, and in autonomous vehicle simulations, where consistent visual representation plays a pivotal role in the system’s effectiveness and reliability. The development of this feature would mark a substantial breakthrough, broadening the applicability of T2V technologies in critical, real-world applications.
4.4 Multi-Agent Co-creation
Agents powered by LLMs can perform tasks independently based on human knowledge, while multi-agent systems bring together individual LLM agents, utilizing their collective capabilities and specialized skills through collaboration [136], [137], [138]. In multi-agent systems, collaboration and coordination are pivotal, enabling a collective of agents to undertake tasks that surpass the capabilities of any single entity, and agents are endowed with unique abilities and assume specific roles, working collaboratively to achieve shared goals [139]. The efficacy of multi-agent systems in reflecting and augmenting real-world tasks requiring collaborative problem-solving or deliberation has been substantiated across various domains. Notably, this includes areas like software development [136], [137] and negotiation games [140]. Despite its potential, the application of multi-agent systems in the T2V generation field remains largely unexplored due to unique challenges. Take filmmaking as an instance: agents assume distinct roles, such as directors, screenwriters, and actors. Each actor must generate their segment in accordance with directives from the director, subsequently integrating these segments into a cohesive video. The primary challenge lies in ensuring that each actor agent not only preserves frame-to-frame coherence within its own output but also aligns with the collective vision to maintain global video style consistency, logical screen layout, and uniformity. This is particularly difficult as outputs from generative models can vary significantly, even when prompted similarly.
5 FUTURE DIRECTIONS
5.1 Robot Learning from Visual Assistance
Traditional methods of programming robots for new tasks demand extensive coding expertise and a significant investment of time [141]. These approaches require users to meticulously define each step or movement needed
for task execution. Although motion planning strategies reduce the necessity to specify each minor action, they still necessitate the identification of higher-level actions, such as setting goal locations and sequences of via points. To address these challenges, recent research has pivoted towards Learning from Demonstration (LfD), which is a paradigm in which robots learn new skills by observing and imitating the actions of an expert [141]. However, just as traditional robotics research faces challenges in dataset collection, acquiring demonstration videos for LfD also presents critical difficulties. Despite recent advancements in tools and methodologies designed to simplify data gathering, such as UMI [142], the process of collecting relevant and comprehensive data continues to be a significant challenge. The more recent generative model research, such as Large Language Models (LLMs) and Vision Language Models (VLMs), have substantially mitigated this obstacle. These innovations enable robots to operate with pre-trained LLMs and LVMs in a zero-shot manner, such as VoxPoser [143], directly applying human knowledge to finish new robotics tasks without prior explicit training. However, prior to the announcement of Sora, applying T2V generative models directly to LfD for robotics was challenging. Most existing models faced difficulties in accurately simulating the physics of complex scenes and often lacked an understanding of specific cause-and-effect instances [5]. This issue is vital for the effectiveness of robotic learning; discrepancies between the data and real-world conditions can profoundly affect the quality of robot training, compromising the robot’s capacity for precise autonomy. On the other hand, 3D Reconstruction technology, such as Neural Radiance Fields (NeRF) [144] and 3D Gaussian Splatting (3DGS) [145], is getting a lot of attention including in robotics research. DFFs [146], capture scene images, extracting dense features via a 2D model and integrating them with a NeRF, mapping spatial and viewing details to create complex 3D models and interactions efficiently. DFFs show that their integration with 3D reconstructed information from NeRF enables robots to better understand and interact with their environment, thus demonstrating how 3D reconstructed information can assist robots in accomplishing tasks more effectively. Apart from this, a notable feature of Sora is its ability to produce multiple shots of the same character within a single sample, ensuring consistent appearance throughout the video [5]. This capability can be leveraged by integrating 3D reconstruction methods with the multi-shot videos generated by Sora, thereby enhancing robot learning through demonstration by accurately capturing scenes.
5.2 Infinity 3D Dynamic Scene Reconstruction and Generation
3D scene reconstruction has garnered significant attention in recent years. The work in [147] stands out as one of the pioneering efforts in 3D sense reconstruction from video, achieving remarkable results by integrating various technologies such as GPS, inertial measurements, camera pose estimation, and advanced algorithms for stereo vision, depth map fusion, and model generation. However, the advent of Neural Radiance Fields (NeRF) [144] marked a substantial simplification in the field. NeRF enables users to reconstruct


16
objects directly from a video with unprecedented detail. Despite this advancement, reconstructing high-quality scenes still demands significant effort, particularly in acquiring numerous viewpoints or videos, as highlighted in [148]. This challenge becomes even more pronounced when the object is large, or the environment makes it difficult to capture multi-view videos. Several studies have demonstrated the effectiveness of 3D reconstruction from video streams, with notable contributions including DyNeRF [148] and NeuralRecon [149]. Particularly, the forefront of coherent 3D reconstruction research [149], [150] is characterized by its emphasis on the seamless generation of new scene blocks, leveraging the spatial and semantic continuity of pre-existing scene structures. Notably, Sora [5] stands out by offering the capability to render multiple perspectives of a singular character, integrating physical world simulations within a unified video framework. These advancements herald a new era of infinite 3D scene reconstruction and generation, promising substantial implications across various domains. In the realm of gaming, for instance, this technology allows for the realtime generation of 3D environments and the instantiation of objects adhering to real-world physics, potentially obviating the need for conventional game physics engines.
5.3 Augmented Digital Twins
Digital twins (DT) are virtual replicas of physical objects, systems, or processes, designed to simulate the real-world entity in a digital platform [151]. They are used extensively in various industries for simulation, analysis, and control purposes. The concept is that the DT receives data from its physical counterpart (often in real-time, through sensors and other data-collection methods) and can predict how the physical twin will behave under various conditions or react to changes in its environment. The key aspect of DT is their ability to mirror the physical world accurately, allowing for responses to external signals or changes just as their real-world counterparts would. This capability enables optimization of operations, predictive maintenance, and improved decision-making, as the digital twin can simulate outcomes based on different scenarios without the risk of impacting the physical object. Sora’s world simulation capabilities have the potential to enhance the current DT systems significantly. One of the major challenges in DT is ensuring real-time data accuracy, as unstable network connections can lead to the loss of crucial data segments. Such losses are critical, potentially undermining the system’s effectiveness. Current data completion methods often lack an in-depth understanding of the objects’ physical properties, relying predominantly on data-driven approaches. By leveraging Sora’s proficiency in understanding physical principles, it might be possible to generate data that is physically coherent and more aligned with the underlying real-world phenomena. Another crucial element of DT is the accuracy with which the visual system responds, effectively simulating and mirroring the real world. Currently, the approach involves users triggering events through programming, followed by the application of machine learning algorithms to predict and subsequently visualize the outcomes. This process is
intricate and often lacks a comprehensive understanding of the object’s physical characteristics. Implementing Sora could potentially streamline this, enabling unified user interaction with the system. Users could interact directly with the visualization interface created by Sora, which would make real-time predictions and provide accurate and real-world like visual feedback while considering the physical properties of the objects involved.
5.4 Establish Normative Frameworks for AI applications
With the rapid advancement of large generative models such as DALL·E, Midjourney, and Sora, the capabilities of these models have seen remarkable enhancement. Although these advancements can improve work efficiency and stimulate personal creativity, concerns about misuse of these technologies also arise, including fake news generation [152], privacy breaches [153], and ethical dilemmas [154]. At present, it is becoming necessary and urgent to establish normative frameworks for AI applications. Such frameworks should answer the following questions legitimately:
How to explain the decisions from AI. Current AI techniques are mostly regarded as black-box. However, the decision process of reliable AI should be explainable, which is crucial for supervision adjustment, and trust enhancement.
How to protect the privacy of users. The protection of personal information already is a social concern. With the development of AI, which is a data-hungry domain, more detailed and strict regulations should be established.
How to ensure fairness and avoid discrimination. AI systems should serve all users equitably, and should not exacerbate existing social inequalities. Achieving this necessitates the integration of fairness principles, along with the proactive avoidance of bias and discrimination, during the algorithm design and data collection phases. Overall, normative frameworks will encompass both social and technical dimensions to ensure that AI applications are developed in ways that benefit society as a whole.
6 CONCLUSION
Based on the decomposition from Sora, this survey provides a comprehensive review of current T2V works. Concretely, we organized the literature from the perspective of the evolution of generative models, encompassing GAN/VAE, autoregressive-based, and diffusion-based frameworks. Furthermore, we delved into and reviewed the literature based on three critical qualities that excellent videos should possess: extended duration, superior resolution, and seamless quality. Moreover, as Sora is declared to be a real-world simulator, we present a realistic panorama that includes dynamic motion, complex scenes, multiple objects, and rational layouts. Additionally, the commonly utilized datasets and metrics in video generation are categorized according to their source and domain of application. Finally, we identify some remaining challenges and problems in T2V, and also propose potential directions for future development.


17
REFERENCES
[1] “Introducing chatgpt,” 2024. [2] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al., “Improving image generation with better captions,” Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, vol. 2, no. 3, p. 8, 2023. [3] “Midjourney,” 2024. [4] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684–10695, 2022. [5] T. Brooks, B. Peebles, C. Homes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. W. Y. Ng, R. Wang, and A. Ramesh, “Video generation models as world simulators,” 2024. [6] W. Peebles and S. Xie, “Scalable diffusion models with transformers,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195–4205, 2023. [7] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234–241, Springer, 2015. [8] S. Chen, M. Xu, J. Ren, Y. Cong, S. He, Y. Xie, A. Sinha, P. Luo, T. Xiang, and J.-M. Perez-Rua, “Gentron: Delving deep into diffusion transformers for image and video generation,” arXiv preprint arXiv:2312.04557, 2023.
[9] A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, L. Fei-Fei, I. Essa, L. Jiang, and J. Lezama, “Photorealistic video generation with diffusion models,” arXiv preprint arXiv:2312.06662, 2023.
[10] X. Ma, Y. Wang, G. Jia, X. Chen, Z. Liu, Y.-F. Li, C. Chen, and Y. Qiao, “Latte: Latent diffusion transformer for video generation,” arXiv preprint arXiv:2401.03048, 2024.
[11] X. Chen, C. Xu, X. Yang, and D. Tao, “Long-term video prediction via criticization and retrospection,” IEEE Transactions on Image Processing, vol. 29, pp. 7090–7103, 2020. [12] S. Ge, T. Hayes, H. Yang, X. Yin, G. Pang, D. Jacobs, J.-B. Huang, and D. Parikh, “Long video generation with time-agnostic vqgan and time-sensitive transformer,” in European Conference on Computer Vision, pp. 102–118, Springer, 2022. [13] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan, “Phenaki: Variable length video generation from open domain textual description,” arXiv preprint arXiv:2210.02399, 2022.
[14] Y. He, T. Yang, Y. Zhang, Y. Shan, and Q. Chen, “Latent video diffusion models for high-fidelity video generation with arbitrary lengths,” arXiv preprint arXiv:2211.13221, 2022.
[15] Y. Wang, L. Jiang, and C. C. Loy, “Styleinv: A temporal style modulated inversion network for unconditional video generation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22851–22861, 2023. [16] S. Zhuang, K. Li, X. Chen, Y. Wang, Z. Liu, Y. Qiao, and Y. Wang, “Vlogger: Make your dream a vlog,” arXiv preprint arXiv:2401.09414, 2024.
[17] D. J. Zhang, D. Li, H. Le, M. Z. Shou, C. Xiong, and D. Sahoo, “Moonshot: Towards controllable video generation and editing with multimodal conditions,” arXiv preprint arXiv:2401.01827, 2024. [18] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836–3847, 2023. [19] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein, “Implicit neural representations with periodic activation functions,” Advances in neural information processing systems, vol. 33, pp. 74627473, 2020. [20] S. Yu, J. Tack, S. Mo, H. Kim, J. Kim, J.-W. Ha, and J. Shin, “Generating videos with dynamics-aware implicit generative adversarial networks,” arXiv preprint arXiv:2202.10571, 2022.
[21] I. Skorokhodov, S. Tulyakov, and M. Elhoseiny, “Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3626–3636, 2022.
[22] F.-Y. Wang, W. Chen, G. Song, H.-J. Ye, Y. Liu, and H. Li, “Genl-video: Multi-text to long video generation via temporal codenoising,” arXiv preprint arXiv:2305.18264, 2023.
[23] X. Chen, Y. Wang, L. Zhang, S. Zhuang, X. Ma, J. Yu, Y. Wang, D. Lin, Y. Qiao, and Z. Liu, “Seine: Short-to-long video diffusion model for generative transition and prediction,” in The Twelfth International Conference on Learning Representations, 2023.
[24] R. Fridman, A. Abecasis, Y. Kasten, and T. Dekel, “Scenescape: Text-driven consistent scene generation,” Advances in Neural Information Processing Systems, vol. 36, 2024.
[25] S. Yin, C. Wu, H. Yang, J. Wang, X. Wang, M. Ni, Z. Yang, L. Li, S. Liu, F. Yang, et al., “Nuwa-xl: Diffusion over diffusion for extremely long video generation,” arXiv preprint arXiv:2303.12346, 2023. [26] V. Voleti, A. Jolicoeur-Martineau, and C. Pal, “Mcvd-masked conditional video diffusion for prediction, generation, and interpolation,” Advances in Neural Information Processing Systems, vol. 35, pp. 23371–23385, 2022. [27] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, “Align your latents: High-resolution video synthesis with latent diffusion models,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22563–22575, 2023. [28] D. J. Zhang, J. Z. Wu, J.-W. Liu, R. Zhao, L. Ran, Y. Gu, D. Gao, and M. Z. Shou, “Show-1: Marrying pixel and latent diffusion models for text-to-video generation,” arXiv preprint arXiv:2309.15818, 2023. [29] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, Y. Li, T. Michaeli, et al., “Lumiere: A spacetime diffusion model for video generation,” arXiv preprint arXiv:2401.12945, 2024.
[30] Y. Tian, J. Ren, M. Chai, K. Olszewski, X. Peng, D. N. Metaxas, and S. Tulyakov, “A good image generator is what you need for high-resolution video synthesis,” arXiv preprint arXiv:2104.15069, 2021. [31] Y. Jiang, S. Yang, T. L. Koh, W. Wu, C. C. Loy, and Z. Liu, “Text2performer: Text-driven human video generation,” arXiv preprint arXiv:2304.08483, 2023.
[32] W. Bao, W.-S. Lai, C. Ma, X. Zhang, Z. Gao, and M.-H. Yang, “Depth-aware video frame interpolation,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3703–3712, 2019. [33] Y.-L. Liu, Y.-T. Liao, Y.-Y. Lin, and Y.-Y. Chuang, “Deep video frame interpolation using cyclic frame generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 8794–8802, 2019. [34] S. Niklaus and F. Liu, “Softmax splatting for video frame interpolation,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5437–5446, 2020.
[35] T. Kalluri, D. Pathak, M. Chandraker, and D. Tran, “Flavr: Flowagnostic video representations for fast frame interpolation,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2071–2082, 2023. [36] L. Liu, Z. Zhang, Y. Ren, R. Huang, X. Yin, and Z. Zhao, “Detector guidance for multi-object text-to-image generation,” arXiv preprint arXiv:2306.02236, 2023.
[37] Y. Wu, Z. Liu, H. Wu, and L. Lin, “Multi-object video generation from single frame layouts,” arXiv preprint arXiv:2305.03983, 2023. [38] H. Chen, X. Wang, G. Zeng, Y. Zhang, Y. Zhou, F. Han, and W. Zhu, “Videodreamer: Customized multi-subject text-to-video generation with disen-mix finetuning,” arXiv preprint arXiv:2311.00990, 2023. [39] L. Ruan, L. Tian, C. Huang, X. Zhang, and X. Xiao, “Univg: Towards unified-modal video generation,” arXiv preprint arXiv:2401.09084, 2024.
[40] C. Vondrick, H. Pirsiavash, and A. Torralba, “Generating videos with scene dynamics,” Advances in neural information processing systems, vol. 29, 2016. [41] H. Lin, A. Zala, J. Cho, and M. Bansal, “Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning,” arXiv preprint arXiv:2309.15091, 2023.
[42] Y. Lu, L. Zhu, H. Fan, and Y. Yang, “Flowzero: Zero-shot textto-video synthesis with llm-driven dynamic scene syntax,” arXiv preprint arXiv:2311.15813, 2023.
[43] F. Long, Z. Qiu, T. Yao, and T. Mei, “Videodrafter: Contentconsistent multi-scene video generation with llm,” arXiv preprint arXiv:2401.01256, 2024.
[44] T. Gupta, D. Schwenk, A. Farhadi, D. Hoiem, and A. Kembhavi, “Imagine this! scripts to compositions to videos,” in Proceedings of the European conference on computer vision (ECCV), pp. 598–613, 2018.


18
[45] L. Lian, B. Shi, A. Yala, T. Darrell, and B. Li, “Llm-grounded video diffusion models,” arXiv preprint arXiv:2309.17444, 2023.
[46] Z. Xing, Q. Feng, H. Chen, Q. Dai, H. Hu, H. Xu, Z. Wu, and Y.-G. Jiang, “A survey on video diffusion models,” arXiv preprint arXiv:2310.10647, 2023.
[47] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, et al., “Sora: A review on background, technology, limitations, and opportunities of large vision models,” arXiv preprint arXiv:2402.17177, 2024.
[48] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” Communications of the ACM, vol. 63, no. 11, pp. 139144, 2020. [49] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath, “Generative adversarial networks: An overview,” IEEE signal processing magazine, vol. 35, no. 1, pp. 53–65, 2018. [50] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F.-Y. Wang, “Generative adversarial networks: introduction and outlook,” IEEE/CAA Journal of Automatica Sinica, vol. 4, no. 4, pp. 588–598, 2017. [51] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.
[52] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” Advances in neural information processing systems, vol. 33, pp. 6840–6851, 2020. [53] “Autoregressive model,” 2024. [54] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural information processing systems, vol. 30, 2017.
[55] G. Mittal, T. Marwah, and V. N. Balasubramanian, “Sync-draw: Automatic video generation using deep recurrent attentive architectures,” in Proceedings of the 25th ACM international conference on Multimedia, pp. 1096–1104, 2017. [56] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation learning,” Advances in neural information processing systems, vol. 30, 2017. [57] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan, “Godiva: Generating open-domain videos from natural descriptions,” arXiv preprint arXiv:2104.14806, 2021.
[58] Y. Li, M. Min, D. Shen, D. Carlson, and L. Carin, “Video generation from text,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, 2018.
[59] K. Deng, T. Fei, X. Huang, and Y. Peng, “Irc-gan: Introspective recurrent convolutional gan for text-to-video generation.,” in IJCAI, pp. 2216–2222, 2019. [60] Y. Pan, Z. Qiu, T. Yao, H. Li, and T. Mei, “To create what you tell: Generating videos from captions,” in Proceedings of the 25th ACM international conference on Multimedia, pp. 1789–1798, 2017.
[61] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, “Howto100m: Learning a text-video embedding by watching hundred million narrated video clips,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 26302640, 2019. [62] J. S. Sepp Hochreiter, “Long short-term memory,” Neural Computation, vol. 9, pp. 1735–1780, 1997. [63] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al., “Stable video diffusion: Scaling latent video diffusion models to large datasets,” arXiv preprint arXiv:2311.15127, 2023.
[64] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, “Video diffusion models,” 2022.
[65]  ̈O.  ̧Cic ̧ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger, “3d u-net: learning dense volumetric segmentation from sparse annotation,” in Medical Image Computing and ComputerAssisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pp. 424432, Springer, 2016. [66] D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng, “Magicvideo: Efficient video generation with latent diffusion models,” arXiv preprint arXiv:2211.11018, 2022.
[67] Y. Zeng, G. Wei, J. Zheng, J. Zou, Y. Wei, Y. Zhang, and H. Li, “Make pixels dance: High-dynamic video generation,” arXiv preprint arXiv:2311.10982, 2023.
[68] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou, “Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation,” in Proceedings of
the IEEE/CVF International Conference on Computer Vision, pp. 76237633, 2023. [69] X. Wang, S. Zhang, H. Zhang, Y. Liu, Y. Zhang, C. Gao, and N. Sang, “Videolcm: Video latent consistency model,” arXiv preprint arXiv:2312.09109, 2023.
[70] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” arXiv preprint arXiv:2010.02502, 2020.
[71] H. Chen, Y. Zhang, X. Cun, M. Xia, X. Wang, C. Weng, and Y. Shan, “Videocrafter2: Overcoming data limitations for high-quality video diffusion models,” arXiv preprint arXiv:2401.09047, 2024.
[72] H. Chen, M. Xia, Y. He, Y. Zhang, X. Cun, S. Yang, J. Xing, Y. Liu, Q. Chen, X. Wang, et al., “Videocrafter1: Open diffusion models for high-quality video generation,” arXiv preprint arXiv:2310.19512, 2023. [73] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, et al., “Make-a-video: Textto-video generation without text-video data,” arXiv preprint arXiv:2209.14792, 2022.
[74] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, et al., “Imagen video: High definition video generation with diffusion models,” arXiv preprint arXiv:2210.02303, 2022.
[75] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu, “Motiondiffuse: Text-driven human motion generation with diffusion model,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.
[76] L. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi, “Text2video-zero: Textto-image diffusion models are zero-shot video generators,” arXiv preprint arXiv:2303.13439, 2023.
[77] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, “Videogpt: Video generation using vq-vae and transformers,” arXiv preprint arXiv:2104.10157, 2021.
[78] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan, “N  ̈uwa: Visual synthesis pre-training for neural visual world creation,” in European conference on computer vision, pp. 720–736, Springer, 2022. [79] C. Wu, J. Liang, X. Hu, Z. Gan, J. Wang, L. Wang, Z. Liu, Y. Fang, and N. Duan, “Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis,” arXiv preprint arXiv:2207.09814, 2022.
[80] H. Liu, W. Yan, M. Zaharia, and P. Abbeel, “World model on million-length video and language with ringattention,” arXiv preprint arXiv:2402.08268, 2024.
[81] J. Bruce, M. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps, et al., “Genie: Generative interactive environments,” arXiv preprint arXiv:2402.15391, 2024.
[82] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, “Cogvideo: Large-scale pretraining for text-to-video generation via transformers,” arXiv preprint arXiv:2205.15868, 2022.
[83] M. Ding, W. Zheng, W. Hong, and J. Tang, “Cogview2: Faster and better text-to-image generation via hierarchical transformers,” Advances in Neural Information Processing Systems, vol. 35, pp. 1689016902, 2022. [84] “Creating video from text,” 2024. [85] R. Wu, L. Chen, T. Yang, C. Guo, C. Li, and X. Zhang, “Lamp: Learn a motion pattern for few-shot-based video generation,” arXiv preprint arXiv:2310.10769, 2023.
[86] Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai, “Animatediff: Animate your personalized text-to-image diffusion models without specific tuning,” arXiv preprint arXiv:2307.04725, 2023. [87] J. Liang, Y. Fan, K. Zhang, R. Timofte, L. Van Gool, and R. Ranjan, “Movideo: Motion-aware video generation with diffusion models,” arXiv preprint arXiv:2311.11325, 2023.
[88] H. Fei, S. Wu, W. Ji, H. Zhang, and T.-S. Chua, “Empowering dynamics-aware text-to-video diffusion with large language models,” arXiv preprint arXiv:2308.13812, 2023.
[89] W. Weng, R. Feng, Y. Wang, Q. Dai, C. Wang, D. Yin, Z. Zhao, K. Qiu, J. Bao, Y. Yuan, et al., “Art•v: Auto-regressive text-to-video generation with diffusion models,” arXiv preprint arXiv:2311.18834, 2023. [90] J. Xing, M. Xia, Y. Zhang, H. Chen, X. Wang, T.-T. Wong, and Y. Shan, “Dynamicrafter: Animating open-domain images with video diffusion priors,” arXiv preprint arXiv:2310.12190, 2023.


19
[91] Y. Wang, J. Bao, W. Weng, R. Feng, D. Yin, T. Yang, J. Zhang, Q. D. Z. Zhao, C. Wang, K. Qiu, et al., “Microcinema: A divideand-conquer approach for text-to-video generation,” arXiv preprint arXiv:2311.18829, 2023.
[92] B. Peng, X. Chen, Y. Wang, C. Lu, and Y. Qiao, “Conditionvideo: Training-free condition-guided text-to-video generation,” arXiv preprint arXiv:2310.07697, 2023.
[93] Y. Wei, S. Zhang, Z. Qing, H. Yuan, Z. Liu, Y. Liu, Y. Zhang, J. Zhou, and H. Shan, “Dreamvideo: Composing your dream videos with customized subject and motion,” arXiv preprint arXiv:2312.04433, 2023. [94] X. Wang, S. Zhang, H. Yuan, Z. Qing, B. Gong, Y. Zhang, Y. Shen, C. Gao, and N. Sang, “A recipe for scaling up text-to-video generation with text-free videos,” arXiv preprint arXiv:2312.15770, 2023. [95] J. Lv, Y. Huang, M. Yan, J. Huang, J. Liu, Y. Liu, Y. Wen, X. Chen, and S. Chen, “Gpt4motion: Scripting physical motions in textto-video generation via blender-oriented gpt planning,” arXiv preprint arXiv:2311.12631, 2023.
[96] J. Yu, H. Zhu, L. Jiang, C. C. Loy, W. Cai, and W. Wu, “Celebvtext: A large-scale facial text-video dataset,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14805–14814, 2023. [97] J. Xu, T. Mei, T. Yao, and Y. Rui, “Msr-vtt: A large video description dataset for bridging video and language,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5288–5296, 2016. [98] L. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell, “Localizing moments in video with natural language,” in Proceedings of the IEEE international conference on computer vision, pp. 5803–5812, 2017. [99] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi, “Merlot: Multimodal neural script knowledge models,” Advances in Neural Information Processing Systems, vol. 34, pp. 2363423651, 2021. [100] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, “Frozen in time: A joint video and image encoder for end-to-end retrieval,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1728–1738, 2021. [101] H. Xue, T. Hang, Y. Zeng, Y. Sun, B. Liu, H. Yang, J. Fu, and B. Guo, “Advancing high-resolution video-language representation with large-scale video transcriptions,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 50365045, 2022. [102] Y. Wang, Y. He, Y. Li, K. Li, J. Yu, X. Ma, X. Li, G. Chen, X. Chen, Y. Wang, et al., “Internvid: A large-scale video-text dataset for multimodal understanding and generation,” arXiv preprint arXiv:2307.06942, 2023.
[103] W. Wang, H. Yang, Z. Tuo, H. He, J. Zhu, J. Fu, and J. Liu, “Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation,” arXiv preprint arXiv:2305.10874, 2023. [104] H. Xu, Q. Ye, X. Wu, M. Yan, Y. Miao, J. Ye, G. Xu, A. Hu, Y. Shi, G. Xu, et al., “Youku-mplug: A 10 million large-scale chinese videolanguage dataset for pre-training and benchmarks,” arXiv preprint arXiv:2306.04362, 2023.
[105] S. Chen, H. Li, Q. Wang, Z. Zhao, M. Sun, X. Zhu, and J. Liu, “Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset,” Advances in Neural Information Processing Systems, vol. 36, 2024. [106] T.-S. Chen, A. Siarohin, W. Menapace, E. Deyneka, H.-w. Chao, B. E. Jeon, Y. Fang, H.-Y. Lee, J. Ren, M.-H. Yang, et al., “Panda-70m: Captioning 70m videos with multiple cross-modality teachers,” arXiv preprint arXiv:2402.19479, 2024.
[107] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal, H. Larochelle, A. Courville, and B. Schiele, “Movie description,” International Journal of Computer Vision, vol. 123, pp. 94–120, 2017. [108] M. Soldan, A. Pardo, J. L. Alc ́azar, F. Caba, C. Zhao, S. Giancola, and B. Ghanem, “Mad: A scalable dataset for language grounding in videos from movie audio descriptions,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5026–5035, 2022. [109] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human actions classes from videos in the wild,” arXiv preprint arXiv:1212.0402, 2012.
[110] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles, “Activitynet: A large-scale video benchmark for human activity
understanding,” in Proceedings of the ieee conference on computer vision and pattern recognition, pp. 961–970, 2015.
[111] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta, “Hollywood in homes: Crowdsourcing data collection for activity understanding,” in Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14, pp. 510–526, Springer, 2016. [112] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al., “The kinetics human action video dataset,” arXiv preprint arXiv:1705.06950, 2017. [113] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles, “Dense-captioning events in videos,” in Proceedings of the IEEE international conference on computer vision, pp. 706–715, 2017.
[114] G. A. Sigurdsson, A. Gupta, C. Schmid, A. Farhadi, and K. Alahari, “Charades-ego: A large-scale dataset of paired third and first person videos,” arXiv preprint arXiv:1804.09626, 2018.
[115] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. MuellerFreitag, et al., “The” something something” video database for learning and evaluating visual common sense,” in Proceedings of the IEEE international conference on computer vision, pp. 5842–5850, 2017. [116] R. Sanabria, O. Caglayan, S. Palaskar, D. Elliott, L. Barrault, L. Specia, and F. Metze, “How2: a large-scale dataset for multimodal language understanding,” arXiv preprint arXiv:1811.00347, 2018. [117] L. Zhou, C. Xu, and J. Corso, “Towards automatic learning of procedures from web instructional videos,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, 2018.
[118] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al., “Scaling egocentric vision: The epic-kitchens dataset,” in Proceedings of the European conference on computer vision (ECCV), pp. 720–736, 2018. [119] “Youku.” https://www.youku.com/. [120] “Youtube.” https://www.youtube.com/. [121] Z. Xing, Q. Dai, H. Hu, Z. Wu, and Y.-G. Jiang, “Simda: Simple diffusion adapter for efficient video generation,” arXiv preprint arXiv:2308.09710, 2023.
[122] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004. [123] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for training gans,” Advances in neural information processing systems, vol. 29, 2016.
[124] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the inception architecture for computer vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. [125] J. R. Hershey and P. A. Olsen, “Approximating the kullback leibler divergence between gaussian mixture models,” in 2007 IEEE International Conference on Acoustics, Speech and Signal ProcessingICASSP’07, vol. 4, pp. IV–317, IEEE, 2007. [126] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale update rule converge to a local nash equilibrium,” Advances in neural information processing systems, vol. 30, 2017. [127] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable visual models from natural language supervision,” in International conference on machine learning, pp. 8748–8763, PMLR, 2021. [128] M. Saito, S. Saito, M. Koyama, and S. Kobayashi, “Train sparsely, generate densely: Memory-efficient unsupervised training of highresolution temporal gan,” International Journal of Computer Vision, vol. 128, no. 10-11, pp. 2586–2606, 2020. [129] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning spatiotemporal features with 3d convolutional networks,” in Proceedings of the IEEE international conference on computer vision, pp. 4489–4497, 2015. [130] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model and the kinetics dataset,” in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 62996308, 2017. [131] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, “Fvd: A new metric for video generation,” 2019.


20
[132] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, “Towards accurate generative models of video: A new metric & challenges,” arXiv preprint arXiv:1812.01717, 2018.
[133] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in Artificial intelligence and statistics, pp. 1273–1282, PMLR, 2017. [134] J. Zhang, S. Vahidian, M. Kuo, C. Li, R. Zhang, G. Wang, and Y. Chen, “Towards building the federated gpt: Federated instruction tuning,” arXiv preprint arXiv:2305.05644, 2023.
[135] X. Huang, P. Li, H. Du, J. Kang, D. Niyato, D. I. Kim, and Y. Wu, “Federated learning-empowered ai-generated content in wireless networks,” IEEE Network, 2024. [136] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al., “Metagpt: Meta programming for multi-agent collaborative framework,” arXiv preprint arXiv:2308.00352, 2023.
[137] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel: Communicative agents for” mind” exploration of large scale language model society,” arXiv preprint arXiv:2303.17760, 2023.
[138] Y. Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing the power of intelligent llm agents,” arXiv preprint arXiv:2306.03314, 2023.
[139] S. Han, Q. Zhang, Y. Yao, W. Jin, Z. Xu, and C. He, “Llm multiagent systems: Challenges and open problems,” arXiv preprint arXiv:2402.03578, 2024.
[140] S. Abdelnabi, A. Gomaa, S. Sivaprasad, L. Scho ̈nherr, and M. Fritz, “Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games,” arXiv preprint arXiv:2309.17234, 2023.
[141] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Billard, “Recent advances in robot learning from demonstration,” Annual review of control, robotics, and autonomous systems, vol. 3, pp. 297330, 2020. [142] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, “Universal manipulation interface: Inthe-wild robot teaching without in-the-wild robots,” arXiv preprint arXiv:2402.10329, 2024.
[143] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, “Voxposer: Composable 3d value maps for robotic manipulation with language models,” arXiv preprint arXiv:2307.05973, 2023.
[144] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “NeRF: Representing scenes as neural radiance fields for view synthesis,” in The European Conference on Computer Vision (ECCV), 2020.
[145] B. Kerbl, G. Kopanas, T. Leimk  ̈uhler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” ACM Transactions on Graphics, vol. 42, no. 4, 2023.
[146] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, and P. Isola, “Distilled feature fields enable few-shot language-guided manipulation,” arXiv preprint arXiv:2308.07931, 2023.
[147] M. Pollefeys, D. Niste ́r, J.-M. Frahm, A. Akbarzadeh, P. Mordohai, B. Clipp, C. Engels, D. Gallup, S.-J. Kim, P. Merrell, et al., “Detailed real-time urban 3d reconstruction from video,” International Journal of Computer Vision, vol. 78, pp. 143–167, 2008. [148] T. Li, M. Slavcheva, M. Zollhoefer, S. Green, C. Lassner, C. Kim, T. Schmidt, S. Lovegrove, M. Goesele, R. Newcombe, et al., “Neural 3d video synthesis from multi-view video,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5521–5531, 2022. [149] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, “Neuralrecon: Real-time coherent 3d reconstruction from monocular video,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15598–15607, 2021. [150] Z. Wu, Y. Li, H. Yan, T. Shang, W. Sun, S. Wang, R. Cui, W. Liu, H. Sato, H. Li, et al., “Blockfusion: Expandable 3d scene generation using latent tri-plane extrapolation,” arXiv preprint arXiv:2401.17053, 2024.
[151] D. Jones, C. Snider, A. Nassehi, J. Yon, and B. Hicks, “Characterising the digital twin: A systematic literature review,” CIRP journal of manufacturing science and technology, vol. 29, pp. 36–52, 2020. [152] C. Chen and K. Shu, “Can llm-generated misinformation be detected?,” arXiv preprint arXiv:2309.13788, 2023.
[153] Z. Liu, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao, W. Liu, D. Shen, Q. Li, et al., “Deid-gpt: Zero-shot medical text deidentification by gpt-4,” arXiv preprint arXiv:2303.11032, 2023.
[154] J. Yao, X. Yi, X. Wang, Y. Gong, and X. Xie, “Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values,” arXiv preprint arXiv:2311.10766, 2023.