Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video
Generation
FARAZ WASEEM and MUHAMMAD SHAHZAD∗, University Of Reading, UK
An image may convey a thousand words, but a video—composed of hundreds or thousands of image frames—tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI’s Sora [110], the current state-of-the-art system, is still limited to producing videos of up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions—essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative AI with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.
Additional Key Words and Phrases: Survey, Text-to-Video Generation, Text-to-Image Generation, Generative AI, Video Editing, Temporal Dynamics, Scalability in AI, Artificial General Intelligence, AI Models Generalization
ACM Reference Format:
Faraz Waseem and Muhammad Shahzad. 2024. Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation. 1, 1 (December 2024), 35 pages. https://doi.org/XXXXXXX.XXXXXXX
1 Introduction
The year 2022 marked a significant milestone in the field of the generative AI era with the release of ChatGPT [108]. ChatGPT is an advanced language model that produces human-like text from user input, supporting tasks like answering questions, creative writing, and conversation. This technology uses complex deep neural networks based on large language models trained on extensive text data to capture intricate linguistic patterns and contextual nuances for precise text generation and understanding. Since then, major tech companies have introduced their large language models (LLMS), such as Facebook’s LLama series [103], Google’s Gemini [102], and a few other notable models, including Claude [101] and Mistral [107].
A transformative breakthrough in image generation accompanied the success of LLMs. DALL-E 2 [79] represented a significant leap beyond traditional Generative Adversarial Network (GAN) and Variational Autoencoder (VAE) models, thanks to its ability to interpret natural language and render a wide range of concepts and styles. It excels at creating photo-realistic images from both realistic and imaginative prompts. Other leading generative AI systems, such as Stable Diffusion 3 [19] and MidJourney [106], also showcase remarkable capabilities in producing photo-realistic visuals based on real and imagined ideas.
Authors’ Contact Information: Faraz Waseem, faraz.waseem@pgr.reading.ac.uk; Muhammad Shahzad, m.shahzad2@reading.ac.uk, University Of Reading, Reading, Berkshire, UK.
2024. Manuscript submitted to ACM
Manuscript submitted to ACM 1
arXiv:2412.18688v1 [cs.CV] 24 Dec 2024


2 Faraz and Shahzad
Fig. 1. Example of semantic content not changing with the progress of frames [34].
Fig. 2. Example Of semantic content changing with the progress of frames [55].
Generating videos is significantly more challenging than creating text or images. Unlike static images or text, videos are dynamic, featuring evolving camera angles, motion, deformations, and occlusion patterns. Producing realistic, long-duration videos requires continuously generating new content while preserving consistency. Videos are composed of multiple frames, with their semantic content evolving. Here, a frame’s "semantic content" refers to a conceptual map that represents how various elements within the frame—such as objects, actions, and interactions—contribute to the overall understanding of the video. Fig. 1 illustrates an example of a single-scene video featuring a girl dancing with repetitive motions against a static background. This example highlights minimal changes in semantic content across frames, with no introduction of new characters or actions. Such a video clip may be classified as a single scene whose semantic content remains unchanged or broadly consistent throughout. In contrast, Fig. 2 shows an example of a multi-scene video where the number of objects changes and a new actor appears at a later frame. In this case, the semantic content of the frames changes as the video progresses.
Due to the complexities of dynamic scenes, early video generation models were limited to producing short clips lasting only a few seconds, often animating a single static frame without incorporating varying backgrounds or objects. For example, Make-A-Video [88] and RunwayML Gen-2 [104] generate 4-5 second videos using a single animated frame with little change in semantic content. CogVideo [35] is among the first long video generation models to create extended videos using autoregressive transformers. However, it operated based on a single prompt and also exhibited minimal changes in the semantic content of the video. Phenaki [120], employing autoregressive video transformers, is one of the first models to generate long videos with dynamic semantic content based on multiple prompts. Similarly, Gen-L-Video [124] video uses a diffusion model to combine short video clips into a single, continuous video. Recently, Sora [110] has established a new state-of-the-art in video generation. It utilizes diffusion transformers and sampling from a compressed spatiotemporal representation and produces videos that are visually impressive and rich in semantic content. Another state of art model in long video generation is Gen-3 Alpha by RunwayML [105]. It is a commercial model that can generate photo-realistic videos up to 10 seconds long and is also based on diffusion transformers. Fig. 3 shows the evolution of these long video generation techniques, which are still in their early stages compared to human-made commercial videos, especially regarding video length and narrative organization.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 3
Fig. 3. Evolution of long video generation models. Later models like SORA [110] and Gen-3 Alpha [105] focus more on the quality of videos than the length of the video.
Fig. 4. Most papers focusing on long video generation were published in 2023 and 2024.
1.1 Survey Contributions — Need for Summarizing Long Video Generation Methods
Long video generation presents numerous challenges beyond crafting and maintaining consistent storylines across scenes. These include the lack of large-scale video datasets with detailed captions and the requirement of significant computational resources. Despite these hurdles, long video generation has emerged as a new frontier in generative AI, offering potential applications in entertainment, education, medicine, marketing, and gaming. This has attracted significant research interest, leading to a rapid increase in studies. As shown in Fig. 4, most papers on long video generation have been published within the last two years.
Owing to these interests and opportunities, the time is right to summarize the state-of-the-art in long video generation and discuss the associated challenges, progress, and future directions to support the advancement of the field. To our knowledge, there are only two related long video-generation surveys [50] and [154]. The former [50] delves into the latest trends in long video generation, emphasizing the divide-and-conquer and autoregressive approaches as two primary themes. It also examines photo-realism trends and generative paradigms, such as VAE, GAN, and diffusion-based models. However, while it highlights the divide-and-conquer approach, which simplifies the complexity
Manuscript submitted to ACM


4 Faraz and Shahzad
of long videos by breaking them into smaller, manageable chunks, a detailed exploration of this methodology—such as how short videos can be seamlessly integrated into longer narratives—is lacking. Our work aims to bridge this gap by thoroughly analyzing the various dimensions of the divide-and-conquer strategy and its role in addressing the challenges of long video generation. In contrast, the latter [154] offers a broader overview of the video generation field, covering topics like long videos, video editing, super-resolution, datasets, and metrics. However, long video generation is only one of many topics discussed. It highlights the need for a focused, in-depth study on long video generation. Our contribution addresses this need by comprehensively analyzing this emerging field and highlighting key approaches, challenges, and future prospects.
The proposed work addresses gaps in existing literature by incorporating recent studies and conducting a comprehensive analysis of the divide-and-conquer approach. Specifically, we focus on underexplored aspects, such as agent-based networks and methods for transitioning from short to long videos, which are missing from current reviews. Furthermore, we extend our examination to methodologies and papers that fall outside the divide-and-conquer and autoregressive categories and the latest research not covered by previous surveys. In addition, we dive deeply into the literature, emphasizing the algorithms, models, and input control techniques employed in generating long videos, offering a more holistic and detailed perspective on the field.
1.2 Survey Focus — Techniques, Challenges & Key Questions in Long Video Generation
Video generation utilizes various techniques, such as sampling from latent space [57], creating small video segments or images, generating intermediate frames ("divide and conquer")[3.2], employing autoregressive methods[3.1] to predict future frames based on initial ones, and enhancing latent state representations for longer videos. Training video generation models presents challenges due to the higher computational requirements and more extensive memory needs for video datasets. Many video generation models build on pre-trained image models[ [37], [62], [5]], enhancing attention mechanisms to ensure consistency between adjacent frames, as video is essentially a sequence of frames. Some long video models are developed by extending short video generation models[ [147], [11], [124]] and improving control mechanisms for longer content. Another significant aspect of video generation is input guidance[[4.1],[4.3],[4.2] ]. Unlike image or short video generation, guidance is crucial for long video generation and is often based on text embedding like CLIP [76]. LLMs [3.2.1] are crucial in guiding the video generation process by understanding world dynamics, object trajectories, and action groupings, using their extensive knowledge to inform the creation of videos. When evaluating the quality of the generated videos[[6.1],[6.2],[ 6.3],[ 6.4]], it is important to assess the quality of individual frames, the fluidity of motion, and the overall aesthetic appeal. It is essential to ensure that the generated videos align with the input text and maintain the consistency of entities throughout the video, such as cars and actors. This has sparked interest in investigating innovative research directions in the field and addressing the following relevant research questions:
(1) How can we generate long videos with multiple semantic segments with different actors, actions, and objects? (2) How can we ensure semantic consistency across long video segments, such as maintaining consistent models of objects like cars? (3) How can we manage resource requirements for training and inference with long videos? (4) How can we handle control signals (conditioning) for extended video sequences?
Our survey article centers around these critical questions, providing insights to guide researchers and practitioners in addressing these challenges. Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 5
1.3 Survey Methodology
For this survey, we conducted searches across several conferences, including but not limited to ICCV, ECCV, CVPR, ICLR, ICML, IEEE, TPAMI, IEEE Neural Networks, AAAI, WACV, ICIP, NeurIPS, KDD, ACCV, IJCV, AI Open, and CVIU. We used keywords such as "video generation," "long video generation," and "LLM-guided video generation." Additionally, we searched academic databases, including Google Scholar, IEEE Xplore, ACM Transactions, and Scopus, focusing on the term "long video generation" for our survey.
Our survey covers papers published between 2021 and 2024 (till August end), with a focus on "video generation" and "generative AI". We gathered 100+ articles through snowball sampling, using keywords like text-to-video, generative AI, visual interpretation, and extended video generation.
1.4 Survey Organization
We will begin by discussing the foundational framework for video generation, including embedding and LLMs, to set the stage for more advanced topics. The goal is to familiarize readers with these fundamental components, enabling them to explore these building blocks according to their level of expertise. Next, we will explore backbone mechanisms for video generation, like divide and conquer autoregressive and global enhancements. We will explore input guidance mechanisms, including strategies like LLM guidance, and categorize them into different levels based on the depth of control the LLM exerts. We will also address the necessary image and video diffusion model modifications to facilitate such control. We will also discuss the post-processing pipelines required to achieve high temporal and spatial quality in videos generated by diffusion models. We will then discuss datasets used to train video generation models. We will then talk about metrics used to measure generated video quality. Finally, we will talk about future trends and open challenges.
2 Long Video Generation: Backbone Architectures and Methods
Progress in long video generation builds on advancements in many foundational building blocks. These include GANsbased architecture 2.1, autoencoders 2.2, Transformers-based models 2.3, LLMs and language understanding 2.4 and image and video diffusion models 2.5.
2.1 GAN Based Video Generation
Generative Adversarial Networks (GANs) were considered the state-of-the-art (SOTA) for generative tasks from around 2014 to the early 2020s, but recent advancements such as diffusion models and transformer-based methods have surpassed them in both performance and versatility, particularly in applications like image synthesis and video generation. This section explores the formulation of GANs and highlights the pivotal papers that have shaped advancements in the field. GANs [22] are inspired by game theory and consist of two core components: the Generator and the Discriminator. The Generator’s purpose is to convert random noise, typically sampled from a simple, uniform distribution, into data samples, which can range from images to videos. Simultaneously, the Discriminator assesses these samples, classifying them as real (originating from the training dataset) or fake (created by the Generator). This GAN formulation is the basis of image and video generation, which further sections explain.
2.1.1 GAN Based Image Generation.
Manuscript submitted to ACM


6 Faraz and Shahzad
GANs initially revolutionized image generation, dominating the field. Here, we examine key ideas and milestones in GAN literature, organized by timeline.
Early GANS: GANs [22] was the first to generate images using adversarial networks but employed simple feedforward neural networks for both the Generator and the Discriminator. DCGANs [77] extends the GAN architecture by incorporating convolutional layers, making them more suitable for image data. They generated images with a resolution of 64×64. LAPGAN [12] increased the resolution of images by generating them multi-scale. It consists of multiple GANs, each generating images at different resolutions. GANs, DCGANs, and LAPGAN are primarily designed to generate images based on random noise vectors. These models don’t take text guidance, and text-based GANs add text-based control, which will be discussed in the next section.
GAN Text Input: [149] is a multistage text-to-image GAN that can create high-quality images. It has two GAN stacks on each other. The first one takes the text and generates a low-resolution image. The second one takes both text and input images and creates high-quality images. AttnGAN [138] uses attention mechanisms to create images from text, allowing it to focus on specific words or phrases in the input description.
Style/Image Transfer: GAN was the first generative model to generate high-quality artistic images, and one of the key innovations was transferring an artistic style like Vincent Van Gogh’s to real-world pictures and image translation. CycleGAN [157] does image-to-image translation and consists of two generators and two discriminators. StyleGAN [43] primarily focuses on generating high-quality, diverse images, particularly faces. It introduced disentangled latent space. Latent space is where a vector of N dimensions represents each image. Projecting different high-level attributes like skin color, hairstyle, etc., on distinct dimensions in latent space provides excellent editing capabilities for realistic image generation, semantic manipulation, and local editing. StyleGAN opened the doors for high-level image manipulation. StyleGAN2 [119] is an improvement over StyleGAN for better quality images. pix2pix [40] specifically designed for image-to-image translation tasks. It learns a conditional generative model and generates an output image conditioned on the input image. GAN also revolutionized video generation, which will be explored in 2.1.2.
2.1.2 Video/Multi Frame Generation.
Early Attempts: Early research in video synthesis primarily concentrated on video prediction like in [65], which involves generating future frames based on a sequence of previously observed ones. [122] extended the convolutional model to include two distinct convolution networks. It decomposes video into a static background and a moving foreground, generating them with 2D and 3D convolutional networks. VGAN can generate 32 frames of realistic videos of golf courses, beaches, train stations, etc. VGAN is an unconditional video generator, i.e., without incorporating any auxiliary input such as text. ’Multi-stage Dynamic Generative Adversarial Networks’ [136] predicts future video frames based on the first frame. It improved existing models by generating 32 frames with 128 x 128 resolution in a time-lapse pattern. The first stage generates a time-lapse video with realistic content in its two-stage model. In contrast, the second stage optimizes the results from the first stage, primarily by incorporating dynamic motion information to enhance realism. The models mentioned above are not prompt-based, and prompt-based models will be explored in the following section.
Prompt Based Guidance: Numerous studies have explored the use of conditional inputs in GAN to guide and refine the generation process. These conditions can take various forms, including audio signals, text prompts, semantic maps, images, or other videos. TGANs-C [71] incorporate text guidance using LSTM-based latent vectors. TGANs-C was designed to input a single sentence.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 7
Long Video Generation Using GAN: DIGAN [144] can create 128 frame video. It introduced an INR (Implicit Neural Representations) based video generator that improves motion dynamics by manipulating space and time coordinates differently and a motion discriminator that efficiently identifies unnatural motions without observing long frame sequences. StyleGan-V [90] improved the state of the art and was built on StyleGAN2 [44]. It can generate highresolution 1024-long videos by designing a holistic discriminator that aggregates temporal information by simply concatenating frames’ features to decrease the training cost.
2.2 Autoencoder Based Video Generation
Autoencoders [114], variational autoencoders [47], and masked autoencoders [25] belong to the family of models that compress information into a compact latent space and serve as building blocks for image and video generation pipelines. Masked autoencoders can generate videos from this learned latent space. We will discuss the foundations of autoencoders and build up the video generation process via masked autoencoders.
2.2.1 Autoencoder Formulation. Autoencoder is an unsupervised neural network that compresses its input to a compact latent layer and then learns to reproduce its input via backpropagation. The autoencoder is trained to minimize the reconstruction loss between the input x and the reconstructed output xˆ. The most common application of autoencoder for video and long video generation is the construction of compressed latent space. For example, in [84], authors use an encoder and decoder to project images from pixel to latent space to decrease the computational complexity of learning image distribution.
Variational Autoencoders (VAEs) [47] are probabilistic generative models based on the principles of Bayesian networks. VAEs utilize a reconstruction strategy, where they project input video data from a high-dimensional space to a low-dimensional space and then reconstruct the video from this compact representation, also called the bottleneck layer, back to the original high-dimensional space. While traditional autoencoders learn compressed latent vectors, they cannot generate new data points, as they learn a fixed latent representation of the input data. VAEs solve this issue by converting the input into a distribution in the latent space rather than discrete points. An application of Variational Autoencoders (VAEs) for video generation is VQ-VAE [117]. VQ-VAE learns downstream latent representations using the Vector Quantized-Variational AutoEncoder (VQ-VAE), which is used in many video and image generation pipelines, such as VideoGen [152], VQGAN [20], and DALL-E [80]. In addition to using VAE models for compression, there is related work on using Variational Autoencoders for video generation. For instance, Hierarchical Patch VAE-GAN [24] uses a hierarchical approach to leverage both VAE and GAN techniques. VAEs have also been applied in video anomaly detection. In [131], the authors designed an LSTM-Convolutional-based Autoencoder to learn the distribution of single-scene videos, where the camera is fixed on a single background.
2.2.2 Masked Autoencoders .
Masked autoencoder [25] are scalable self-supervised learners for computer vision and have been used as the backbone for video generation by masking random patches of the input image and reconstructing the missing pixels. Video mask autoencoder is based on image mask autoencoder. During pre-training, a sizeable random portion of image patches are masked. The encoder processes only the remaining visible patches. After encoding, mask tokens are introduced, and the combined set of encoded patches and mask tokens is passed through a small decoder to reconstruct the original image at the pixel level. An extension of this approach to the video domain but using convolution instead of transformers is VideoMAC [73]. It employs Masked Encoders and resource-friendly convNet architecture. VideoMAC is
Manuscript submitted to ACM


8 Faraz and Shahzad
designed to reconstruct masked patches, randomly and symmetrically applied to frame pairs with a high masking ratio (0.75, as used in this study). Another notable model is the MAGVIT Masked Generative Video Transformer [142]. MAGVIT outperforms existing diffusion and autoregressive models by order of magnitude in inference time. MAGVLT [46] is also based on a masked autoencoder backbone. It proposed a unified generative vision-and-language (VL) model to produce images and text sequences.
2.3 Transformer Based Video Generation
GANs have limitations like mode collapse [32], training instability, which needs fine-tuning of parameters, and a lot of training time and resources. Transformers, introduced in 2017 [118], made inroads into image and generation via autoregressive and masked encoding. Some of the key concepts to understand are Vision Transformers and Video Transformers.
2.3.1 Transformer Based Image Generation.
Vision Transformers (ViT) [18] represent a significant shift in the landscape of computer vision. The ViT architecture operates similarly to transformers used in natural language processing by dividing an image into smaller patches, like breaking text into tokens. DALL-E [81] trained autoregressive transformer using 250 million image text pairs. They used ’Discrete VAE’ [83] to compress 256*256 images to 32 × 32 grid of image tokens. They concat these tokens with corresponding BPE text tokens and autoregressively learn the joint distribution of text and image tokens using a GPT transformer. CogView [15] based on GPT transformer architecture outperforms DALL-E using FID distance, but DALL-E has better rendering capabilities of complex prompts. Autoregressive approaches like DALL-E and CogView suffer from slow generation because of their unidirectional token-by-token generation nature. CogView2 [17] improved the limitation of previous methods by training the Cross-Modal General Language Model by learning the joint distribution of image and text tokens using masking.
2.3.2 Autoregressive Based Video Generation.
Video Transformers (VViT) [3] represent a significant shift in the landscape of computer video generation. The VViT architecture functions like vision transformers [18], but instead of tokenizing image patches, it tokenizes video patches. Phenaki [121] can generate arbitrary long videos conditioned on a sequence of prompts. Phenaki uses a pre-trained language model t5x for text embeddings, [82]. It uses a variation of ViViT [3], C-ViViT, which extracts and compresses video tokens to a minimum. It masked some tokens and learned to predict masked video tokens by the bidirectional transformer. During inference, it can predict the next token using an autoregressive transformer. The compression mechanism allows us to train and predict very long video sequences. CogVideo [35] extend CogView2 [17]. It uses hierarchical training with multiple frame rates to enhance the alignment of text-clip pairs, significantly improving generation accuracy, especially for movements with complex semantics. In its two-stage architecture for video generation, it first generates keyframes auto-regressive, and then another transformer interpolates between these frames.
2.4 Language Understanding In Video Generation
2.4.1 Text To Image Feature Representation.
The core principle behind text-based visual generation tasks is effectively pairing text with the visual content. Many visual generation pipelines leverage pre-existing image-text pair models like CLIP (Contrastive Language-Image
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 9
Pretraining) [76]. CLIP has been pre-trained using a contrastive learning approach that optimizes the cosine similarity between image and text embedding. Given CLIP’s robust performance, many visual generation models like DALL·E 2 [79] incorporate CLIP’s text embedding to leverage its superior semantic understanding. It allows these models to enhance their ability to generate visually relevant and contextually appropriate content, effectively bridging the gap between text and visual representation.
2.4.2 LLMs Based Video Guidance.
Many visual generation models like LLM Director [156] leveraged standalone large language models (LLM) [6] to enhance their performance. By integrating LLM, visual generation models can benefit from advanced natural language processing capabilities, allowing them to interpret and generate more nuanced and contextually relevant descriptions of captions in a single or multiple prompt with detailed scenes. One example of this design is LLM grounded VDM [52]. When paired with visual inputs, LLMs can transform simple image descriptions into more elaborate storytelling, adding layers of meaning and context that enhance the viewer’s experience. LLM can also act as director of the whole video generation process and create coherent script shown by Vlogger [158]. The details on how the recent long video generation methods leverage LLMs are explained in 3.2.1.
2.5 Diffusion Models
Diffusion models are now state-of-the-art in image and video diffusion. Diffusion-based image and video pipelines use many building blocks, such as variational autoencoders, transformers, and language understanding, and have introduced new baselines. We will briefly cover the history and progress of diffusion-based architecture. The foundational work [92] established a crucial framework for this approach, applying principles from nonequilibrium thermodynamics to unsupervised learning. This insight paved the way for subsequent advancements in generative modeling. A probabilistic diffusion model [30] is a parameterized Markov chain trained via variational inference to generate samples that closely resemble the data over a finite duration. These ideas are applied to image and video generation, as discussed in the next section.
2.5.1 Image Diffusion .
Image diffusion models generate images by denoising noisy inputs through noise prediction. [94] introduced a novel perspective by estimating the data distribution’s gradients, further enriching the understanding of generative processes. However, the significant breakthrough in this area came in with [30] influential paper on "Denoising Diffusion Probabilistic Models." do forward and reverse diffusion processes in latent space. The general mechanism for conditioning from single modality like text is to have a conditional denoising decoder using a cross attention layer of transformer in UNET backbone trained with embedding from text encoder like CLIP [76]. In addition to CLIP, text encoders like BERT [13] or T5 Encoder [78] can be used. In their paper [84], the authors implement this paradigm by conditioning image generation through modifications to the attention layer of the U-Net. They achieve this by concatenating embeddings from different modalities, such as text or others [118].
2.5.2 Video Generation From Diffusion models.
Video diffusion models are built on the same diffusion architecture as image diffusion models, but video diffusion models also need to ensure consistency between frames. Image diffusion model can generate videos by either extending it into the temporal domain, as done in the Video Diffusion Model [31], or by generating keyframes using an image
Manuscript submitted to ACM


10 Faraz and Shahzad
diffusion model with a modified attention and noise sampling strategy to maintain frame consistency. We can interpolate these keyframes to achieve smoother transitions, as discussed in FreeBloom. [38].
U-Net and Transformers are the two primary backbone architectures driving video generation models. Video Diffusion Models adopt a standard U-Net-based diffusion model setup but modify the architecture for video modeling. It extends the traditional 2D U-Net into a 3D architecture, where each feature represents a 4D tensor that includes frames, height, width, and channels. This 3D U-Net is factorized over space and time, incorporating spatial and temporal attention mechanisms. The spatial attention block focuses on relevant regions within a single frame, while the temporal attention block captures dependencies across different frames over the temporal domain. On the other hand, Sora [109] is based on a Transformer-based diffusion model [72], which operates on spacetime patches of video and image latent codes. Visual input is represented as a sequence of spacetime patches, which serve as input tokens for the Transformer, as explained in [109]. Long video generation strategies are discussed in detail in the next section.
3 Long Video: Generation Paradigm
We summarized various video generation approaches into three core paradigms.
• Auto-regressive Paradigm: Videos are generated sequentially, with each frame conditioned on the frames generated before. • Divide and Conquer Approach: Videos are produced by creating keyframes or short video segments guided by storyline prompts, often with the aid of a large language model (LLM). • Implicit Video Generation: Videos are generated implicitly from the model without needing explicit extrapolation (autoregressive approach) or explicit interpolation (divide-and-conquer) by designing latent space to represent variable-size videos.
These approaches will be explained in the following sections.
3.1 Auto Regressive Approaches
The Autoregressive generation paradigm works by future frame prediction. It trains and performs inference of the model on previous video frames. Each frame serves as the input for the future frames. This prediction process ensures the generated videos are consistent and coherently structured. It generates a context from anchor frame (generated via text prompt), text-to-image embedding, image, or multimodal input [ [14], [120], [ [113]]. It uses context and previous N frames and generates future N frames optionally based on additional prompts and prior frames. It ensures robust consistency between frames due to the sequential nature of frame transition. This is illustarted in Fig. 5. Nevertheless, it needs to improve performance bottlenecks when generating long videos, as it has to create frames sequentially and can not leverage parallelization.
CogVideo [35] is one of the first open-source autoregressive transformers for long video generation. CogVideo builds on the pre-trained text-to-image model CogView2 [16] to inherit the knowledge gained from text-to-image pretraining. CogVideo has limitations on the length of the input sequence and the spatial resolution of frames (default 160 x 160 can be upsampled to 480 x 480). NUWA-Infinity [54] improved spatial resolution by adopting a hierarchical video generation model. One limitation of CogVideo and NUWA-Infinity is that they consume a single prompt as input. Phenaki [120] can handle multiple progressive text prompts. The model manages to preserve the temporal coherence of the video while adapting to the new prompt. It uses a single-level autoregressive model instead of the two-level Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 11
Fig. 5. The basic theme of the auto-regressive approach is that it generates new frames, given the initial anchor frame, previous frames, and optional progressive prompts [132].
architecture and compresses dynamic tokens using C-ViViT (build on top of ViViT [3] ). The compression mechanism allows it to train and predict very long video sequences. Phenaki uses a pre-trained language model t5x [82] for text embeddings. It masked some tokens and learned to predict masked video tokens by the bidirectional transformer. During inference, It can predict the next token using an autoregressive transformer. One of the limitations of Phenaki is the memory requirements for training. MeBT [141] proposed a memory-efficient bidirectional transformer that overcomes the memory limitations of auto-regressive methods by proposing an efficient transformer for video synthesis that can fully leverage the long-range dependency of video frames during training, while being able to achieve fast generation with linear time complexity. MeBT employs an encoder-decoder architecture based on a fixed number of latent bottleneck tokens, and that fixed scale of latent tokens gives its memory efficiency.
All approaches discussed [ [54]],[ [14]],[ [120]] are based on transformers. Grid Diffusion [49] is based on diffusion. Grid Diffusion first used compression and represented video using an image created from keyframes, which covers the primary motions or events of the video. It is called a ’grid image,’ which consists of 4 subframes representing video keyframes. During the training phase, they masked these frames and learned to produce masked frames conditioned on previous grid-image and nonmasked images. This design paradigm is illustrated in Fig. 6. As they replaced the challenge of video generation with image generation, they can create long videos up to 128 frames autoregressively with high image quality (low FVD scores [115]. They utilized transfer learning of pre-trained stable diffusion model. [84] and have used only 2 Nvidia A-100 for training. Fig. 6 explains this architecture.
Previously described approaches [ [54], [14], [120], [49]] have limitation to render complex compositional prompts. Compositional prompts describe spatio-temporal entities that have dynamic interaction. For example, let’s take the example of a prompt: a man is walking with a black dog on the right side, and a blue car is driving from the opposite side. VideoTetris [113] take on the challenge of compositional prompts. VideoTetris addresses this issue by manipulating and composing the attention maps of spatially and temporally denoising networks. It supports long video generation with progressive compositional prompts, where ’progressive’ refers to continuous changes in objects’ position, quantity, and attributes by introducing spatiotemporal Compositional Diffusion, which manipulates cross-attention values in denoising networks to synthesize videos that adhere to complex or evolving instructions.
Manuscript submitted to ACM


12 Faraz and Shahzad
Fig. 6. Grid Diffusion Model. It first generates a grid image and then learns a spatial auto-regressive model by learning to predict masked subframe conditions on previous images and unmasked frames. [49].
Table 1. Auto Regressive Approaches.
Model Theme Month/Year
StyleGAN-V [91] Time-continuous signals [89] [67] extended from StyleGAN2 [119] Dec 2021
DIGAN [144] Implict neural representation based [89] video generation model Feb 2022
CogVideo [155] Long videos using autoregressive and interpolation stages May 2022
NUWA-Infinity [54] Long videos with hierarchical autoregressive modeling July 2022
Phenaki [120] Compresses videos into discrete tokens for efficient frame generation Oct 2022
PVDM [143] PVDM uses diffusion in latent space for video generation Feb 2023
MeBT [141] Memory efficient transformer for long-range dependency videos March 2023
ART•V [132] Auto-regressive using keyframes and image diffusion Nov 2023
StreamingT2V [2] Long videos with consistent transitions and scene preservation March 2024
Grid Diffusion Models [49] Video generation by merging four keyframes into images March 2024
ViD-GPT [21] GPT-style autoregressive generation into video diffusion models June 2024
FlexiFilm [70] Long videos with temporal conditioning and resampling strategy June 2024
VideoTetris [113] Text-to-video generation with spatio-temporal compositional diffusion Oct 2024
Autautoregressive long video generation approaches have evolved from single prompt, low-resolution video of CogVideo [14] animating single scene to modern architecture like VideoTetris, which can model complex science dynamics with multiple prompts and can maintain high image quality. Autoregressive approaches provide smooth motion transitions between frames but suffer from slow generation due to their sequential nature. Also, it offers less control over complex scene dynamics like actors, bounding boxes, and spatial relationships between entities. Divide and conquer approaches, as discussed in 3.2, attempt to solve this problem by generating parallel frames and, often with the help of LLM, create video blueprints that are better suited to handle complex dynamic scenes, actors, and spatial relationships. Table 1 lists key papers with critical insights.
3.2 Divide And Conquer Paradigms
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 13
Fig. 7. Divide-and-conquer-timeline: We used the date these papers were published in online resources like arXiv or https://openreview.net/. Papers catalog here are Align your Latents [5], Gen-L-Video [124], Free-Bloom [37], VideoDirectorGPT [55], LLM-grounded Video Diffusion Models [52], SEINE [11], FlowZero [62], Mora [145], Vlogger [158], Vidgen [86], DreamFactory [135] and Kubrik [26]
The basic theme of divide and conquer is to generate keyframes or short clips based on single or multiple prompts and then interpolate between these frames or clips. The system often uses an anchor image as a reference frame for generating all subsequent frames. The system generates each key frame independently, allowing for parallel processing. Some challenges with the divide and conquer approach are keeping semantic consistency, smooth motion transformation between interpolated frames, and frame quality. One key theme of the Divide and Conquer approach is separating the planning and video generation stages, differentiating it from the autoregressive approach as illustrated in Fig. 8. Divide And Conquer Paradigms can be divided into three sub-paradigms: the Large language model as director, the Intermediate transition model, and the Agent-based framework. Some milestone papers with timelines are illustrated in Fig. 7
3.2.1 Large Language Model As Director .
The Large Language Model as Director [55] [125] [86],[86] [38] paradigm starts by identifying key frames that define the core narrative and then creates the intermediate frames to smoothly connect these critical frames into a cohesive, extended video. This approach differentiates between essential storyline keyframes and the additional frames required for continuity. It consists of two main components: the LLM Planner, which produces the storyline blueprint, and the Video Generator Backbone, which follows the blueprint provided by the LLM to generate the video as illustrated in Fig. 9. Initially, the model develops a script detailing each frame and optional metadata, such as entities, layouts, and actions. The model can then create images and interpolate between them using the semantic descriptions and entities from earlier frames to guide the denoising process. LLMs are crucial in this approach, as they generate detailed storylines from text prompts, specifying scenes, characters, and actions [55]. This paradigm works with both training-based and training-free (zero-shot) approaches. Free-Bloom [37] took a zero-shot approach. It has a three-stage architecture starting with LLM to generate a sequence of text-based prompts that decompose the original prompt into a semantically coherent sequence of prompts using world knowledge of LLM. It proposes novel techniques like joint noise sampling, step-aware attention, and dual-path interpolation to ensure the temporal coherence of the generated frames.
Manuscript submitted to ACM


14 Faraz and Shahzad
Fig. 8. LLM as director approach uses LLM as the spatiotemporal director of the script and a separate video generation module that can understand the DSL(metadata) generated by LLM. [52]
The model implements joint noise sampling by combining unified noise applied at the video level with individual noise applied at the frame level. By making the attention mechanism aware of each specific frame in the generation process, the model dynamically adjusts its focus on different aspects of the input data, including the text prompt and previously generated frames. In dual interpolation, the contextual path interpolates the latent variables between the contextual frames to ensure temporal coherence. The denoising path interpolates latent variables by DDIM [93] denoising process conditioned on interpolated text embedding to improve semantic coherence. One of the limitations of Free Bloom is its limited use of LLM script generation capacity, as LLM can create a rich script with storylines, scene descriptions, and entities. Video Director GPT [55] is built in a zero-shot setting, similar to Free Bloom, but it provides much more detailed guidance from GPT-4 as a video planner. It develops a comprehensive story blueprint with multiple components, including layouts, bounding boxes, and entities like actors and objects. This plan is executed using Layout2Vid, based on ModelScopeT2V [125], a grounded video generation module that ensures layout and consistency control across multi-scene videos. The LLM-grounded video diffusion model [53], similar to VideoDirectorGPT, uses detailed guidance from LLM, such as frame layout and trajectory information. However, it is a training-based approach instead of a zero-shot approach. It can understand complex spatiotemporal dynamics from text and generate layouts that align closely with the prompts and the object motion patterns typically observed in the real world.
Fig. 9. VideoDirectorGPT: GPT-4 generates a blueprint for video generation, including scene and entity description. Separate module Layout2Vid generates video from this video plan [55]
Like VideoDirectorGPT, FlowZero [62] took a zero-shot (training free) approach, but LLM plays a more detailed role than Video Director GPT. It generates a detailed dynamic scene syntax (DSS), including scene descriptions, object arrangements, and background motion patterns. The DSS components direct an image diffusion model to generate
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 15
videos with smooth object movements and consistent frame transitions. The theme of using dynamic scene layout is illustrated in Fig. 8. The LLM as director approach has limitations as it is a two-stage architecture, and adding speech or integrating short clips will need modification in the pipeline. We can extend the LLM-based divide-and-conquer approach by incorporating more specialized components, such as a model for generating reference images, a module for video creation, and a plug-and-play module for adding transitional clips and speech. That is achieved using the multi-agent framework, as described in the section 3.2.2.
3.2.2 Multi stage/Agent Based Divide And Conquer Approach .
Fig. 10. Mora utilizes a multi-agent framework. The prompt selection agent enhances prompts with detailed instructions, the text-to-image image agent generates images from the input prompts, the image-to-image agent enhances the quality of the photos, the text-to-video agent generates video segments, and the video transition agent integrates these videos into a longer video [145].
An Agent-Based LLM Framework [146] is a system where multiple models function as specialized agents, each designed to perform a distinct task or interact within a defined environment. In this system, the LLM serves as the "brain," responsible for overseeing complex operations, while simpler models function as tools, executing more specific, supportive tasks. A multi-agent framework for video generation represents a multi-layered approach to text-to-video generation. It produces high-quality long videos like those generated by Sora [58] by dividing the video creation challenge into multiple systems, each specializing in some aspect of the video generation pipeline as illustrated in Fig. 10. The framework facilitates efficient and dynamic video production by coordinating these agents flexibly and iteratively. Systems like Mora [145] and Vlogger [158] illustrate how integrating and coordinating multiple specialized agents can enhance and streamline the video creation. In Mora [145] Multiple models collaborate through an agent framework for script creation, image generation, image enhancement, video production, and video patching. VLogger [158] is a comprehensive system designed to produce cohesive vlogs by integrating various modules. It begins with the LLM Director translating a user’s story into a detailed script outlining scenes and duration. The process continues with the Actor module, where the LLM Director assesses the script to identify characters and collaborates with a character designer like SD-XL [74] to generate appropriate reference images for the actors. It has a show maker module to act as a videographer and a voicer module that synchronizes the script with voice-over, utilizing a model like Bark [111] to align subtitles with the video. This setup exemplifies a modular agent-based system for high-quality video production.
Manuscript submitted to ACM


16 Faraz and Shahzad
DreamFactory [135] simulates an AI-driven film production team, with LLM-based agents assuming roles such as directors, art directors, screenwriters, and artists. These agents collaborate on script writing, storyboarding, character design, keyframe creation, and video synthesis. DreamFactory takes an approach similar to Mora. Still, it has enhanced the script creation part by using multiple LLM models as different movie actors, like a screenwriter and director, and these agents engage in iterative conversions, finalizing scripts and action plans through the Chain of Thought (COT) pattern. Kubrick [26] employs a similar collaborative approach for script generation involving LLM agents in dialogue to finalize scripts based on user prompts. It divides the video generation into phases managed by the Director, Programmer, and Reviewer agents. Utilizing iterative reasoning, Kubrick leverages the Blender engine [100] to produce realistic 3D animations. Another way the divide-and-conquer approach can be used is to view a long video as a composition of short videos, first generating these short videos and then integrating them into a long video. That will be discussed in the next section.
3.2.3 Divide and Conquer Compositional/Transition Approach .
This method uses or generates short video segments using any standard video generation technique, such as divide-and-conquer strategies or autoregressive models. It then integrates these segments by employing a transition model to create smooth transitions between them. The challenge with transition videos is to ensure coherence and visual quality and smoothly transition from one set of entities to another. SEINE [11] adopts a predictive approach for creating transitions between short videos produced by any public text-to-video model. It uses the last frame of one segment and the first frame of the next as input, applying a random masked diffusion model to handle the transitions. This approach involves jointly denoising overlapping short videos in the temporal domain. It starts by generating these short videos with a standard diffusion model, incorporating temporal overlaps based on given prompts. The model then stitches these segments together into a longer video. The process includes transforming the video into a reduced-noise domain through noise inversion and additional denoising with the GenL model. MEVG [68] ensures temporal coherence between independently generated video clips based on multiple event descriptions. It first creates a video for each event description using a public single-prompt video generation model. Subsequent clips are then generated by conditioning on the last frame of the previous clip and the provided text, maintaining continuity. MAVIN [147](Multi-Action Video INfilling model) is designed to generate transition videos that seamlessly connect two given videos, forming a cohesive integrated sequence. It learns video infilling by dividing a training video into three segments, corrupting the intermediate segment, and then learning to predict the noise of the missing segment. ENCODER-EMPOWERED GAN [140] generates long videos using an encoder-based GAN to connect short generated video clips into longer sequences of hundreds of frames. It models temporal relationships between consecutive video clips. The process involves generating short video clips with EncGAN3 and then training to enforce temporal connectivity between these clips through a recall mechanism.
The autoregressive approach ensures smooth transitions between frames by generating each frame conditioned on the previous ones, but its sequential nature makes it inherently slow for long video generation. The divide-and-conquer 3.2.1 approach can generate keyframes in parallel but suffers from challenges involving interpolation between frames with smooth transitions and higher video quality while maintaining semantic consistency. Implicit generation 3.3 approaches combine the best of both worlds by generating complete videos directly from a model conditioned on user input without the need for interpolation (divide and conquer) or extrapolation (autoregressive) between frames. A brief summary of some key papers that explore themes related to the Divide and Conquer approach is provided in Table 2.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 17
Table 2. Divide And Conquer Paradigms: Catalog Of key papers. In the category column, training-free or training-based represents 3.2.1 pattern , ’multi-stage’ represents the 3.2.2 pattern and ’integration’ represents the 3.2.3 pattern.
Model Theme Month/Year category
Align your Latents [5] Diffusion models for interpolation and upsampling. April 2023 training-based
Gen-L-Video [124] Integrate short videos into long, consistent video. June 2023 training-based
Free-Bloom [37] LLMs and LDMs [84] for consistent video generation. Sept 2023 training-free
VideoDirectorGPT [55] Consistent multi-scene videos using GPT-4 guidance. Sept 2023 training-free
LVD [52] Dynamic video scenes using LLM-guided diffusion. Sept 2023 training-free
SEINE [11] Long video with smooth transitions from short vidoes. Oct 2023 integration
Encoder GAN [140] Connects short video by temporal relationships. Oct 2023 integration
FlowZero [62] Multi-frame story and aligns spatiotemporal layouts. Nov 2023 training-free
MEVG [69] Multiple prompts, preserving visual coherence. Dec 2023 training-based
Vlogger [158] Specialized models to generate long videos in stages. Jan 2024 multi-stage
Mora [145] Collaborative models for script, image, and video. March 2024 multi-stage
Vidgen [86] LLM for story pre-processing and textual Inversion Memory Module.
April 2024 training-based
MAVIN [147] Transition videos creating a cohesive sequence. May 2024 integration
DreamFactory [135] LLM collaboration for script and movie creation. August 2024 multi-stage
Kubrick [26] Agent collaborations to generate Blender scripts. August 2024 multi-stage
3.3 Implicit Video Generation
Implicit video generation covers a family of models that generate complete video using compact latent space representation, enhanced attention mechanisms to incorporate long-term dependencies, or denoising strategies to include short-term and global noise in the generated video. What differentiates implicit video generation from divideand-conquer and the autoregressive approach is that it generates all frames simultaneously instead of sequentially or parallelly. For example, Sora [57] compresses raw video into a latent spacetime representation, extracting patches to capture visual appearance and motion dynamics over short intervals. As a diffusion transformer, Sora consists of three components: (1) a time-space compressor that maps the original video into latent space, (2) a Vision Transformer (ViT) that processes this latent representation to produce a denoised version, and (3) a CLIP-like conditioning mechanism that uses LLM-augmented instructions and visual prompts to guide video generation. This is illustrated in 11. FreeNoise [75] presents a tuning-free and time-efficient paradigm that enhances pre-trained video diffusion models’ generative capabilities while maintaining semantic consistency. GLOBER [97] is a video auto-encoder that includes a video encoder to compress videos into global features and a video decoder based on a diffusion model to reconstruct video frames from these features in a non-autoregressive manner.
Despite progress in these areas, motion consistency, semantic alignment, and parallel processing remain key obstacles to achieving scalable, high-quality long video generation. An example of this limitation can be seen in state-of-the-art video generation models like SORA [110]. For instance, SORA generates videos that are only up to one minute long, far shorter than the typical duration of entertainment or educational videos. Issues such as unrealistic
Manuscript submitted to ACM


18 Faraz and Shahzad
and incoherent motion, intermittent object appearances, and unrealistic phenomena continue to plague its output, as documented by [98]. Some key papers that explore themes related to the implicit approaches are cataloged in Table 3.
Fig. 11. Transformer-Based Diffusion Model Sora compressed video of variable length into fixed space-time latent compressed representation [109].
Table 3. Implicit Video Generation: Milestone Models Catalog
Model Theme Year
GLOBER [97] Global features to synthesize coherent video frames. Sept 2023
FreeNoise [75] Extended videos using pre-trained video diffusion models. Oct 2023
SORA [57] Compact latent and patch-based representations. Feb 2024
In addition to generation strategy, another theme in the long videos is input control mechanisms like text, bounding boxes, and images for video guidance. We will discuss that in the next section.
4 Long Video: Input Control
Input conditioning involves diffusion models, GANs, or autoencoders using signals from user text prompts, entity layouts, bounding boxes, and images to condition video generation. Although long video generation utilizes many of the same techniques for input control as image and video generation models, it also faces the additional challenge of preserving long-term dependencies. Video generation models utilize innovative strategies like use of LLM to create progressive prompts from single input prompt [37], [33], [52], [128], [55] and enhancement in generation mechanism to create semantic consistency between frames [37], [62], [33].
Poplar mechanisms for input conditioning of long videos are User Textual Prompt, User Textual Prompt With Scene Layout, and Image Input With Textual Prompt And Scene Layout, which will be discussed next.
4.1 User Textual Prompt
User textual prompts are the most common way of conditioning the video generation models. Video generation can utilize single or multiple text prompts. Dall-E [80] is one of the first transformer-based text-to-image generation models. Dall-E concatenated BPE-encoded text tokens with the image tokens and trained an autoregressive transformer to model the joint distribution over the text and image tokens. CogVideo[34] takes a similar approach and trains autoregressive transformers with text and image tokens together on joint distribution. CogVideo has conditioning limitations to a single prompt. Phenaki [121] improved this paradigm by incorporating t5x [82] pre-trained embedding and conditioned
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 19
video generation on the sequence of prompts in story fashion. During inference in the autoregressive phase, the model conditions the next frame on the previous frame and text embeddings, which can either remain the same or be based on a new prompt. Phenaki has incoherent motion and semantic transition between video segments from one prompt to another, as these sequential prompts need more semantic coherence. Different approaches attempt this challenge by generating long videos and managing multiple prompts while ensuring temporal consistency and alignment with detailed frame-level descriptions, often with the help of LLM. Free-Bloom [37] addresses this issue by using LLM to generate semantic coherent prompts combining global and local noise distributions for each frame and employing spatial-temporal self-attention to integrate current and previous features. LLM Grounded Video Diffusion [52] utilize a multi-modal approach that alternates between steps guided by domain-specific language detailing entities, backgrounds, and scene layouts and denoising steps within the video diffusion model. VideoDrafter [60] introduces modifications to cross-attention maps in image and video generation diffusion models, enabling the production of images and videos that align with both input prompts and reference images, highlighting changes made to the cross-attention mechanism. DirecT2V [33] utilized LLM like GPT4 to generate step-by-step textual prompt instructions.
Fig. 12. DirectT2V Modulated self-attention for capturing interactions between frames [33].
Fig. 12 illustrates the DirecT2V [33] architecture, which modifies the attention block of U-Net and incorporates modulated self-attention.Text-only prompts can guide long video generation but lack semantics for fine-grain control over long video generation. In addition to frame description, adding metadata like bounding boxes of entities like persons and cars and background can help generate a more accurate depiction of videos and help with fine alignment between text and video.
4.2 User Textual Prompt with Scenes layout
Multi-modal input control mechanisms can work with multi-modalities like text, images, edge maps, bounding boxes, music, etc. We can enhance text-only prompts by adding metadata about bounding boxes, entity descriptions, trajectories, and background context, as illustrated in Fig. 8. Stable Diffusion did some groundwork for multi-modal input control [84]. It conditions diffusion-based general conditioning mechanisms based on cross-attention by augmenting their underlying UNet backbone as illustarted in Fig. 13. ControlNet [150] lays out a groundbreaking approach to
Manuscript submitted to ACM


20 Faraz and Shahzad
managing multi-modal control diffusion. It fine-tunes an existing diffusion-based network by leveraging a large pretrained model as a robust foundation for learning various conditional controls. In this architecture, a trainable copy of the pre-trained model is linked to the original, fixed model through zero convolution layers, with initial weights set to zero. This design allows the trainable model to gradually incorporate new features during training while preventing the introduction of disruptive noise into the deep features of the large diffusion model. It is used in many multi-modal video generation strategies like [36], [148] [123], [151]. In [55], authors trained video generation module, Layout2Vid build of ModelScopeT2V [125] with additional metadata about layouts, entities, and background. They merged bounding boxes, text, and image embedding with MLP. They enhanced ModelScopeT2V to consume additional conditional inputs from grounding tokens from layouts, textual entity description, and images to modulate the visual latent representation. LLM Grounded VDM [52] added metadata with textual prompts termed dynamic scene layouts(DSL), but they took a training-free approach. FlowZero [62] uses LLM to understand complex spatiotemporal dynamics from text. It generates a dynamic scene index containing scene descriptions, object layouts, and background motion patterns
Fig. 13. A latent diffusion model with input conditioning generates data by applying a reverse diffusion process on latent representations, conditioning the generation of additional input information (e.g., text or images) to guide output [84].
Dynamic scene layout increases control and detail alignment between text and video but does not provide aesthetic infusion. Images can guide aesthetics if generated by a high-quality text-to-image diffusion model or provided as a reference. We will discuss a few papers incorporating images as a guidance mechanism.
4.3 Image Input With Textual Prompt And Scenes Layout
Images provide high-quality aesthetic context for video generation. image prompts provide spatial information as well as details with different granularities like the car’s color, the carpet’s texture, and the relative position and sizes of entities, which are difficult to specify in the scene description capability of LLM. We can generate or provide key entity images like actor images as a condition for the video generation pipeline. NUWA-Infinity [54] can process both
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 21
text or image as input. In VideoStudio [61], the video generation pipeline uses foreground reference images like actors or objects like the kitchen using text-to-image diffusion models. We then feed the photos and text prompts to [61], which uses the scene reference image, the action described in the event prompt, and the camera movement in the script as inputs. Microcinema [128] utilized a multi-stage pipeline where photorealistic images are constructed from user prompt using high-quality text to image model like SDXL [74], or DALL-E [80]. It first generates images from text and then takes text prompts and images for video generation. VideoBooth [41] explored video generation from reference images and user text. It projected images into text space using CLIP [76]. VideoDrafter [60] works on the concept of a two-stage control pipeline where the text encoder first generates an image, and then both reference image and text prompt are used to create a video. Table 4 captures important milestone papers with input control mechanisms.
Table 4. Input Conditioning: Milestone Papers. ’Text Prompt’ represents 4.1, ’Text, Scene Layout’ represents 4.2 and ’Image, Text, Layout’ represents 4.3
Model Theme input control Year
Free-Bloom [37] LLM prompts with cross-attention and step-blocks. Text Prompt 2023
MEVG [68] Initial latent code, cross-attention. Text Prompt 2023
SEINE [11] Transition frames using CLIP and Lavie [129]. Text Prompt 2023
GLOBER [97] CLIP encoder, cross-modal instructions with attention. Text Prompt 2023
FlowZero [62] Frame sequences using LLM with cross-frame attention. Text, Scene Layout 2023
Vidgen [86] LLM generates frames, LDM with modified attention. Text Prompt 2024
VideoDirectorGPT [55] LLM generates video plan, interpreted by U-Net. Text, Scene Layout 2024
LLM grounded VDM [52] LLM story with attention maps and bounding layouts. Text, Scene Layout 2024
VideoTetris [113] ControlNet for autoregressive video generation. Text, Scene Layout 2024
VideoDrafter [60] LLM scripts scenes with CLIP embeddings and prompts. Image, Text, Layout 2024
MAVIN [147] 3D UNET with cross-attention and CLIP embeddings. Image, Text, Layout 2024
VideoBooth [41] Video from images and prompts using latent space. Image, Text, Layout 2024
MicroCinema [128] LLM scripts multi-stage 3D Unet with cross-attention. Image, Text, Layout 2024
Sora [57] LLM prompts with optional visual input encoding. Image, Text, Layout 2024
User textual prompts, additional metadata for scene layouts, entity descriptions, and reference images provide a rich context to the video generation model and fine alignment between user intention and generated videos. We also need long video datasets with labels or captions to train the long video generation and input control mechanism, which will be discussed in the next section.
5 Existing Datasets
The existing datasets for long video generation can be categorized as classification datasets 5.1 and captions datasets 5.2
5.1 Classification Datasets
Manuscript submitted to ACM


22 Faraz and Shahzad
Classification datasets contain videos labeled with different categories, such as actions, objects, scenes, or events. UCF-101 [95] is one of the earlier popular long video classification datasets with single class labels. It includes 101 action classes with 13,320 clips from 2,500 distinct videos divided into five categories: Human-Object Interaction, Body Motion Only, Human-Human Interaction, Playing Musical Instruments, and Sports. UCF101 has limitations due to the scale and number of classes. The Kinetic dataset [9] developed by DeepMind Google improves the scale of video datasets to 306,245 videos. DeepMind released four versions of the Kinetics dataset: Kinetics-400, Kinetics-600, Kinetics-700, and Kinetics-700-2020, with the numbers representing the number of classes. Kinetics-700-2020 was the latest version, released in 2020. The Kinetics dataset contains 306,245 distinct videos, a vast improvement compared to 2500 videos from the UCF101 dataset, and is also a single class per video. Youtube-8M [1] increased the scale of videos further by curating 8 million YouTube videos with 350k+ hours of video as a multi-label dataset as compared to single label UCF101 and Kinetics. Compared to UCF and Kinetics datasets, 8M YouTube datasets contain videos on diverse topics, including gaming, food, entertainment, health, and finance. HowTo100M [66] scaled it further by 136 million video clips with multi labels per video sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. The granularity of video labeling can be enhanced by moving from class labels to sentences that describe scenes in the video captions dataset. The video captions datasets pair clips with sentences like in MSR-VTT [137]; researchers sampled four frames from each video and annotated them with human-labeled sentences. This evolution of video datasets represents a general theme from single-label limited video datasets to large-scale video datasets with caption-based annotation.
5.2 Captions Datasets
MSR-VTT (MSRVideo to Text) is one of the first datasets to feature natural language video descriptions. It has 41.2 hours of videos and 200K clip-sentence pairs, covering the most comprehensive categories in earlier days of video research. MSR-VTT is illustrated in Fig. 14. WebVid-2M [4] improved the scale of the dataset by compiling two million videos and captions generated through automatic image captioning, similar to Google’s conceptual captions [87]. InternVid [130] took a similar approach of using automatic captioning of images and enhanced the volume of videos further by creating a dataset of 234M video clips accompanied by detailed descriptions of a total of 4.1B words. A critical aspect of video generation is using spatial and temporal clues to create more detailed descriptions of scenes and events. InternVid and WebVid-2M lack high-quality captions with spatial and temporal context due to the algorithmic nature of caption generation and, in many cases, are often short(12-30 words). The latest dataset, like VideoInstruct-100K [64], addresses the quality of caption needed for video generation. VideoInstruct-100K took a hybrid approach to improve the quality of captions and used human-assisted and semi-automatic annotation techniques. Human workers took a subset of ActivityNet [27] and enriched it with detailed information about spatial and temporal aspects, object relationships, reasoning, scene descriptions, and the chronological sequence of events enriches the captions. Another recent dataset focused on improving the quality of video captions is MiraData [42]. They sampled video frames and used GPT4-V to create a "dense caption," which covers the main subject, movements, style, backgrounds, and cameras using structured tags.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 23
Fig. 14. Here are examples from the MSR-VTT dataset showcasing video clips paired with labeled sentences. Each example includes four frames representing the video clip and five human-generated sentences that describe the content [137].
With these datasets, the next challenge is measuring the quality of generated videos, which involves assessing various aspects such as the quality of individual frame images, the smoothness of frame transitions, and the alignment between the text and the generated video discussed in 6.3. Table 5 catalogs milestone dataset used for long video generation.
Table 5. Datasets For Long Videos
Dataset Size Month/Year Avg duration Category
UCF-101 [95] 13320 Dec 2012 7.21 sec classification
Kinetics-400 [9] 306,245 May 2017 10 sec classification
Kinetics-600 [8] 480,000 Aug 2018 10 sec classification
HowTo100M [66] 136 million June 2019 6.5 min classification
YouTube 8M [1] 6.1 million Sept 2016 230 sec classification
YouTube 8M Segments [1] 237k June 2019 25 sec classification
WebVid-2M [4] 2.5 million April 2021 18 sec captions
Pandas 70m [10] 70.8 million Feb 2024 8.5 sec captions
HD-VG-130M [127] 130 million May 2023 10 sec captions
InternVid [130] 234 million July 2023 39 sec captions
VidProM [126] 6.69 million Sept 2024 2.5 sec captions
Ego4D [23] 3670(hours) Oct 2021 180-300 sec captions
Manuscript submitted to ACM


24 Faraz and Shahzad
Dataset Size Year Avg duration Category
E-SyncVidStory [139] 6k May 2024 39(s) captions
LGVQ [153] 2808 July 2024 8-96 sec captions
VideoInstruct-100K [64] 100k June 2024 2-3 min captions
MiraData [42] 788k July 2024 72.1(s) captions
Vimeo25M [129] 25M Sept 2023 19.6(s) captions
Long Video generation needs extensive metrics to measure aesthetic, motion, and semantic alignment between text prompts and video. We will discuss these metrics in the next section.
6 Performance Measures
Video generation metrics can mainly be categorized into four categories, including Image Quality Metrics 6.1, Video Quality Metrics 6.2, Semantics Quality Metrics 6.3 and Composite Metrics 6.4.
6.1 Image Quality Metrics
Image quality metrics are essential for evaluating the quality of individual image frames produced by generative models. Among the various metrics, the Inception Score (IS) [85] is one of the most widely used to assess imagegenerative models. Inception Score measures the quality and diversity of generated images using a pre-trained Inception model [99]. The Inception Score combines two key factors: image quality, defined by the clarity and accuracy of object classification, and image diversity, which refers to the variety of image classes the model can generate. A high IS indicates that the model produces diverse images that are identifiable across various categories. However, there are significant limitations to IS. While it measures the classification accuracy of generated images, it does not account for their perceptual quality—the sharpness, realism, or naturalness that a human observer might judge. Additionally, because the Inception model was trained on ImageNet, it struggles to generalize to images from domains outside of ImageNet, such as medical images, artwork, or other specialized datasets. Fréchet Inception Distance (FID) [29] was proposed to overcome the limitations of IS. FID directly measures the quality of generated images by comparing their feature distributions to those of natural images. The FID metric has been widely adopted because it provides a more comprehensive evaluation of generated image quality, as it is less reliant on specific class labels and better captures the perceptual and statistical properties of the generated images. Although image metrics can measure frame quality, they are limited in measuring flow and motion transition between frames. Video metrics attempt to solve these issues.
6.2 Video Quality Metrics
To qualitatively assess the generated videos, we need metrics that measure the visual quality and temporal coherence of the generated samples. This task is more challenging than evaluating the quality of images, as in videos, both spatial and temporal dimensions must be analyzed together. More than traditional image quality metrics like FID are required to capture the dynamics and temporal relationships unique to video generation. One such metric adapted for this purpose is the Fréchet Video Distance (FVD) [116], which extends the FID distance to handle video data. FVD utilizes a 3D convolutional network to extract features from video clips, effectively capturing the temporal dependencies essential
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 25
Fig. 15. Dover score. Samples from a dataset with human labeling of aesthetics and technical aspects of images. Dover score could be computed by aggregating or averaging the human-assigned aesthetic and technical scores [133].
to video content. It allows FVD to evaluate the quality of individual frames and the consistency of the generated video’s temporal relationships (e.g., motion, transitions, and scene continuity).
However, like FID, FVD remains computationally expensive, as it involves processing large volumes of video data and requires significant computational resources. Despite its utility, FVD and FID primarily focus on measuring the distributional similarity between accurate and generated data. They do not directly assess aesthetic quality or technical flaws such as blur, noise, or flicker. It brings us to another important category of metrics designed to address these aspects. One such metric is Dover [133], which stands for Disentangled Objective Video Quality Evaluator. Dover is intended to evaluate the aesthetic and technical quality of generated videos, including issues like motion blur, image noise, camera shaking, and flicker. Dover provides a more nuanced video quality evaluation by disentangling these technical and aesthetic components.
Dover created DIVIDE-3k dataset [133], the first User-Generated Content Video Quality Assessment (UGCVQA) database. DIVIDE-3k includes over 450,000 subjective quality opinions on videos from aesthetic and technical perspectives, helping researchers understand how these factors collectively influence the overall quality score. Some samples from the dataset are illustrated in Fig. 15. Motion quality is another dimension of generated videos, in addition to aesthetics. RAFT (Recurrent All-Pairs Field Transforms) [112] captures motion patterns and temporal coherence between frames. RAFT is a method for estimating optical flow, which measures the apparent motion of objects across successive video frames. By computing the optical flow, RAFT can describe the movement and transformation of objects,
which is crucial for evaluating the temporal alignment of generated videos. While video quality metrics like FVD and Dover work well for assessing the technical quality of generated videos, they have limitations in measuring how well a generated video aligns with the user’s intentions or the semantic content outlined in the prompts. To address this gap, we must explore semantic alignment metrics and composite metrics, which combine technical quality and alignment with user-defined content. These metrics are designed to evaluate how well the generated video reflects the intentions expressed in the input prompts.
6.3 Semantics Alignment Metrics
Semantic quality metrics are designed to evaluate the alignment between generated videos and the user’s intentions, specifically assessing how well the content of a video matches the descriptions or prompts provided as input. One of the most widely used tools in this domain for image-to-text alignment is CLIP (Contrastive Language-Image Pretraining) [76]. CLIP uses contrastive learning to jointly embed text and images into a shared space, allowing it to
Manuscript submitted to ACM


26 Faraz and Shahzad
Fig. 16. The prompt dataset is designed to evaluate the model by focusing on three key quality aspects: (1) spatial quality (frame appearance), (2) temporal quality (frame coherence), and (3) text-to-video alignment (content-text correspondence) [153].
Fig. 17. GRiT locates different entities in scenes with their relations and matches with dense captions [134].
measure the semantic similarity between text descriptions and their corresponding visual representations. By calculating the distance between the embeddings of the caption and the image, CLIP measures how well the image corresponds to the description. An extension of CLIP for assessing image-text alignment is CLIPScore [28]. CLIPScore utilizes the embeddings generated by the pre-trained CLIP model to calculate the similarity between an image and a given textual prompt. This metric has become popular because it is reference-free and does not require ground truth or human-labeled data to compute its score. It offers an efficient way to assess the quality of image generation models based on how accurately the generated images align with the provided textual descriptions. While CLIP and CLIPScore are potent tools for measuring the semantic alignment of images and text, they focus primarily on the visual and textual features that can be easily represented as embeddings. These metrics are often limited by their inability to capture complex interactions between objects, actions, or attributes within an image. To address this, newer approaches like GRiT (Generative Region-to-Text Transformer) [134] have been introduced to enhance the semantic understanding in generative models by going beyond simple image-text matching. GRiT is particularly notable for its ability to focus on high-level object understanding within images, allowing it to interpret not just simple nouns (e.g., "dog," "car") but also more complex and descriptive sentences that include object attributes or actions (e.g., "a brown dog running across a field" or "a car speeding through the city"). By incorporating such fine-grained descriptions, GRiT can provide a much richer understanding of the objects and actions in a generated image, making it more capable of aligning the generated content with the semantic intent of the user.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 27
Table 6. Metrics for video quality evaluation. In the "Direction" column, ↑ represents higher score is better, while ↓ represents that a lower score is better.
Metrics Type Year Direction
Inception Score [85] frame June 2016 ↑
FID [29] frame Jan 2018 ↓
FVD [116] video March 2018 ↓
CLIPScore [28] image March 2021 ↑
FETV [59] video Nov 2023 ↑
VBench [39] video Nov 2023 ↑
T2VQA [48] frame March 2024 ↑
FVD Motion [56] video June 2024 ↓
UGVQ [153] video July 2024 ↑
T2V-CompBench [96] video July 2024 ↑
MiraBench [42] video July 2024 ↑
Cross-Scene Face/Style Consistency Score [135]
video August 2024 ↑
As illustrated in Fig. 17, GRiT uses a transformer-based architecture to learn the relationships between different regions of an image and their corresponding textual descriptions. It allows the model to break down the image into distinct regions and understand how each part corresponds to specific prompt components. In conclusion, while CLIP and CLIPScore provide practical ways to measure the similarity between images and text, GRiT offers a significant advancement by enabling a deeper semantic understanding of the content within images. By considering not only individual objects but also their relationships, attributes, and actions, GRiT enhances the ability to evaluate generated content on a much more complex and nuanced level. These advances in semantic quality metrics are crucial for improving the alignment of generated videos with user intentions and ensuring that the videos are visually accurate and semantically meaningful.
These semantic alignment metrics have limitations in the video domain because the video may contain hundreds of frames, and they must match caption boundaries with corresponding frames. Composite metrics, aggregates of many individual algorithms, and manual scoring address this limitation.
6.4 Composite Metrics
Composite metrics are aggregates of different individual metrics and use a suite of text prompts in various categories like animals, plants, objects, people, etc, as well as other aspects like spatial, temporal, motion, and alignment. They use algorithms and manual labeling efforts to score text-to-video models on these metrics. FETV Bench [59] is one of the first metrics-based compositions of prompts. FETV is a multi-aspect prompt creation system that categorizes prompts based on three orthogonal aspects: content, attributes, and prompt complexity. The context and control are further divided into spatial and temporal as illustrated in Fig. 18. They have done a manual evaluation on four models and also used automatic metrics like CLIPScore [28], BLIPScore [63], and OtterVQA. UGVQ [153] (Unified Generated Video Manuscript submitted to ACM


28 Faraz and Shahzad
Fig. 18. FETV is multi-faceted, classifying prompts into three distinct aspects: the main content, controllable attributes, and prompt complexity [59].
Quality) assesses generated videos’ spatial, motion, and semantic fidelity through a comprehensive, unified framework. It constructed a Large-scale Generated Video Quality assessment LGVQ [153] dataset to evaluate the three most critical quality dimensions: spatial quality, temporal quality, and text-to-video alignment. They partition the text prompts into foreground, background, and motion. These categories have further divided these aspects into cross-cutting themes like animals, peoples, objects, and plants. They selected six text-to-video models as illustrated in Fig. 16. They used manual labeling to score these models on these aspects.
T2V-CompBench [96] takes a similar approach as FETV Bench, and evaluates videos’ quality and semantic fidelity based on various aspects of prompt composition, including attributes, objects, spatial relationships, actions, motion binding, and numeric accuracy. The benchmark includes a dataset of 700 prompts across seven categories. It uses manual human feedback and automated metrics such as CLIPScore [28] and BLIP-VQA [51] for evaluation but has concluded that these automatic metrics do not correlate poorly with human metrics. VBench [39] is also a suite of prompts to evaluate video quality. It used human feedback and automatic models like Dino [7], CLIP [76], RAFT [112] for video quality. Text alignment uses MUSIQ [45] and GRiT [134]. MiraBench [42] also uses a similar approach. It has a prompt dataset of 150 prompts examining six perspectives, including temporal consistency, temporal motion strength, 3D consistency, visual quality, text-video alignment, and distribution consistency. These composite benchmarks have the limitation that they need manual labor to evaluate. One challenge for research is to create models that can algorithmically evaluate these metrics, i.e., developing reliable, automated metrics for long video generation remains crucial to accurately evaluate temporal consistency, semantic fidelity, and visual quality and is one of the active research areas. Table 6 summarizes the important metrics for video evaluation.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 29
7 Conclusion & Future Trends
This survey equips users with a broad overview of the history, recent progress, and ongoing challenges in long video generation, focusing on video generation strategies, datasets, metrics, and open research areas. Long video generation is one of the actual north goals of generative AI, aiming to produce coherent and realistic videos over extended durations. Some of the challenges to be addressed by long video generation are maintaining temporal coherence and visual consistency while ensuring that the generated video aligns with a narrative or specific user intentions. Several strategies have been explored to tackle this challenge, including divide-and-conquer autoregressive models and intrinsic methods. Despite progress in these areas, motion consistency, semantic alignment, and parallel processing remain key obstacles to achieving scalable, high-quality long video generation. Future research in long video generation can focus on enhanced autoregressive models, novel frame and video segment merging techniques, and enhanced training paradigms.
One of the significant open research areas in long video generation is the generation of longer videos that accurately reflect spatial, temporal, and physical dynamics. A key challenge is the need for large-scale video datasets with comprehensive spatial, temporal, and physical context (e.g., trajectories, shadows, interactions). Existing largescale datasets like HD-VG-130M [127] offer scale but have limitations in terms of caption quality (e.g., captions are restricted to 15-20 words and lack rich spatial and temporal contextual information). On the other hand, datasets such as VideoInstruct-100K [64] provide rich spatial and temporal context but fall short in scale. Developing datasets that balance both scale and rich context is critical for advancing long video generation research. In addition to datasets, measuring generated video quality presents another challenge. Current state-of-the-art metrics, such as FETV [59], MiraBench [42], and VBench [39], rely on manual human feedback to assess video quality, which is time-consuming, subjective, and challenging to scale. Future research should focus on developing fully automated metrics that can evaluate the quality of generated videos in a more scalable and objective manner.
Another open area of research in long video generation is the integration of audio. Currently, most commercial video generation models, such as SORA and Stability AI, do not produce accompanying audio. Developing methods to generate audio that aligns seamlessly with visual content is crucial for creating immersive and comprehensive videos, making this an important focus in the field of long video generation.
Long video generation promises to revolutionize multiple fields, including entertainment, education, virtual reality, and game development. However, it also introduces significant challenges, such as the potential for fake video creation, bias, violence, and moral concerns. Additionally, issues like hallucinations can limit the applicability of generative videos, particularly in domains like education and science. In conclusion, this survey offers readers an in-depth overview of the state of the art in long video generation, highlighting key research areas and opportunities for future exploration.
References
[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark, 2016. [2] Anonymous. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. under review.
[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. ViViT: A Video Vision Transformer . In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6816–6826, Los Alamitos, CA, USA, October 2021. IEEE Computer Society. [4] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval . In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1708–1718, Los Alamitos, CA, USA, October 2021. IEEE Computer Society. [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: Highresolution video synthesis with latent diffusion models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22563–22575, 2023.
Manuscript submitted to ACM


30 Faraz and Shahzad
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in selfsupervised vision transformers, 2021. [8] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600, 2018. [9] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
[10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-Wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13320–13331, Los Alamitos, CA, USA, June 2024. IEEE Computer Society. [11] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. SEINE: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2024. [12] Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 1486–1494, Cambridge, MA, USA, 2015. MIT Press. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. [14] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822–19835, 2021. [15] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: mastering text-to-image generation via transformers. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS ’21, Red Hook, NY, USA, 2024. Curran Associates Inc. [16] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 16890–16902. Curran Associates, Inc., 2022. [17] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: faster and better text-to-image generation via hierarchical transformers. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc. [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.
[19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. [20] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming Transformers for High-Resolution Image Synthesis . In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12868–12878, Los Alamitos, CA, USA, June 2021. IEEE Computer Society. [21] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. Vid-gpt: Introducing gpt-style autoregressive generation in video diffusion models, 2024. [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 63(11):139–144, October 2020. [23] Kristen Grauman, Andrew Westbury, Eugene Byrne, Vincent Cartillier, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Devansh Kukreja, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4D: Around the World in 3,000 Hours of Egocentric Video . IEEE Transactions on Pattern Analysis & Machine Intelligence, (01):1–32, July 5555.
[24] Shir Gur, Sagie Benaim, and Lior Wolf. Hierarchical patch vae-gan: Generating diverse videos from a single sample, 2020.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 31
[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15979–15988, 2022.
[26] Liu He, Yizhi Song, Hejun Huang, Daniel Aliaga, and Xin Zhou. Kubrick: Multimodal agent collaborations for synthetic video generation, 2024. [27] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 961–970, 2015.
[28] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning, 2022. [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6629–6640, Red Hook, NY, USA, 2017. Curran Associates Inc. [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. [31] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. [32] Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Multi-generator generative adversarial nets, 2017. [33] Susung Hong, Junyoung Seo, Heeseong Shin, Sunghwan Hong, and Seungryong Kim. Large language models are frame-level directors for zero-shot text-to-video generation. In First Workshop on Controllable Video Generation @ICML24, 2024.
[34] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2023.
[35] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2023.
[36] Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided video-to-video translation framework by using diffusion model with controlnet, 2023. [37] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free-bloom: Zero-shot text-to-video generator with LLM director and LDM animator. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[38] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free-bloom: zero-shot text-to-video generator with llm director and ldm animator. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, Red Hook, NY, USA, 2024. Curran Associates Inc. [39] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21807–21818, 2024.
[40] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967–5976, 2017.
[41] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6689–6700, 2024.
[42] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: A large-scale video dataset with long durations and structured captions. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.
[43] Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks . IEEE Transactions on Pattern Analysis & Machine Intelligence, 43(12):4217–4228, December 2021.
[44] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and Improving the Image Quality of StyleGAN . In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8107–8116, Los Alamitos, CA, USA, June 2020. IEEE Computer Society. [45] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. MUSIQ: Multi-scale Image Quality Transformer . In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5128–5137, Los Alamitos, CA, USA, October 2021. IEEE Computer Society. [46] Sungwoong Kim, Daejin Jo, Donghoon Lee, and Jongmin Kim. MAGVLT: Masked Generative Vision-and-Language Transformer . In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23338–23348, Los Alamitos, CA, USA, June 2023. IEEE Computer Society. [47] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022. [48] Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, and Ning Liu. Subjective-aligned dataset and metric for text-to-video quality assessment, 2024. [49] Taegyeong Lee, Soyeong Kwon, and Taehwan Kim. Grid diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8734–8743, 2024.
[50] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. A survey on long video generation: Challenges, methods, and prospects, 2024. [51] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. [52] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. LLM-grounded video diffusion models. In The Twelfth International Conference on Learning Representations, 2024.
Manuscript submitted to ACM


32 Faraz and Shahzad
[53] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. LLM-grounded video diffusion models. In The Twelfth International Conference on Learning Representations, 2024.
[54] Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. NUWA-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[55] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. VideodirectorGPT: Consistent multi-scene video generation via LLM-guided planning, 2024. [56] Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, and Renjie Liao. Fréchet video motion distance: A metric for evaluating motion consistency in videos. In First Workshop on Controllable Video Generation @ICML24, 2024.
[57] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background, technology, limitations, and opportunities of large vision models, 2024. [58] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background, technology, limitations, and opportunities of large vision models, 2024. [59] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: a benchmark for fine-grained evaluation of open-domain text-to-video generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, Red Hook, NY, USA, 2024. Curran Associates Inc. [60] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos, 2024. [61] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos, 2024. [62] Yu Lu, Linchao Zhu, Hehe Fan, and Yi Yang. Flowzero: Zero-shot text-to-video synthesis with llm-driven dynamic scene syntax, 2023. [63] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, 2023. [64] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models, 2024. [65] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error, 2016. [66] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2630–2640, 2019. [67] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. [68] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, and Sangpil Kim. Mevg: Multi-event video generation with text-to-video models. In Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XLIII, page 401–418, Berlin, Heidelberg, 2024. Springer-Verlag. [69] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, and Sangpil Kim. Mevg: Multi-event video generation with text-to-video models, 2024. [70] Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, and Bo zhao. Flexifilm: Long video generation with flexible conditions, 2024. [71] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions, 2018. [72] William Peebles and Saining Xie. Scalable diffusion models with transformers. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4172–4182, 2023. [73] Gensheng Pei, Tao Chen, Xiruo Jiang, Huafeng Liu, Zeren Sun, and Yazhou Yao. VideoMAC: Video Masked Autoencoders Meet ConvNets . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22733–22743, Los Alamitos, CA, USA, June 2024. IEEE Computer Society. [74] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. [75] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. In The Twelfth International Conference on Learning Representations, 2024.
[76] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [77] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks, 2016. [78] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), January 2020. [79] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. [80] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Clip: Connecting vision and language with contrastive learning. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 8821–8831. PMLR, 2021. [81] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 33
Proceedings of Machine Learning Research, pages 8821–8831. PMLR, 18–24 Jul 2021.
[82] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio, 2022. [83] Jason Tyler Rolfe. Discrete variational autoencoders, 2017. [84] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models . In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674–10685, Los Alamitos, CA, USA, June 2022. IEEE Computer Society. [85] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page 2234–2242, Red Hook, NY, USA, 2016. Curran Associates Inc. [86] Ram Selvaraj, Ayush Singh, Shafiudeen Kameel, Rahul Samal, and Pooja Agarwal. Vidgen: Long-form text-to-video generation with temporal, narrative and visual consistency for high quality story-visualisation tasks. In 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), pages 1–8, 2024. [87] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. [88] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023.
[89] Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions, 2020. [90] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3616–3626, 2022.
[91] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3626–3636, June 2022.
[92] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. [93] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [94] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution, 2020. [95] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild, 2012. [96] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: A comprehensive benchmark for compositional text-to-video generation, 2024. [97] Mingzhen Sun, Weining Wang, Zihan Qin, Jiahui Sun, Sihan Chen, and Jing Liu. GLOBER: Coherent non-autoregressive video generation via GLOBal guided video decodER. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[98] Rui Sun, Yumin Zhang, Tejal Shah, Jiahao Sun, Shuoying Zhang, Wenqi Li, Haoran Duan, Bo Wei, and Rajiv Ranjan. From sora what we can see: A survey of text-to-video generation, 2024. [99] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, 2016.
[100] Blender Team. Blender is the free and open source 3d creation suite. it supports the entirety of the 3d pipeline-modeling, rigging, animation, simulation, rendering, compositing, motion tracking and video editing., 2002. [101] Claude A.I Team. Anthropic by claude, 2023. [102] Google A.I Team. Google gemini series, 2023. [103] Meta A.I Team. Meta llama models, 2023. [104] Midjourney Team. Gen2 by runway ml, 2023. [105] Midjourney Team. Gen-3 alpha by midjourney, 2024. [106] Midjourney A.I Team. The midjourney v5.2 model for image generation, 2024. [107] Mistral A.I Team. Mistral large model by mistral, 2024. [108] Oepn A.I Team. Introducing chatgpt by open a.i, 2022. [109] Open A.I Team. Video generation models as world simulators. [110] Sora Team. Video generation models as world simulators by open a.i, 2024. [111] Suno Team. Bark by suno a.i, 2023.
Manuscript submitted to ACM


34 Faraz and Shahzad
[112] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II, page 402–419, Berlin, Heidelberg, 2020. Springer-Verlag. [113] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di ZHANG, and Bin CUI. Videotetris: Towards compositional text-to-video generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [114] Standford tutorial. Auto-encoders. [115] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. FVD: A new metric for video generation, 2019. [116] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. FVD: A new metric for video generation, 2019. [117] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. [118] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc. [119] Yuri Viazovetskyi, Vladimir Ivashkin, and Evgeny Kashin. Stylegan2 distillation for feed-forward image manipulation. In Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII, page 170–186, Berlin, Heidelberg, 2020. Springer-Verlag. [120] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2023.
[121] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2023.
[122] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics, 2016. [123] Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang. Easycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation, 2024. [124] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising, 2023. [125] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. [126] Wenhao Wang and Yi Yang. Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusion models. In Proceedings of the 2024 NeurIPS Conference. NeurIPS, 2024. Poster number: 97505. [127] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation, 2024. [128] Yanhui Wang, Jianmin Bao, Wenming Weng, Ruoyu Feng, Dacheng Yin, Tao Yang, Jingxu Zhang, Qi Dai, Zhiyuan Zhao, Chunyu Wang, Kai Qiu, Yuhui Yuan, Xiaoyan Sun, Chong Luo, and Baining Guo. Microcinema: A divide-and-conquer approach for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8414–8424, June 2024.
[129] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video generation with cascaded latent diffusion models, 2024. [130] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2024.
[131] Faraz Waseem, Rafael Perez Martinez, and Chris Wu. Visual anomaly detection in video by variational autoencoder, 2022. [132] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, Chong Luo, Yueyi Zhang, and Zhiwei Xiong. ART•V: Auto-Regressive Text-to-Video Generation with Diffusion Models . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 7395–7405, Los Alamitos, CA, USA, June 2024. IEEE Computer Society. [133] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives . In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 20087–20097, Los Alamitos, CA, USA, October 2023. IEEE Computer Society. [134] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding, 2022. [135] Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, and Saad Ezzini. Dreamfactory: Pioneering multi-scene long video generation with a multi-agent framework, 2024. [136] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2364–2373, 2018.
[137] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5288–5296, 2016.
Manuscript submitted to ACM


Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation 35
[138] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1316–1324, 2018. [139] Dingyi Yang, Chunru Zhan, Ziheng Wang, Biao Wang, Tiezheng Ge, Bo Zheng, and Qin Jin. Synchronized video storytelling: Generating video narrations with structured storyline, 2024. [140] Jingbo Yang and Adrian G. Bors. Enabling the encoder-empowered gan-based video generators for long video generation. In 2023 IEEE International Conference on Image Processing (ICIP), pages 1425–1429, 2023.
[141] Jaehoon Yoo, Semin Kim, Doyup Lee, Chiheon Kim, and Seunghoon Hong. Towards end-to-end generative modeling of long videos with memory-efficient bidirectional transformers. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22888–22897, 2023. [142] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10459–10469, June 2023. [143] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18456–18466, 2023.
[144] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2022.
[145] Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun. Mora: Enabling generalist video generation via a multi-agent framework, 2024. [146] Zhengqing Yuan, Yixin Liu, Yihan Cao, Weixiang Sun, Haolong Jia, Ruoxi Chen, Zhaoxu Li, Bin Lin, Li Yuan, Lifang He, Chi Wang, Yanfang Ye, and Lichao Sun. Mora: Enabling generalist video generation via a multi-agent framework, 2024. [147] Bowen Zhang, Xiaofei Xie, Haotian Lu, Na Ma, Tianlin Li, and Qing Guo. Mavin: Multi-action video generation with diffusion models via transition video infilling, 2024. [148] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions, 2024. [149] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 5908–5916, 2017. [150] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [151] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation, 2023. [152] Yunzhi Zhang, Wilson Yan, Pieter Abbeel, and Aravind Srinivas. Videogen: Generative modeling of videos using {vq}-{vae} and transformers, 2021. [153] Zhichao Zhang, Xinyue Li, Wei Sun, Jun Jia, Xiongkuo Min, Zicheng Zhang, Chunyi Li, Zijian Chen, Puyi Wang, Zhongpeng Ji, Fengyu Sun, Shangling Jui, and Guangtao Zhai. Benchmarking aigc video quality assessment: A dataset and unified model, 2024. [154] Pengyuan Zhou, Lin Wang, Zhi Liu, Yanbin Hao, Pan Hui, Sasu Tarkoma, and Jussi Kangasharju. A survey on generative ai and llm for video generation, understanding, and streaming, 2024. [155] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[156] Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, and Jiang Bian. Compositional 3d-aware video generation with llm director. In Proceedings of the 38th Annual Conference on Neural Information Processing Systems (NeurIPS 2024), 2024. Poster presentation.
[157] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2242–2251, 2017.
[158] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream a vlog. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8806–8817, 2024.
Manuscript submitted to ACM