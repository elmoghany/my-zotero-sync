Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising
Fu-Yun Wang4,1,2 Wenshuo Chen5 Guanglu Song3 Han-Jia Ye4 Yu Liu1† Hongsheng Li2† 1Shanghai AI Laboratory 2Multimedia Laboratory, The Chinese University of Hong Kong 3Sensetime Research 4Nanjing University 5Tsinghua University
{wangfuyun@smail, yehj@lamda}.nju.edu.cn cws21@mails.tsinghua.edu.cn songguanglu@sensetime.com liuyuisanai@gmail.com hsli@ee.cuhk.edu.hk
Abstract
Leveraging large-scale image-text datasets and advancements in diffusion models, text-driven generative models have made remarkable strides in the field of image generation and editing. This study explores the potential of extending the textdriven ability to the generation and editing of multi-text conditioned long videos. Current methodologies for video generation and editing, while innovative, are often confined to extremely short videos (typically less than 24 frames) and are limited to a single text condition. These constraints significantly limit their applications given that real-world videos usually consist of multiple segments, each bearing different semantic information. To address this challenge, we introduce a novel paradigm dubbed as Gen-L-Video capable of extending off-the-shelf short video diffusion models for generating and editing videos comprising hundreds of frames with diverse semantic segments without introducing additional training, all while preserving content consistency. We have implemented three mainstream text-driven video generation and editing methodologies and extended them to accommodate longer videos imbued with a variety of semantic segments with our proposed paradigm. Our experimental outcomes reveal that our approach significantly broadens the generative and editing capabilities of video diffusion models, offering new possibilities for future research and applications. Code is available at: https: //github.com/G-U-N/Gen-L-Video.
1 Introduction
Benefitting from pre-training on large-scale text-image datasets [50] and the development and refinement of the diffusion model [40, 46, 5, 52, 49], we have witnessed a plethora of successful applications, including impressive image generation, editing, and even fine-grained generation control through the injection of additional layout information [39, 71]. A logical progression of this approach is its extension to the video realm for text-driven video generation and editing [52, 43, 21, 63].
Currently, there are three primary strategies for text-driven video generation and editing:
• Pretrained Text-to-Video (pretrained t2v) [57, 62, 22] involves training the diffusion model on a large-scale text-video paired dataset such as WebVid-10M [2]. Typically, a temporal interaction module, like Temporal Attention [21], is added to the denoising model, fostering inter-frame information interaction to ensure frame consistency.
• Tuning-free Text-to-Video (tuning-free t2v) [43, 29, 3] utilizes the pre-trained Text-toImage model to generate and edit video frame-by-frame, while applying additional controls to maintain consistency across frames (for instance, copying and modifying the attention map [16, 43, 29], sparse causal attention [63], etc.).
† Correspondence to: Hongsheng Li (hsli@ee.cuhk.edu.hk), Yu Liu (liuyuisanai@gmail.com)
Preprint. Under review.
arXiv:2305.18264v1 [cs.CV] 29 May 2023


• One-shot tuning Text-to-Video (one-shot-tuning t2v) [63, 41] proposes to fine-tune the pretrained text-to-image generation model on a single video instance to generate videos with similar motions or contents. Despite the extra training cost, one-shot tuning-based methods often offer more editing flexibility compared to tuning-free based methods. As depicted in Fig. 4, both attempt to substitute the rabbit in the source video with a tiger or a puppy. The outcome produced by the tuning-free t2v method reveals elongated ears, losing authenticity. One-shot-tuning-based method, in contrast, effectively circumvents this problem.
Despite significant advances made by previous methods, they are accompanied by some fatal limitations, restricting their practical applications. First of all, the number of video frames generated by these methods is usually limited, generally less than 24 frames [43, 63, 29, 25, 32]. On one hand, the computational complexity of temporal attention scales quadratically with the number of frames, making the direct generation of ultra-long videos infeasible. On the other hand, ensuring consistency becomes more challenging with the increase in the number of frames. Another noteworthy limitation is that these methods typically generate videos controlled by a single text condition and cannot accommodate multiple text prompts. In reality, the content of a video often changes over time, meaning that a comprehensive video often comprises multiple segments each bearing different semantic information. This necessitates the development of video generation methods that can handle multiple text conditions. Though there are already attempts at generating long videos, they typically require additional training on large-scale text-video datasets and follow the autoregressive mechanism (i.e.,the generation of later frames is conditioned on former ones), which suffers from severe content degradation and inference inefficiency (see Sec. 2 for more discussion).
In light of these challenges, we propose a novel framework aimed at generating long videos with consistent, coherent content across multiple semantic segments. Unlike previous methods, we do not construct or train a long-video generator directly. Instead, we view a video as a collection of short video clips, each possessing independent semantic information. Hence, a natural idea is that generation of long videos can be seen as the direct splicing of multiple short videos. However, this simplistic division falls short of generating long videos with consistent content, resulting in noticeable content and detail discrepancies between different video clips. As shown in the third row of Fig. 2, the color of the jeep car changes drastically among different video clips when they are denoised isolatedly. To counter this, we perceive long videos as short video clips with temporal overlapping. We demonstrate that under certain conditions, the denoising path of a long video can be approximated by joint denoising of overlapping short videos in the temporal domain. In particular, as depicted in Fig. 1, the noisy long video is initially mapped into multiple noisy short video clips via a designated function. Subsequently, existing off-the-shelf short video diffusion models can be employed to denoise these video clips under the guidance of various text conditions. These denoised short videos are then merged and inverted back to a less noisy original long video. Essentially, this procedure establishes an abstract long video generator and editor without necessitating any additional training, enabling the generation and editing of videos of any length using established short video generation and editing methodologies.
Our method was tested in three scenarios: pretrained t2v, tuning-free t2v, and one-shot-tuning t2v, all of which yielded favorable results. Furthermore, the incorporation of additional control information and advanced open-set detection [30] and segmentation [26] technologies allows for more impressive results, such as precise layout control and arbitrary object video inpainting. Extensive experimental results validate the broad applicability and effectiveness of our proposed Gen-L-Video.
2 Related Work
As we mentioned, the current mainstream strategies for video generation and editing can be mainly categorized into three types: pretrained Text-to-Video (pretrained t2v), tuning-free Text-to-Video (tuningfree t2v), and one-shot-tuning Text-to-Video (one-shot-tuning t2v). The breakthroughs in video generation and editing techniques have largely drawn from the existing technologies for image editing and generation. In this section, we first introduce the development of text-to-image technology and then provide a brief overview of the key accomplishments of each of the three strategies. In the end, we discuss recent advances in long video generations and the advantage of Gen-L-Video over them.
Text-to-Image generation. Many early works [74, 70, 65, 64, 55] train GANs [10] on image captioning datasets to produce text-conditional image samples. Other works [45, 68, 67, 5, 7] apply
2


Short Video Diffusion Model
Long Video Diffusion Model
DDIM inversion using Long VDM Denoising using Long VDM
“An astronaut is walking on the moon”
vt0 = F0(vt)
vt0−1
vt1−1
c0
c1
vt−1 = argmin
v
෍ i=0
i=1 ∥ Wi ⊗ Fi v − vit−1 ∥2
2
vt1 = F1(vt)
Figure 1: Left: High-level overview of Temporal Co-Denoising. Our framework treats long videos of arbitrary lengths and multiple semantic segments as a collection of short videos with temporal overlapping. It allows us to effectively apply short video diffusion models to approximate the denoising path of these extended videos, ensuring consistency and coherence throughout. Right: The pipeline of our framework for video editing. With the extended long video diffusion model, we first invert the given long video into an approximated initial noise through DDIM inversion. Then, we sample a new video guided by the given prompts (either single or multiple).
vector quantization [57] and then adopt autoregressive transformers to predict image tokens followed by text tokens. Recently, several works [40, 49, 11, 47] adopt diffusion [19] models for Text-toImage Generation. GLIDE [40] introduces classifier-free guidance [18] in the diffusion model to enhance image quality, while DALLE-2 [46] improves text-image alignment using the CLIP [44] feature space. Imagen [49] employs cascaded diffusion models for high-definition video generation. VQ-diffusion [11] and Latent Diffusion Model (LDM, also known as Stable Diffusion) [47] train diffusion models in an autoencoder’s latent space to boost efficiency. Variants of LDM are fine-tuned to achieve more functionality like inpainting, image variants, etc. ControlNet [71], T2I-adapter [39] add new modules to accept additional image inputs, achieving precise generative layout control. Many fine-tuning strategies [48, 23, 8] are also developed to force diffusion models to generate new concepts and styles, which shares a similar idea to continual learning [72, 60, 53].
Pretrained Text-to-Video. Despite significant advancements in Text-to-Image generation, generating videos from text remains a challenge due to the scarcity of high-quality, large-scale textvideo datasets and the inherent complexity of modeling temporal consistency and coherence. Early works [36, 42, 34, 27, 12, 31] primarily focus on generating videos in simple domains, such as moving digits or specific human actions. GODIVA [57] is the first model to utilize VQ-VAE and sparse attention for Text-to-Video generation, enabling more realistic scenes. NÜWA [62] builds upon GODIVA with a unified representation for various generation tasks via multitask learning. CogVideo [22] incorporates additional temporal attention modules on top of the pre-trained Text-toImage model [5]. Similarly, Video Diffusion Models (VDM) [21] use a space-time factorized U-Net with joint image and video data training. Imagen Video [20] improves VDM with cascaded diffusion models and v-prediction parameterization for high-definition video generation. Make-A-Video [52], MagicVideo [73], and LVDM [15] share similar motivations, aiming to transfer progress from t2i to t2v generation. Video Fusion [32] decomposes the denoising process by resolving per-frame noise into base noise and residual noise to reflect the connections among frames. For video editing, Dreamix [38] and Gen-1 [6] utilize the video diffusion model for video editing.
Tuning-free Text-to-Video. It’s nontrivial to directly apply pretrained Text-to-Image model for video generation or editing without tuning. Recent diffusion-based image editing models [35, 16, 4, 61, 24, 56], although powerful in processing individual frames in a video, results in inconsistencies between frames due to the models’ lack of temporal awareness. Tune-A-Video [63] finds that extending spatial self-attention to sparse causal attention with pretrained weight produces consistent content across frames. Fate-Zero [43] and Video-P2P [29] apply sparse causal attention and attention control proposed in prompt2prompt, achieving consistent video editing. Pix2Video [3] adds additional regularization to penalize the dramatic frame changes. Text2Video-Zero [25] first proposes to generate videos in zero-shot settings with only pretrained text-to-image diffusion model. It applies the sparse causal attention and object mask to preserve the content consistency among frames and add motion dynamics in different scales to enrich the base latent code to generate consecutive motions.
One-shot tuning Text-to-Video. Single-video GANs [1, 13] generate new videos with similar appearance and dynamics to the input video, while they suffer from extensive computational burden.
3


Isolated Gen-L-Video sparse casual Gen-L-Video
“A jeep car is moving on the beach”
Figure 2: Comparison between our method Gen-L-Video and isolated denoising with short video diffusion models. The long video consist of two short video clips (each with 20 frames), and we sample one in five for better visualization. Gen-L-Video achieves the most smooth and consistent generation. Gen-L-Video sparse causal means we replace the bi-directional cross-frame attention with the sparse causal attention proposed in [63], which typically leads to the first few frames inconsistent with the later ones, and we analyze the subtle reason in Sec. 4.
SinFusion [41] adapts diffusion models to single-video tasks and enables autoregressive video generation with improved motion generalization capabilities. Tune-A-Video [63] proposes to finetune the pretrained text-to-image diffusion model on a video to enable generation of videos with similar motions, demonstrating powerful editing ability.
Long video generation. The generation of long videos has garnered significant attention in recent years, resulting in various attempts to address the challenges associated with this task [59, 69, 14, 9, 28]. Existing approaches typically rely on autoregressive models, such as NUWA-Infinity [28], Phenaki [58], and TATS [9], or diffusion models, including MCVD [59], FDM [14], PVDM [69], and LVDM [15]. All these methods employ an autoregressive mechanism for long video generation, wherein the generated frames serve as conditioning for subsequent frames. However, this mechanism often leads to significant content degradation after several extrapolations due to error accumulation. Furthermore, the autoregressive mechanism constrains generation to a sequential process, substantially reducing efficiency. Recently, NUWA-XL [66] proposed a novel hierarchical diffusion process that enables parallel long video generation. Despite its advantages, this approach necessitates extensive pretraining on large long video datasets and requires a well-designed global diffusion model to generate key frames at the outset. Our framework, instead of directly training or constructing a long video diffusion model, can approximate the arbitrary length long video denoising path with parallel joint denoising of off-the-shelf short video generation or editing models. In general, Gen-L-Video presents an efficient, convenient, and scalable paradigm for long video generation, addressing the limitations of existing methods and offering new possibilities for future research and applications. We make a direct comparison of our method with existing methods in Table 2, and our approach demonstrates significant superiority.
3 Method
3.1 Preliminaries
Diffusion models [19] perturb the data by gradually injecting noise to data x0 ∼ q(x0), which is formalized by a Markov chain:
q(x1:T |x0) =
T
Y
t=1
q(xt|xt−1), q(xt|xt−1) = N (xt|√αtxt−1, βtI),
where βt is the noise schedule and αt = 1 − βt. The data can be generated by reversing this process, i.e.,we gradually denoise to restore the original data. The diffusion model pθ(xt−1|xt) parameterized by θ is trained to approximate the reverse transition q(xt−1|xt, x0), which is formulated as
q(xt−1|xt, x0) = N (xt−1; μ ̃t(xt, x0), β ̃tI), μ ̃t (xt, x0) = √1αt
xt − 1 − αt
√1 − α ̄t
√αt
ε,
4


where α ̄t = Qt
s=1 αs, β ̃t = 1−α ̄t−1
1−α ̄t βt, and ε is the noise injected to x0 to obtain xt. Therefore, the learning of pθ(xt−1|xt) is equivalent to the learning a noise prediction network εθ(xt, t):
mθin Et,x0,ε∥ε − εθ(xt, t)∥2
2,
where t is uniformly sampled from {1, 2, . . . , T } and xt = √αtx0 + √1 − αtε.
DDIM [54] generalize the framework of DDPM and propose a deterministic ODE process, achieving faster sampling speed. The inversion trick of DDIM [37], based on the assumption that the ODE process can be reversed in the limit of small steps, can be used to approximate the corresponding noise of the given instance:
xt+∆t
√αt+∆t
= xt
√αt
+
s
1 − αt+∆t
αt+∆t
−
r 1 − αt
αt
!
εθ (xt, t) .
Latent Diffusion Model (LDM) [47] is a variant of text-to-image diffusion models. An autoencoder is first trained on large image datasets, where the encoder E compresses the original image x0 into a latent code zx0 , and the decoder D reconstructs the original image from the latent code. That is
zx0 = E (x0), x0 ≈ D(zx0 ).
Then a conditional DDPM pθ(vt−1|vt, c) is trained to gradually remove noise for data sampling. Classifier-free guidance (GFC) [17] is proposed to improve the text-image alignment by linearly combining the conditional predicted noise and the unconditional one.
εˆθ(xt, t, c) = (1 + w)εθ(xt, t, c) − wεθ(xt, t, ∅),
where w > 0 is the guidance scale. Larger w typically improves the image-text alignment but overlarge w causes the degradation of sample fidelity and diversity.
3.2 Temporal Co-Denoising
As we discussed above, current Text-to-Video diffusion methods for generation and editing typically view the video as a whole. Given a noisy video vt, they train a diffusion model pθ(vt−1|vt, c) with respect to noise prediction model εθ(vt, t, c) to denoise it as a whole. This greatly limits the video length that they are able to generate and makes it hard for them to accommodate multi-text conditions. Though some works performed long videos generation via the autoregressive mechanism, this manner suffers from severe content degradation and only supports serialization generation, leading to inference inefficiency.
In contrast, we consider the denoising process of the entire video as multiple short videos with temporal overlapping undergoing parallel denoising in the temporal domain. We approximate the denoising trajectory of a long video through the joint denoising model of short videos in the temporal domain. More specifically, we suppose there exists a model pl
θ(vt−1|vt, c) (with a corresponding
noise prediction network εl
θ(vt, t, c)) capable of denoising the given long video vt, resulting in the denoising trajectory,
vT , vT −1, . . . , v0, s.t. vt−1 ∼ pl
θ(vt−1|vt, c),
where we use the diffusion model to gradually transform the pure noise vT ∼ N (0, I) to the clean video v0. c can be represented as a single or multiple text prompts.
We define a set of mappings Fi to project all original videos vt (both noisy and clean) in the trajectory
to short video segments vti, specifically,
Fi(vt) = vi
t = vt,S∗i:S∗i+M , t = 1, 2, . . . , T, i = 0, 1, . . . , N − 1,
where vt,S∗i−S∗i+M represents the collection of frames with frame id from S ∗ i to S ∗ i + M , S represents the stride among adjacent short video clips, M is the fixed length of short videos, and N is the total number of clips. Empirically, we find that setting S to M//2 or M//4 yields excellent results and preserves efficiency. When setting S = M , our method degrades into isolated denoising. The total number of frames of the video is S ∗ N + M . Each short video vti is guided with an
independent text condition ci. After obtaining these short videos, we are able to denoise these short
5


vit,j vit,j+1 vt, M
2
i
vit,j−1 vit,j
Anchor frame
Resnet Block Resnet Block Resnet Block Resnet Block Resnet Block
Temporal Attention (Optional)
× Num blocks
Cross Frame Attention Cross Attention
Cross Frame Attention Cross Attention
Self-Spatial Attention Cross Attention
Cross Frame Attention Cross Attention
Cross Frame Attention Cross Attention
“A man is surfing on the sea”
Text Encoder
ci
Figure 3: High-level overview of the model architecture for short video denoising. The given noisy short video is treated as a collection of noisy images. In each block, we begin by processing each image independently with 2d convolutions (with normalization and residual connection). Then, spatial attention is applied to the selected anchor frame to enhance pixel interactions, while bi-directional cross-frame attention enables other frames to interact with both adjacent frames and the anchor frame. After that, cross-attention is incorporated to integrate textual information for guiding the denoising process, and temporal attention is trained to further refine the relationships among frames.
videos using the off-the-shelf short video diffusion models pi
θ(vti−1|vti, ci). For simplicity, we can
set the diffusion models for all video clips to a single diffusion model ps
θ(vti−1|vti, ci), and we find that it works quite well. Then we have,
vi
t−1 ∼ ps
θ (vi
t−1|vi
t, ci).
The remaining question is how to obtain the denoised vt−1 after acquiring all short video clips vti−1.
Considering that we have assumed Fi(vt) = vti for all t and i, the ideal vt−1 should satisfy that
Fi(vt−1) is as close as vti−1 as possible. Therefore, the optimal vt−1 can be obtained by solving the following optimization problem.
vt−1 = arg min
v
N −1
X
i=0
Wi ⊗ (Fi(v) − vi
t−1) 2
2,
where Wi is the pixel-wise weight for the video clip vti, and ⊗ means the tensor product. It is not difficult to verify that for an arbitrary frame j in the video vt−1, namely vt−1,j, it should be equal to the weighted sum of all the corresponding frames in short videos that contain the j frame. We provide the proof in Sec. III.
In this way, we are able to approximate the transition function pl
θ(vt−1|vt, c) with ps
θ(vti−1|vti, ci). As we claimed, our method establishes an abstract long video generator and editor without necessitating any additional training, enabling the generation and editing of videos of any length using established short video generation and editing methodologies.
Condition interpolation. Considering that it is rather cumbersome to label all short videos with exact text descriptions, we allow the acceptance of sparse conditions. For instance, assuming that only vtki, k ∈ N+ are labeled with text descriptions cki, we obtain the text conditions of other video
clips through adjacent interpolation cki+j = k−j
k cki + j
k ck(i+1), where j = 0, 1, . . . , k. This allows
for more flexibility in text-based guidance, enabling smoother content generation and simplifying the overall process for users.
4 Integrate Gen-L-Video with Mainstream Paradigms
As previously mentioned, our method can be applied to three mainstream paradigms: pretrained t2v, tuning-free t2v, and one-shot-tuning t2v. In this section, we will introduce our implementation and improvements for these paradigms in long video generation and editing. Furthermore, by utilizing additional control information, we can achieve more accurate layout control. Advances in open-set detection and segmentation allow us to achieve precise editing of arbitrary objects in the video without altering the other contents (e.g.,background), resulting in a more powerful and flexible video editing process. All our implementations are based on the pretrained LDM and its variants.
Pretrained Text-to-Video. For pretrained Text-to-Video generation and editing, we choose the open-sourced VideoCrafter [15]. VideoCrafter is a Text-to-Video model fine-tuned from LDM on the
6


large text-video dataset WebVid-10M [2]. For modeling the dynamic relationships among frames, VideoCrafter adds additional temporal attention blocks in the original LDM. The pipeline for long video generation and editing follows our proposed temporal co-denoising as illustrated in Fig. 1.
Tuning-free Text-to-Video. For tuning-free Text-to-Video, we follow the pipeline of Pix2Video [3] for its efficiency, with an additional distance penalty among adjacent frames to avoid dramatic changes. Though many tuning-free Text-to-Video generation [25] or editing [3, 43] methods apply the sparse causal attention mechanism [63], where the spatial attention is replaced by the cross attention between each frame and its former adjacent frame and the very first frame in the video. We find that it typically causes noticeable inconsistency between the initial few frames and the subsequent frames when generating long videos as shown in Fig. 2. We argue that it is because when the anchor frame is chosen as the first frame, its denoising path will not be influenced by any other frames in the current and following video clips, leading to unidirectional information propagation. It is obvious considering that in sparse causal attention, vanilla spatial attention is conducted in the very first frame, and the denoising of other video clips will also not influence the noise path of the first frame. Instead, we propose to set the anchor frame as the middle frame in each short video clip and bidirectionally propagate the information to both the start and the end, which we dubbed as Bi-Directional Cross-Frame Attention. The bidirectional information propagation allows the mutual influence of anchor frames among different video clips, making it easier to find a compatible denoising path for the long video. Specifically, denoting the input feature for the cross attention block of jth frame in video vti as zi
t,j, the computation of Attention(Q, K, V ) is formulated as
Q = W Qzi
t,j , K = W K zi,∗
t,j , V = W V zi,∗
t,j ,
where zi,∗
t,j =

 
 
zi
t,j−1, zt,⌊M/2⌋ , j > ⌊M/2⌋
h
zi
t,j+1, zi
t,⌊M/2⌋
i
, j < ⌊M/2⌋
zt,⌊M/2⌋, j = ⌊M/2⌋
and [·, ·] means the operation of sequence dimen
sional concatenation.
One-shot tuning Text-to-Video. For one-shot tuning Text-to-Video, we follow the pipeline of Tune-A-Video [63]. Similar to Tuning-free Text-to-Video, we also replace the sparse causal attention mechanism with our proposed bidirectional cross-frame attention. However, applying this pipeline directly to the training and generation of long videos presents non-trivial challenges. Although we can prompt the model to learn denoising for each short video clip, it struggles during generation. This is because many short video clips in a video share the same text description, making it difficult for the model to determine which clip it is denoising based on randomly initialized noise and similar text conditions alone. To address this, we suggest learning clip identifier ei for each short video clip vti to guide the model in denoising the corresponding clip.
However, introducing ei could lead to overfitting of the corresponding clip content, causing the model to overlook the text information and lose its editing ability. Drawing on the idea of CFG [17], we randomly drop ei during training. At test time, we base our approach on:
εˆθ (vi
t, t, ci, ei) = (1 + w)εθ(vi
t, t, ci, ei) − wεθ(vi
t, t, ∅, ∅).
This effectively alleviates the overfitting phenomenon. We believe that video learning consists of learning content and motion information. Although ei learns both content and motion information for the video clip vti, we aim to retain only the motion information. By dropping ei, the model learns across all video clips, gaining a large amount of content information. Shifting the denoising direction away from the scenario when ei is dropped allows us to reduce overfitting to video content.
Personalized and controllable generation. Our method can be easily extended to personalized and controllable layout generation. Users can easily combine personalized diffusion models obtained through fine-tuning strategies like DreamBooth [48] and LoRA [23] with our generation pipelines. Besides, we are able to inject additional layout control such as pose and segmentation maps with ControlNet [71] and T2I-Adapter [39, 33] pretrained on image datasets [51]. This allows us for more smooth and more precise video generation and editing.
Edit anything in the video. Open-set detection [30] and segmentation [26] have demonstrated remarkable ability and inspired plenty of interesting applications. We show that it is possible to
7


Table 1: Quantitative comparison.
Method Frame Consistency Textual Alignment
Avg. Score ↑ Human Pref. ↑ Avg. Score ↑ Variance (×100) ↓
Isolated 91.65 14.62% 21.16 0.57 Gen-L-Video 93.18 85.38% 21.18 0.48
Table 2: Comparison to different methods.
Method Long Multi-Text Condition Vast Video Corpus Parallel Denoise Versatile
Tune-A-Video [63] × × × × × LVDM [15] ✓ × ✓ × × NUWA-XL [66] ✓ ✓ ✓ ✓ × Gen-L-Video ✓ ✓ × ✓ ✓
“A rabbit is eating a
watermelon”
Tiger
Source Video
w/ Identifier
One-shot tuning
Source Video
w/o Identifier
“A grizzly bear is crawling on rocks.”
Dog, Orange Tiger
Tuning-free
Figure 4: Left: Comparison between tuning-free t2v and one-shot tuning t2v. One-shot tuning t2v yields more editing flexibility. Right: Comparison between one-shot-tuning t2v w/ or w/o clip identifier. One-shot-tuning t2v w/o clip identifier fails to generate consecutive motions in the source video with random initial noise.
combine these with our method to achieve precise arbitrary object editing in long videos. Specifically, given a prompt of a specific object (e.g.,man), we apply the open-set detection model Grouding DINO [30] to detect the corresponding object in each frame. Then, the open-set segmentation model SAM [26] is applied to get the precise mask of the object in each frame of the video. With these masks, we are capable of applying inpainting methodologies for video editing while keeping the other components of the video content unchanged. We find that directly using an pretrained Text-to-Image diffusion model with our proposed bi-directional cross-frame attention can already yield acceptable results. To achieve more precise layout generation, we additional add a controlnet pretrained on Text-To-Image datasets [51] to accept the SAM maps.
5 Experiments
Implementation details. All our experiments are heavily built upon the pretrained LDM [47] (a.k.a Stable Diffusion), as we mentioned. By default, DDIM sampling strategy is applied for all our experiments and the sampling step and guidance scale is set to 50, and 13.5, respectively. In most cases, we set the number of frames of short video clips to 16 and stride between adjacent short videos clips to 4. For the one-shot-tuning Text-to-Video pipeline, we set the basic learning rate as 3e − 5 and scale up the learning rate as the batch size, which greatly accelerates the training. The default beach size is set to 5, and the loss typically converges within 100 epochs for videos with around 100 frames.
Benchmarks. To evaluate our method, we collect a video dataset containing 66 videos whose lengths vary from 32 to hundreds of frames. These videos are mostly drawn from the TGVE competition [63] and the internet. For each video, we label it a source prompt and add four edited prompts for video editing, including object change, background change, style transfer, similar motion changes, and multiple changes. We provide details about the dataset in Sec. I.
Qualitative results. We provide a visual presentation of representatives of our generated videos in Fig. 5, including results in various lengths generated through pretrained t2v, tuning-free t2v, one-shot tuning t2v, personalized diffusion model, multi-text conditions, pose layout control, and video inpainting through the auto-detected mask, respectively. All of them show favorable results, demonstrating the strong versatility of our framework. More results can be seen in Sec. IV and our project page: https://github.com/G-U-N/Gen-L-Video.
Quantitative results. For quantitative comparisons, we assess the video frame consistency and the textual alignment following the prior work [63]. We compare Gen-L-Video with Isolated Denoising, where each video clip is denoised isolatedly. Regarding the video frame consistency, we employ the CLIP [44] image encoder to extract the embeddings of individual frames and then compute the average cosine similarity between all pairs of video frames. To evaluate the textual alignment, which refers to the alignment between text and video, we calculate the CLIP score of text and each frame in the video. The average value of the scores is used to measure the alignment degree while the variance of those is used to measure the alignment stability. For human preference, we select several
8


Tuning-free One-shot tuning Pretrained
Controllable Multi-text Personalized
“A man is boating, village.” “A man is boating, a man is walking by the river, sunset, city.”
“A pretty girl is walking in the sea.”
“A woman is playing tennis.”.
“A jeep car is moving in the snow.”
“A tiger in the grass in the sun.”
”Astronaut is riding horse.”
“Iron man is surfing.”
Edit Anything
Edit
Edit
Figure 5: Qualitative generation results of Gen-L-Video.
participants to vote on which method yields better frame consistency and textual alignment and get 1040 votes in total. The experimental results are shown in Table. 1.
Ablation study. Bi-directional cross-frame attention. We compare our proposed Bi-directional cross-frame attention with the sparse causal attention when temporal co-denoising is applied. As illustrated in Fig. 2, our method achieves the most smooth and consistent result while sparse causal attention typically causes the first few frames incwonsistent with subsequent ones. Video clip identifier. We compare the generation results of one-shot tuning t2v with or without the clip identifier in Fig. 4 (Right). When random initial noise is used, one-shot tuning t2v without the clip identifier fails to generate consecutive content in the source video, while the other method succeeds
6 Conclusion
In this work, we propose Gen-L-Video, a universal methodology that extends short video diffusion models for efficient multi-text conditioned long video generation and editing. We implement mainstream Text-to-Video methods and make additional improvements to integrate them with Gen-L-Video for long video generation and editing. Experiments verify our framework is universal and scalable.
Limitations: In general, our framework should be able to be extended into the co-working of various different video diffusion models with different lengths to obtain more flexibility in generation and editing, but we haven’t experimented with that. This avenue of research is left as future work.
9


References
[1] Rajat Arora and Yong Jae Lee. Singan-gif: Learning a generative video model from a single gif. In CVPR, pages 1310–1319, 2021. 3
[2] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, pages 1728–1738, 2021. 1, 7
[3] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J Mitra. Pix2video: Video editing using image diffusion. arXiv preprint arXiv:2303.12688, 2023. 1, 3, 7
[4] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. 3
[5] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-toimage generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022. 1, 2, 3
[6] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. 3
[7] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, pages 89–106. Springer, 2022. 2
[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3
[9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. arXiv preprint arXiv:2204.03638, 2022. 4
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020. 2
[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages 10696–10706, 2022. 3
[12] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In ECCV, pages 598–613, 2018. 3
[13] Shir Gur, Sagie Benaim, and Lior Wolf. Hierarchical patch vae-gan: Generating diverse videos from a single sample. NeurIPS, 33:16761–16772, 2020. 3
[14] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022. 4
[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. 3, 4, 6, 8
[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 1, 3
[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS, 2021. 5, 7
[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3
10


[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 3, 4
[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3
[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv:2204.03458, 2022. 1, 3
[22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1, 3
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, 7
[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276, 2022. 3
[25] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2, 3, 7
[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2, 7, 8
[27] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In AAAI, volume 32, 2018. 3
[28] Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. NeurIPS, 35:15420–15432, 2022. 4
[29] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. arXiv preprint arXiv:2303.04761, 2023. 1, 2, 3
[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 2, 7, 8
[31] Yue Liu, Xin Wang, Yitian Yuan, and Wenwu Zhu. Cross-modal dual learning for sentence-tovideo generation. In ACM MM, pages 1239–1247, 2019. 3
[32] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. arXiv e-prints, pages arXiv–2303, 2023. 2, 3
[33] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023. 7
[34] Tanya Marwah, Gaurav Mittal, and Vineeth N Balasubramanian. Attentive semantic video generation using captions. In ICCV, pages 1426–1434, 2017. 3
[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2021. 3
[36] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In ACM MM, pages 1096–1104, 2017. 3
11


[37] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022. 5
[38] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 3
[39] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 1, 3, 7
[40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1, 3
[41] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion: Training diffusion models on a single image or video. arXiv preprint arXiv:2211.11743, 2022. 2, 4
[42] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions. In ACM MM, pages 1789–1798, 2017. 3
[43] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023. 1, 2, 3, 7
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763. PMLR, 2021. 3, 8
[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821–8831. PMLR, 2021. 2
[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 3
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pages 10684–10695, 2022. 3, 5, 8
[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 3, 7
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1, 3
[50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 1
[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush Katta, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: laion5b: A new era of open large-scale multi-modal datasets. https://laion.ai/ laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/, 2022. 7, 8
[52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1, 3
12


[53] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. 3
[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 5
[55] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865, 2020. 2
[56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 3
[57] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30, 2017. 1, 3
[58] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 4
[59] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022. 4
[60] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. In ECCV, pages 398–414. Springer, 2022. 3
[61] Chen Henry Wu and Fernando De la Torre. Unifying diffusion models’ latent space, with applications to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559, 2022. 3
[62] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nüwa: Visual synthesis pre-training for neural visual world creation. In ECCV, pages 720–736. Springer, 2022. 1, 3
[63] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 1, 2, 3, 4, 7, 8, 15
[64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In CVPR, pages 1316–1324, 2018. 2
[65] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving text-to-image synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021. 2
[66] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 4, 8
[67] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2
[68] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2
[69] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. arXiv preprint arXiv:2302.07685, 2023. 4
[70] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-to-image generation. In CVPR, pages 833–842, 2021. 2
13


[71] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 1, 3, 7
[72] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan Zhan. Pycil: A python toolbox for class-incremental learning. arXiv preprint arXiv:2112.12533, 2021. 3
[73] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 3
[74] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In CVPR, pages 5802–5810, 2019. 2
14


Supplementary of Gen-L-Video
I Dataset Details
To evaluate our method, we have selected several videos (totaling 66) from TGVE competition [63] and Internet. For internet videos, we have designed 4 distinct prompts that introduce dynamic changes in the areas of object recognition, stylistic elements, background variations, similar motion patterns, or a combination thereof, based on the original prompts.
Table 3: Names of videos selected.
cows-grazing shopping-entertainment-center hike warsaw-multimedia-fountain deer-eating-leaves seagull-flying singapore-airbus-a380-landing pigs mallard-water red-roses-sunny-day cows miami-surf sharks-swimming pouring-beer-from-bottle dog bird-on-feeder airbrush-painting bear wind-turbines-at-dusk horsejump-low camel lotus typewriter-super-slow-motion wetsuit-surfing setting-sun cat-in-the-sun street-artist-painting gold-fish motorbike rabbit-watermelon earth-full-view snowboard eating-pizza fireworks-display weightlifting-sofa drift-turn surfer-on-wave american-flag-in-wind basketball-mall ship-sailing aircraft-landing dirt-road-driving audi-snow-trail eiffel-flyover butterfly-feeding-slow-motion kettleball-training ferris-wheel-timelapse dj-mixing-music ski-lift-time-lapse swimmer raindrops tennis swans sunset-beach-yoga sunset-swinging mbike-trick man-skiing car-turn boxer-punching-towards-camera man-surfing horizontal-match-striking airplane-and-contrail ski-follow las-vegas-time-lapse geometric-video-background blackswan
II User Study Details
We utilized the aforementioned dataset as the benchmark to contrast our approach, Gen-L-Video, with the Isolated method. Beyond quantitative indices (frame consistency and textual alignment), we also enlisted several participants to vote on which method was superior. To assess frame consistency, we generated 264 videos from 4 prompts and an additional 65 videos using both methods, respectively. Then we replicate and shuffle them to 1040 videos. We distributed pairs of these videos to several individuals, posing the question, "Which video exhibits superior frame consistency between these two?" In order to gauge textual alignment, we created 279 videos from varying prompts using both methods separately. We subsequently asked the participants, "Which video aligns more accurately with the text description among these two videos? A?" No B nr diffeerenc?? Most participants reported no significant difference in the degree of textual alignment, but a clear improvement in alignment stability, indicating that our method maintains good text-based editing capabilities.The more detailed results of these comparisons are depicted in Table 1.
III Proof for the Optimal Approximation
As we claimed in Sec. 3.2, the ideal vt−1 should satisfy that Fi(vt−1) is as close as vti−1 as possible. The optimal vt−1 can be obtained by solving the following quadratic optimization problem:
vt−1 = arg min
v
N −1
X
i=0
Wi ⊗ (Fi(v) − vi
t−1) 2
2,
15


where Wi is the pixel-wise weight for the video clip vti, and ⊗ means the tensor product. Here we show that, for an arbitrary frame j in the video vt−1, namely vt−1,j, it should be equal to the weighted sum of all the corresponding frames in short videos that contain the j frame.
Proof III.1 Considering that there are N video clips in total, we denote the set of all video clips and the set of their indices as V = {v0, v1, . . . , vN−1} and I = {0, 1, . . . , N − 1}, respectively. Further, we assume that there exists a set of video clips Vj consisting of short video clips containing the corresponding jth frame v♢,j in the original long video. Similarly, the set of indices corresponding to Vj is denoted as Ij. Here we use the ♢ to represent that it could be suitable for all the time t in the denoising path.
Given a short video clip vi ∈ Vj, we denote the corresponding frame of v♢,j in vi as vi
♢,j∗ for
simplicity. Note that the j∗ in different video clips represents different values.
Therefore, the original optimization objective can be written as:
N −1
X
i=0
Wi ⊗ Fi(vt−1) − vi
t−1
2 2
=
X
i∈Ij
Wi ⊗ Fi(vt−1) − vi
t−1
2
2+
X
i∈I\Ij
Wi ⊗ Fi(vt−1) − vi
t−1
2 2
=
X
i∈Ij
Wi,j∗ ⊗ Fi,j∗ (vt−1) − vi
t−1,j∗
2
2+
X
i∈Ij
X
j̸=j∗
Wi,j ⊗ Fi,j (vt−1) − vi
t−1,j
2 2
+
X
i∈I\Ij
Wi ⊗ (Fi(vt−1) − vi
t−1) 2
2.
Where, Wi,j is the pixel-wise weight for the jth frame in vti−1 (i.e.,vi
t−1,j ), and Fi,j (vt−1) is the jth frame in Fi(vt−1). It is not difficult to observe that the last two terms in the formula have nothing to do with vt−1,j. We denote them as constant C. Then we have,
X
i∈Ij
Wi,j∗ ⊗ Fi,j∗ (vt−1) − vi
t−1,j∗
2
2+C
=
X
i∈Ij
Wi,j∗ ⊗ Fi,j∗ (vt−1) − vi
t−1,j∗
⊤ Wi,j∗ ⊗ Fi,j∗ (vt−1) − vi
t−1,j∗ + C
=
X
i∈Ij
h
(Wi,j∗ ⊗ Fi,j∗ (vt−1))⊤ (Wi,j∗ ⊗ Fi,j∗ (vt−1)) + Wi,j∗ ⊗ vi
t−1,j∗
⊤ Wi,j∗ ⊗ vi
t−1,j∗
−2 Wi,j∗ ⊗ vi
t−1,j∗
⊤ (Wi,j∗ ⊗ Fi,j∗ (vt−1))
i
+ C.
Denote the above objective as L and take the gradient of L with respect to vt−1,j, and then we have
∂L
∂ vt−1,j
=2
X
i∈I j
(Wi,j∗ ⊗ Fi,j∗ (vt−1)) − Wi,j∗ ⊗ vi
t−1,j∗ ⊗ Wi,j∗ ⊗ ∂Fi,j∗ (vt−1)
∂ vt−1,j
.
Note that Fi,j∗ (vt−1) = vt−1,j , therefore ∂Fi,j∗ (vt−1)
∂vt−1,j = 1 and we can replace the Fi,j∗ (vt−1) in the above formula with vt−1,j. Then, we have
∂L
∂ vt−1,j
=2
X
i∈Ij
Wi,j∗ ⊗ (Wi,j∗ ⊗ vt−1,j ) − Wi,j∗ ⊗ vi
t−1,j∗
=2
" X
i∈I j
(Wi,j∗ )2 ⊗ vt−1,j −
X
i∈I j
(Wi,j∗ )2 ⊗ vi
t−1,j∗
#
.
Therefore, set the gradient to be zero, and then we get the optimal vt−1,j,
vt−1,j =
P
i∈Ij (Wi,j∗ )2 ⊗ vi
t−1,j∗
P
i∈Ij (W 2
i,j∗ )2 ,
which is the weighted sum of all the corresponding frames in short video clips that contain the jth frame.
16


IV Additional Results
Here, we showcase additional results.
Multi-text long video. Fig. 6 illustrates an example of our multi-text conditioned long video. Specifically, we first split the original video into several short video clips with obvious content changes and of various lengths, and then we label them with different text prompts. The different colors in Fig. 6 indicate short videos with different text prompts. Then, we split the original long video into short video clips with fixed lengths and strides. For video clips only containing frames conditioned on the same prompt, we can directly set it as the condition. In contrast, for video clips containing frames conditioned on different prompts, we apply our proposed condition interpolation to get the new condition. After all of these, our paradigm Gen-L-Video can be applied to approximate the denoising path of the long video.
Pretrained Text-to-Video. Gen-L-Video can also be applied to the pretraiend short video generation model for longer video generation. We compare the results generated through our method and isolated denoising in Fig. 7, and the result reveals that our method significantly enhances the relevance between different video clips.
Controllabel video generation. We showcase the results generated by injecting additional control information in Fig. 8. The results show that our method can be easily combined with additional information to achieve precise layout control.
Edit anything. Our approach demonstrates significant compatibility with inpainting tasks. As depicted in Fig.9 and Fig.10, our method can reliably edit very long videos and maintain consistent content. The examples given in Fig. 9 are longer than 12 and 20 seconds, respectively.
Long video with smooth semantic changes. Our paradigm also allows us a pleasant application where we are able to edit the source video to generate videos with smooth semantic changes. For example, we are able to generate cars running on the road from day to night to reflect the time flies. The generated results are represented in Fig. 11.
17


1. A boy/girl is fighting, holding a katana. 2.The boy/girl is attacked by an assassin using katana. 3. The boy/girl is stepping back to the forest. 4.The assassin is running towards the boy/girl. 5. The boy/girl is fighting against the assassin with a katana.
Figure 6: Multi-text conditioned long video.
18


A car is moving on the road.
a monkey is drinking water
An astronaut is riding a horse.
Isolated Gen-L-Video Isolated Gen-L-Video Isolated Gen-L-Video Isolated Gen-L-Video
An astronaut is riding a horse, loving Vincent style.
Figure 7: Long video generation with pretrained short video diffusion models.
19


A cute boy is playing tennis.
+
+
+
Iron man is fighting in the snow.
A Van Gogh style painting of a man dancing.
Detector
Estimator
A dog in the sun.
A realistic tiger in the sun.
A pretty girl in the sun
+
+
+
Figure 8: Controllable long video generation.
20


Mask Detector
Mask Detector
eating cake.
+
eating pizza, cartoon style.
+
eating watermelon.
+
Cyberpunk.
+
goggles.
+
pink sunglasses.
+
Figure 9: Edit anything in videos.
21


Mask Detector
Bat Man.
+
Iron Man.
+
Figure 10: Edit anything in videos.
A man is boating, village. A man is walking by, city, sunset.
A jeep car is running on the beach, sunny A jeep car is running on the beach, night
A jeep car is running on the snow, sunny. A jeep car is running on the snow, night.
Lion, grass, rainy. Cat, grass, Sun.
Iron man is skiing in the snow. Iron man is flying in the sky.
A man is surfing in the sea. A man is skiing in the snow.
Figure 11: Long videos with smooth semantic changes.
22