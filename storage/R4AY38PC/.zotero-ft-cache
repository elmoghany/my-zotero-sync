JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
DreamStory: Open-Domain Story Visualization by
LLM-Guided Multi-Subject Consistent Diffusion
Huiguo He , Huan Yang , Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang , Zeyu Liu , Wenhao Huang, Hongyang Chao , Jian Yin
Scene 6: The little girl, surrounded by a bright glow from the bundle of matches, joyfully seeing her kind and lovely grandmother appear before her.
Scene 7: The small, fragile girl with long fair curls and a kind and lovely elderly woman, surrounded by a radiant light, joyfully ascend into the sky, leaving the cold world behind.
Scene 8: A little girl with long fair curls, lies in the corner, her eyes closed and a smile on her face, surrounded by burnt matches.
Scene 5: The girl lighting another match and seeing a magnificent Christmas tree, larger and more beautiful than any she's seen, with thousands of candles.
Scene 2: The little girl lighting a match against the wall, her face illuminated by the sudden warm glow, with snow falling around her.
Scene 3: The little girl imagining a warm, iron stove in front of her as she holds her hands over the lit match, trying to warm them.
Scene 4: A bright match light revealing a sumptuous feast with a roast goose that seems to walk towards the little girl from the table.
Scene 1: A small, fragile girl with a box of matches walks through snow-covered streets, with snowflakes falling on her long fair hair.
Input Story:
The Little Match Girl
It was so terribly cold. Snow was falling, and it was almost dark. ...
It became bright again, and in the glow the old grandmother stood clear and shining, kind and lovely. ... The child sat there, stiff and cold, holding the matches, of which one bundle was almost burned.
Fig. 1. Illustration of our proposed DreamStory framework. This system takes a full narrative text as input, generates vivid visual content, and maintains the consistency of multiple subjects across various scenes within the story. Please visit the project homepage to watch the video.
Abstract—Story visualization aims to create visually compelling images or videos corresponding to textual narratives. Despite recent advances in diffusion models yielding promising results, existing methods still struggle to create a coherent sequence of subject-consistent frames based solely on a story. To this end, we propose DreamStory, an automatic open-domain story visualization framework by leveraging the LLMs and a novel multi-subject consistent diffusion model. DreamStory consists of (1) an LLM acting as a story director and (2) an innovative Multi-Subject consistent Diffusion model (MSD) for generating consistent multi-subject across the images. First, DreamStory employs the LLM to generate descriptive prompts for subjects and scenes aligned with the story, annotating each scene’s subjects for subsequent subject-consistent generation. Second, DreamStory utilizes these detailed subject descriptions to create portraits of the subjects, with these portraits and their corresponding textual information serving as multimodal anchors (guidance). Finally, the MSD uses these multimodal anchors to generate story scenes with consistent multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA module ensures detailed appearance consistency with reference images, while MMCA captures key attributes of subjects from their reference text to ensure semantic consistency. Both modules employ masking mechanisms to restrict each scene’s subjects to referencing the multimodal information of the corresponding subject, effectively preventing blending between multiple subjects. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, which can assess the overall performance of the story visualization framework, subject-identification accuracy, and the consistency of the generation model. Extensive experiments validate the effectiveness of DreamStory in both subjective and objective evaluations. Please visit our project homepage at https://dream-xyz.github.io/dreamstory.
Index Terms—Story Visualization, Diffusion Model, Multi-Subject Consistency, Large Language Model.
✦
•Huiguo He is with Sun Yat-sen University, currently interning at 01.AI. (email: hehg3@mail2.sysu.edu.cn). •Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, and Wenhao Huang are with 01.AI. •Yuhang Zhang is with Beijing University of Posts and Telecommunications, currently interning at 01.AI. •Zeyu Liu is with Tsinghua University, currently interning at 01.AI. •Hongyang Chao and Jian Yin are with Sun Yat-sen University. •Huan Yang (email: hyang@fastmail.com) and Jian Yin are the corresponding authors.
1 INTRODUCTION
Story visualization aims to create a visually captivating and coherent sequence of visual content (including images and videos) that aligns with a given story. This field has become increasingly important in entertainment [1] and education [2], [3]. However, story visualization is particularly daunting in open-domain contexts, subjectized by diverse
0000–0000/00$00.00 © 2021 IEEE
arXiv:2407.12899v2 [cs.CV] 9 Mar 2025


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
content and themes. Despite the significant advancements in diffusion models [4]–[14] for visual content creation, current methods still struggle with the challenge of directly translating textual narratives into corresponding consistent visual representations. The primary challenges stem from two key issues. The first is generating effective story prompts using a Large Language Model (LLM) while accurately identifying recurring subjects within the narrative. The second is seamlessly incorporating this information into the diffusion model to maintain consistency in multi-subject generation has proven to be a significant obstacle. On the one hand, LLM has recently demonstrated impressive capabilities in long-text understanding [15], [16] and In-Context Learning (ICL) [17]. Chain-of-Thought (CoT) [18]–[20] reasoning has significantly improved their performance in handling intricate text understanding tasks. However, in the context of story visualization, the effectiveness of LLMs heavily depends on selecting appropriate prompts, posing challenges for generating coherent visual content and requiring further refinement. On the other hand, although previous works [21]–[27] have made efforts to improve the consistency in the diffusion model, their attempts have not yielded a satisfactory level of multi-subject consistency in open domain story. These methods fall into four categories: (1) dataset-based training, (2) few-shot fine-tuning, (3) encoder-based, and (4) trainingfree methods. Dataset-based methods [25] rely on specific story datasets (e.g., PororoSV [28] and FlintstonesSV [29]). Therefore, they are closed-domain methods and are limited in open-domain capabilities. Few-shot fine-tuning methods [23], [24] offer customization but necessitate additional training costs for each story, inevitably leading to overfitting and diversity degradation. Encoder-based methods [22], [30] aim to train an image encoder to convert the reference image into the image condition aligned with the original text condition. It guides the generation process by injecting image conditions into the cross-attention layer. However, these methods mainly focus on a single subject and are hindered by diversity degradation [27] and computational resource consumption. Training-free methods [26], [27], [31] maintain subject consistency by facilitating interaction between target and reference images in the self-attention layer. These methods have gained widespread attention due to their efficiency. However, they still face issues of subject confusion and overlooking fine-grained descriptions in opendomain story visualization. To address the aforementioned challenges, we introduce DreamStory, a training-free, automatic, open-domain story visualization framework. Specifically, given a story, DreamStory first employs the LLM (such as GPT-4 [32] and Yi [33]) to generate detailed descriptive prompts for both subjects and scenes, ensuring alignment with the narrative. This includes annotating each scene’s subjects, as well as performing necessary rewrites, for subsequent consistent generation. Subsequently, DreamStory utilizes these detailed subject descriptions to create accurate portraits of the subjects. These portraits, along with their corresponding text, are served as multimodal anchors (guidance) for the consequent generation process. The motivation of this approach is that the text and images are naturally aligned in the diffusion
model’s semantic space, as the image is generated based on the corresponding text. Therefore, this aligned multimodal information, which is rich in subjects’ semantics, attributes, and visual appearance, benefits the model in generating more consistent subjects. Finally, a novel Multi-Subject consistent Diffusion model (MSD) utilizes these multimodal anchors to produce story scenes that maintain consistency across multiple subjects. The MSD consists of two key modules: Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA). The MMCA module captures essential subject attributes to ensure semantic consistency. While previous studies show that aligning image encoders with text embeddings maintains layout [30] and identity [22] consistency, our MMCA module uniquely preserves subject attributes (e.g., clothing and accessories) by using the naturally aligned text. In parallel, the MMSA module maintains detailed appearance consistency by allowing the Query (Q) token to query the Key (K) and Value(V ) tokens that belong to the same subject in the anchor. Our specific innovation is characterized by the use of masking mechanisms in both modules, ensuring that each scene’s subjects only reference information pertinent to the corresponding subject. This approach effectively prevents the blending of attributes between different subjects, thereby preserving their individual consistency. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, comprising 100 stories and 400 synthetic samples. The 100 Stories assesses the overall framework of automatic open-domain story visualization. The remaining 400 synthetic samples, each with 0, 1, 2, and 3 subjects, are utilized to evaluate the precision of the LLM in annotating scene subjects and the consistency of multi-subject generation. The main contributions of this paper are as follows: 1) We introduce DreamStory, a training-free framework for automatic open-domain story visualization, which utilizes LLMs as a story director to generate concise prompts of subjects and scenes, annotating the subjects in each scene. This information guides diffusion models in creating visually consistent content that aligns with the story narrative. 2) We propose a novel Multi-Subject consistent Diffusion model (MSD) that leverages both the subject prompt and its corresponding portrait to maintain consistency in multiple subjects across frames. 3) We build an evaluation benchmark DS-500 for open domain story visualization. Our method outperforms the mainstream methods on aesthetics, image-text consistency, and subject consistency through objective and subjective evaluations.
2 RELATED WORKS
2.1 Visual Content Generation
Variational AutoEncoders (VAEs) [34] and Generative Adversarial Networks (GANs) [28], [35]–[37] used to dominate visual generation field. Despite the significant advancements made by GAN, its optimization challenges persist [38][40]. Later, diffusion-based generative models [4], [5], [41][44] have emerged, achieving impressive image quality and diversity. Notably, Stable Diffusion (SD) [6] utilizes


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
a diffusion model in latent space, trained on the largest LAION-5B [45] dataset. While subsequent studies [8], [9], [46] have improved resolution and aesthetics, ensuring subject consistency across multiple images remains a challenge.
2.1.1 Dataset-based Story Visualization
Early methods [47]–[53] for story visualization relied on collecting datasets, such as PororoSV [28] and FlintstonesSV [29]. For example, Rahman et al. [50] propose a novel autoregressive diffusion-based framework. This framework includes a visual memory module that implicitly captures the actor and background context across the generated frames. Pan et al. [53] propose an auto-regressively diffusion model conditioned on history captions and generated images. It employs multimodal guidance (a CLIP [54] text encoder and a BLIP [55], [56] multimodal encoder) to ensure the generation of relevant and coherent images. Liu et al. [25] further proposed the StorySalon dataset and achieved SOTA results. However, these methods are constrained by the size and quality of existing datasets, limiting their performance in open-domain tasks. In contrast, our approach is designed for open-domain scenarios and is training-free, circumventing the challenges of gathering high-quality story visualization datasets.
2.1.2 Few-shot Finetuning Consistent Generation
Few-shot finetuning methods [10], [21], [23], [24], [57]–[59] primarily revolve around personalized image generation based on a few subject images. The model is finetuned on these images to learn their unique textual expressions. For example, Dreambooth [23] first proposed fine-tuning SD with LORA [60] on several images to make the model remember specific subject tokens for reference images. Sun et al. [10] further extend it in a never-ending manner, i.e., new concepts from the user are quickly learned without catastrophic forgetting. Jang et al. [24] proposed using a segmentation model to segment subjects for training and inference, effectively mitigating the influence of multi-subject blending. It has achieved SOTA performance in the field of few-shot finetuning for multi-subject consistent generation. However, these methods necessitate finetuning for each story or subject, resulting in extra computational costs. Besides, this approach inevitably risks overfitting, leading to a decline in the aesthetic quality and diversity of the generated images [27].
2.1.3 Encoder-Based Consistent Generation
In foundational T2I models, such as SD [6] and SDXL [8], the text is typically encoded into an embedding vector and injected into a cross-attention mechanism to generate images satisfying textual conditions. To achieve consistent generation, previous methods [22], [61]–[65] attempt to design an image encoder for generation under the image condition. Specifically, some studies [62], [64] tried to train a face encoder to ensure that the generated images maintain ID consistency. Similarly, Ye et al. [22] tried to train an image encoder that converts image conditions into a space aligned with the original text embedding. However, these methods can only handle a single subject. Therefore, they
are unsuitable for open-domain story visualization, which may involve multiple subjects of various types, including anthropomorphized animals.
2.1.4 Training-free Consistent Generation
Training-free methods have gained widespread attention due to their efficiency. These methods [26], [27], [31] maintain subject consistency by facilitating interaction between the target and reference images in the self-attention layer. For example, MasaCtrl [26] introduced mutual self-attention, which replaces the key and value in self-attention with those from the reference image. They also utilized a crossattention map as a mask to ensure that mutual self-attention concentrates on relevant subjects. ConsiStory [27] introduced Subject Driven Self-Attention (SDSA), which allows each frame to refer all subjects from multiple reference images in a batch. They also implemented token dropout and blended Vanilla Query techniques to increase layout diversity, and used DIFT [66] for feature injection in self-attention to enhance detail consistency. However, these methods still struggle to generate multiple subjects because all subjects in the target image can refer to all reference images regardless of whether their roles are the same. Furthermore, they failed to consider fine-grained descriptions of subjects that contain rich information on attributes which are beneficial for maintaining consistency.
2.2 Large Language Model
2.2.1 LLM in Text Understanding
Large Language Model (LLM) has recently demonstrated impressive capabilities in various NLP tasks, such as text summarization [67], [68], and question answering [69], [70]. Moreover, ChatGPT employs Reinforcement Learning from Human Feedback (RLHF) [71] to align the model’s output with human preferences, demonstrating an impressive ability for human interaction. Its remarkable In-Context Learning (ICL) [17] ability enables it to generate expected outputs by completing the input text’s word sequence, without additional fine-tuning. Furthermore, some studies [18]–[20] have revealed that carefully crafted Chain of Thought (CoT) strategies can significantly improve the performance of LLM models in handling intricate and lengthy tasks. Though LLM models can summarize texts and answer human questions, their ability to generate suitable prompts that guide diffusion models for story visualization is less studied. In the story visualization field, diffusion models are limited to recognizing subjects in novel visual vocabulary as they are usually referred to by their names without visual descriptions. In this paper, we study how to adjust the prompts generated by LLM models to better bootstrap diffusion models for story visualization.
2.2.2 LLM in Image Generation
Advanced visual generation models struggle with lowquality descriptions, which impedes their comprehension of subtle semantics. Many research [72]–[77] efforts have aimed to enhance the capabilities of T2I models by refining datasets and modifying user prompts. Specifically, Segalis et al. [72] enhanced generation performance by re-captioning the images using a specialized LLM and retraining a text-to-image model


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
LLM-Based Subject/Scene Subject Portrait Generation Prompt Generation
CoT-Based Prompt Alignment and Rewriting
Multi-Subject Consistency Diffusion Generation
Input Story
The Little Match Girl
It was so terribly cold. Snow was falling, and it was almost dark. ... It became bright again, and in the glow the old grandmother stood clear and shining, kind and lovely. ... The child sat there, stiff and cold, holding the matches, of which one bundle was almost burned.
Subject 1: a small, frail girl with long, fair curls, wearing tattered clothing and barefoot in the snow, holding a lit match with a hopeful yet sorrowful expression. Subject 2 ~ Subject N
Scene 1: The little girl and her grandmother embracing in a radiant warm light, flying upwards, leaving the cold, dark alley behind. Scene 2 ~ Scene M
System Prompt: List the main subjects/scenes in the stroy.
Subject 1: The little girl
Subject 2: The grandmother
Alignment System Prompt: Is subject A in scene B? Scene 1 contains Subject 1, Subject 2 Scene 2 ~ Scene M
Rewriting System Prompt: Generate a short prompt for subject A and rewrite the subject in scene B based on its short prompt. Rewrited Scene 1: The little girl and the grandmother embracing in a radiant, warm light, flying upwards, leaving the cold, dark alley behind.
Rewrited Scene 2 ~ Scene M
...
Scene 1 Scene M
Text+Image to Video (Optional)
Input Narrative Story
LLM-Based Modules
Diffusion-Based Modules
Fig. 2. The framework of our proposed DreamStory. Initially, the LLM comprehends a story and generates detailed prompts for key subjects and scenes. These prompts are aligned and rewritten to enhance understanding of the diffusion model, ensuring accurate visual content generation. Subject portraits are then generated based on these prompts, serving as multimodal anchors for maintaining multi-subject consistency and enriching scenes with high-quality visual details, which facilitates subsequent video creation using an image-to-video model.
on the updated data. Some approaches propose rewriting user prompts to enhance generated images regarding the aesthetic [75] and NFT market values [76]. Yang et al. [78] utilizes language models for planning, recaptioning, and generating images with coherent layouts. Cheng et al. [79] proposes employing LLM to facilitate user editing in an interactive manner. These methods all indicate that powerful LLM has the potential to enhance image generation. In this article, we utilize the LLM as a director to guide the generation of a series of story images with consistent multi-subject.
3 OUR APPROACH
3.1 Overall Framework
In this subsection, we introduce our automated story visualization framework (DreamStory), shown in Fig. 2. The framework operates as follows:
1) Story comprehension and prompt generation. Given a story (e.g., The Little Match Girl), a Large Language Model (LLM), such as GPT-4 [32], comprehends the narrative and generates concise yet detailed prompts for key subjects and scenes. These prompts serve as the foundation for subsequent visual content generation.
2) Prompt alignment and rewriting. The LLM identifies the subjects within each scene and performs necessary rewrites, replacing names with descriptions that the diffusion model can understand, such as rewriting "Kondo" to "towering gorilla." This enriches the scenes for visual content generation.
3) Subject portraits generation. The Text-to-Image (T2I) model then utilizes these prompts to create subject portraits. By focusing on individual subjects, this approach ensures alignment with the provided prompts.
4) Multimodal anchors for scene generation. The subject portraits, accompanied by their textual descriptions, act as multimodal anchors. The subsequent T2I model leverages these multimodal anchors to maintain subject consistency. It enriches the scenes with additional details, resulting in high-quality visual representations. These images can be transformed into video clips using an Image-to-Video (I2V) model, such as SVD [74], ConsistI2V [80], and Kling 1.
1. Kling
Our comprehensive process enhances the final image quality, making DreamStory indispensable for vivid story visualization.
3.2 LLM Prompt Generation Model
The Chain of Thought (CoT) [81]–[84] strategy has shown promising results in LLMs. The core idea of CoT is to break down complex problems into a series of simpler, manageable tasks, which guides the model towards generating anticipated results and enhances overall performance [81], [82]. Inspired by these pioneering works, we designed a prompt generation model based on the CoT strategy for the diffusion model. Our approach simplifies the entire process into a sequence of simple steps: generating prompts for subjects or scenes, annotating whether subjects are present in scenes, and making necessary revisions. Each of these tasks (text understanding or rewriting) is considerably easier due to its widespread presence in the LLM’s training samples compared with that of directly obtaining a suitable prompt for the diffusion model to visualize stories. All the task prompts are designed with at least two in-context examples to improve the performance and formatting of the results [84]. In the process of annotating scenes, we utilize the LLM to determine if a subject is present in the scene’s imagery, given the subject’s name and detailed prompts. We have observed that the LLM often generates scene prompts using the subject’s name, such as "Kondo". However, these prompts encounter difficulties when applied to the diffusion model, which often fails to recognize the names of subjects, particularly when the subject is not well-known and is absent from the training data. To address this issue, we propose rewriting the scene prompts. Specifically, we employ the LLM to create a concise prompt for the subject that encapsulates its key attributes. We then instruct the LLM to rewrite the scene based on this newly created short prompt. For example, the subject “Kondo” would be replaced with a description such as "towering gorilla". This method ensures a more accurate visual representation of the subject within the scene and is more suitable for the diffusion model. Our approach to LLM prompt generation presents a logical sequence of steps that address the challenges of


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
Query Key Value
×
Softmax ×
U-Net
SA CA SA CA SA CA SA CA
Query for Subject 1
Query for Subject 2
Query for Background
Key/Value for Subject 1
Key/Value for Subject 2
Key/Value for Original Features
Key/Value for Scene Prompt
Key/Value for Subject 1 Prompt
Key/Value for Subject 2 Prompt
The little girl and the
grandmother embracing
in a radiant, warm light,
flying upwards, leaving
the cold, dark alley
behind.
a small, frail girl with
long, fair curls, wearing
tattered clothing and
barefoot in the snow,
holding a lit match with
a hopeful yet sorrowful
expression.
A kind and lovely elderly
woman, radiating
warmth and light, with a
gentle smile, appearing
as if illuminated from
within.
Compute Attention
Ignore Attention
SA
CA Masked Mutual Cross-Attention
Masked Mutual Self-Attention
Multi-Subject Consistent Diffusion Model Masked Mutual Self-Attention
Masked Mutual Cross-Attention
Concat Concat Concat
T
Query Key Value
×
Softmax
×
×
Key for Subject 1 Prompt
Key for Subject 2 Prompt
Key for Scene Prompt
×
Value for Subject 1 Prompt
Softmax
Value for Subject 2 Prompt
×
Softmax
Value for Scene Prompt
×
Weighted Sum
Conv Conv Conv
Conv
Fig. 3. The illustration of our Multi-Subject consistent Diffusion models (MSD), along with its Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) mechanisms. It uses two subjects as examples and can be extended to any number of subjects. Query, Key, and Value projection in the attention layer have been omitted for ease of presentation.
generating detailed descriptions in vivid stories, as highlighted in Section 1. It provides a structured way to generate appropriate prompts with precise details for the diffusion model. It should be noted that our approach can generate an arbitrary number of scenes, which can be specified by the user or determined by the LLM based on the story content.
3.3 Multi-Subject Consistent Diffusion Model
Preserving subject consistency is a crucial objective in the generation of story images. Our MSD is specifically designed to provide a training-free solution for open-domain story visualization, as shown in Fig. 3. This approach is necessitated by the considerable costs involved in obtaining high-quality datasets for story visualization.
3.3.1 Existing Attention Mechanism
A standard attention layer in the popular diffusion model (e.g., SD [6], SD-XL [8], and Playground [9]) can be formulated as follows,
Ai = softmax QiKi/pdk , (1)
Oi = convout(Ai · Vi), (2)
where the Oi represents the attention output and Ai indicates the attention weight for the i-th image. Q is the query features projected from the spatial features, and K V are the key and value features projected from the spatial features (in selfattention layers) or the textual embedding (in cross-attention layers) with corresponding projection matrices. A simple convolution layer convout is finally applied to fuse the output features. We omitted the residual connection and layer count to simplify our expression. Existing work has verified that self-attention can control the appearance of the generated image [85], while crossattention controls the layout and can be used to locate the area of the target subject [86]. Based on these discoveries, recent works have verified the appearance information of reference images can be injected into the generation process
by substiting [26] or cascading [27] K and V with that of in the reference images. In the multi-subject scenario, however, they failed to keep multi-subject consistency because all subjects in the target image can refer to the information from all reference images regardless of whether their roles are the same.
3.3.2 Accurate Object Mask Generation
Accurate subject mask generation has been verified as a crucial problem in image generation [26], [27] and editing [65], [87]. However, obtaining the subject mask in an unregenerated target image is difficult. Previous works utilize LLM to manage the layout of generated images for editing [79] and accurate attribute binding [78]. This potentially leads to a lack of aesthetic layout and the generation of objects with unreasonable sizes, as LLM has not been optimized in this situation. Since diffusion models tend to generate similar layouts under close control conditions if the random seeds are the same [88], we adopt an openvocabulary segmentation model, e.g., GroundingSAM [89], to obtain an accurate subject mask in rehearsal target images, which is pre-generate with the original diffusion model. The detection phrases are marked by LLM, as mentioned above. To improve accuracy, we contact tokens of all subjects separated by periods as detection prompts for the target image, such as "man. girl.". A simple post-processing is adopted to guarantee the non-overlapping of all masks, enhancing the robustness of our approach. Due to the imposition of a new control process, the image may go beyond the SAM’s mask during generation, especially in the later steps. So, it is necessary to adjust the mask according to the features in the generation process. Previous work [26], [27] mainly adopts a segmentation mask by averaging the cross-attention maps of the subject token. However, this strategy may create a holed and noisy mask [87]. Therefore, we obtain a segmentation mask by multiplying the self-attention and cross-attention maps, with the self-attention map serving as a completion of the cross


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
attention maps [87]. So, the semantics maps are calculated as follows,
mi = mean(X
l∈L
X
r∈R
(Asa)r × Aca), (3)
Mi = Otsu F (mi
TGT )
⊤
× Otsu F (mi) , i ̸= TGT (4)
where Asa and Aca denote the self-attention and crossattention layers in the same block. The mask of the target image, denoted with the subscript ‘TGT’. L denotes the layer for gathering the attention map, and the averaging operation is denoted by mean(·). R is a hyper-parameter set as 4 followed by [87]. To save computational cost, we only collect all layers L from the previous timestep t − 1 to calculate the mask. The threshold is determined by Otsu’s method [90], represented as Otsu(·). The flatten operation, F (·), converts the matrix into a one-dimensional array. The symbol × stands for matrix multiplication. The final matrix Mi illustrates the correlation between the elements of the target and reference images, ensuring that elements in the target image reference only the related regions in the reference image. It should be noted that this is an optional strategy, employed only when the re-generated targets significantly exceed the initial range.
3.3.3 Masked Mutual Self-Attention
To alleviate the confusion between multiple subjects, we propose that only the appearances between the same roles can be referenced, i.e., multiple subjects in the target image can only refer to the same corresponding subject in other reference images. Given N subject portraits (reference image), we aim to generate one corresponding scene image (target image). By constructing the subject mask, the formalization of our self-attention layer is as follows,
K+ = [K1 ⊕ K2 ⊕ . . . ⊕ KN ⊕ KTGT], (5)
V + = [V1 ⊕ V2 ⊕ . . . ⊕ VN ⊕ VTGT], (6)
M + = [M1 ⊕ M2 ⊕ . . . ⊕ MN ⊕ 1], (7)
A+ = softmax QTGTK+/pdk + log M + , (8)
OTGT = convout(A+ · V +), (9)
where Mi is the subject mask for i-th reference images, and ⊕ indicates the concatenation operation. We assume the last one to be the target image, denoted with the subscript ‘TGT’. The standard attention masking technique is adopted, which nullifies softmax’s logits by assigning their scores to −∞ based on the mask, followed by previous works [26], [27]. It should be noted that, unlike ConsiStory [27] and StoryDiffusion [31], which allows all areas of the target image to reference the subject in the reference image, our method only permits referencing information from the same subject.
3.3.4 Masked Mutual Cross-Attention
As mentioned above, the rich information about the subject is not only contained in the reference image but also in the reference text. To fully utilize this information, we’ve implemented a Masked Mutual Cross-Attention (MMCA) mechanism. Its core idea is to allow the subject in the target image to query their reference text embedding and obtain rich, detailed attributes. We replace K and V with
those of the corresponding reference. Meanwhile, a subject mask ensures that only the subject area of the target image will query the corresponding reference text embedding. To enhance the stability, we adopt fusing multiple frames of information before adding the residuals. Therefore, our crossattention can be formalized as follows,
Ai
TGT = softmax QTGTKi/pdk + log(F (mi
TGT) × 1) , (10)
Oi
TGT = convout Ai
TGT · Vi . (11)
Subsequently, all the Oi
TGT are accumulated in a maskweighted manner to reduce the confusion of different subjects. For the overlapping area of the mask, we take the average value. The calculation can be formulated as follows,
OTGT = λ mu
ms
N
X
i=0
Oi
TGT + Ovanilla
TGT ∗ (1 − mu) ∗ (1 − λ), (12)
where the ms and mu respectively represent the sum and intersection of mi (i ∈ [1, N ]) and λ is the weight of text feature injection. In practice, we add a small number (10−8) to ms to prevent division by zero errors. The symbol
Ovanilla
TGT indicates the output of the target image from the vanilla forward process, which contains much background information of the target image. This mechanism ensures that each subject in the scene image references only its corresponding text, thus obtaining a wealth of attributes. Such a strategy is vital for extracting text information from the reference anchor, which in turn aids in the generation of a vivid and precise story visualization.
4 EXPERIMENTS
This section will introduce the evaluation benchmark and metrics, implementation details, and comprehensive experimental results. In Sec. 4.1, we will introduce the constructed benchmark, which includes 100 stories and 400 synthetic cases. Sec. 4.2 will introduce the objective and subjective evaluation metrics. We present the specific implementation details in Sec. 4.3, including our LLM, diffusion backbone, and MSD module. In Sec. 4.4 and Sec. 4.5, we will respectively present the comparative results with the current state-of-theart (SOTA) methods and conduct an ablation study. Sec. 4.7 briefly discuss the limitations of our method.
4.1 Evaluation Benchmark
To our knowledge, few datasets can validate the proposed DreamStory’s performance in open-domain story visualization. To address this issue, we constructed a benchmark DS-500, including 100 real stories and 400 synthetic cases. The benchmark of 100 real stories assesses our framework’s holistic performance. The additional 400 synthetic samples, divided into four groups of 100 samples, each with 0, 1, 2, and 3 subjects, are utilized to evaluate the precision of the LLM in annotating subjects present in the scene and the efficacy of multi-subject consistent generation.


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
DreamStory (Ours) MuDI ConsiStory StoryDiffusion
Confrontation at Starlight Ranch: A dimly lit, chaotic interior of a ranch house with overturned chairs and signs of struggle.
Potts's Discovery: An injured old trooper, Frank Potts, lying on the floor with a look of recognition and shock on his face.
Zoe's Desperation: A beautiful young woman with long golden hair, opening a second-floor window in distress, calling for help against a night sky.
The Wife's Plea: A woman kneeling on the floor, cradling the injured Potts in her arms, surrounded by dim lighting and chaos.
Scene name: Scene Prompt
Potts's Final Moments: A dying trooper, Frank Potts, reaching out to touch the golden hair of his kneeling daughter, Zoe, in a tender and poignant gesture.
The Revelation: The wife revealing the truth to Zoe about her father, with a baptismal certificate in hand, in a room with a few onlookers.
Reference Subjects' Portraits
Fig. 4. Qualitative comparisons of our DreamStory with SOTA approaches on the FSS real story benchmark. Ours, MuDI, and ConsiStory utilize the subject image on the bottom-left as the reference image. In contrast, StoryDiffusion references the subject image on the bottom-right. Different subjects are indicated with different colors. Please refer to the supplementary materials to watch the video.
4.1.1 The 100 Stories Benchmark
To validate the effectiveness of our overall framework, we first constructed a dataset. This dataset consists of 50 real, copyright-free English stories randomly downloaded from free-short-stories 2, and 50 short stories generated by ChatGPT. These data effectively simulate the distribution of real stories, thereby providing a robust validation of the performance of our DreamStory in open-domain story visualization.
4.1.2 The 400 Synthetic Benchmark
Firstly, we instruct GPT to generate a variety of non-repetitive subjects, each accompanied by detailed portrait prompts. We then employ GPT to annotate these subjects with type attributes (e.g., girl, man, dog), which are applicable for DINO detection. Subsequently, a subset of subjects is randomly selected, and GPT is tasked to generate scene prompts that exclusively include the chosen subjects. To prevent performance degradation of the diffusion model due to overly lengthy output text, we limit GPT’s output to approximately 40 words (roughly 50 tokens). These scene prompts, along with their associated subject prompts and type attributes, constitute the 400 synthetic benchmarks. Finally, these datasets will be manually checked and filtered to ensure accuracy.
2. Free Short Stories
4.2 Evaluation Metrics
In story visualization, aesthetics and image-text alignment are commonly employed metrics. In addition, the consistency of subjects across multiple frames is another crucial metric, which is one of the main problems this paper aims to address. Therefore, we evaluate generated results using three criteria: 1) aesthetics, 2) consistency between scene image and text, and 3) subject consistency between scene and reference image. To ensure accuracy and reliability, each criterion is evaluated objectively and subjectively.
4.2.1 Objective Evaluation
Followed by previous works [8], [9], we utilize an aesthetic predictor 3 to determine aesthetic scores. The CLIP 4 score is adopted to evaluate the similarity between the scene text and scene image, denoted as CLIP-T. To better assess subject consistency, we employ DreamSim [91] to evaluate the similarity between two subject images. The GroundingDINO [92] is first applied to detect the bounding box of the target subject based on its category, e.g., man or dog. For each subject, we use the image cropped from the highest-probability bounding box
3. improved-aesthetic-predictor 4. clip-vit-base-patch16


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
DreamStory (Ours) MuDI ConsiStory StoryDiffusion Naruto Meets the Ninja Cat: Naruto Uzumaki, a young ninja with bright blue eyes and sun-kissed hair in an orange jumpsuit, curiously approaches a sleek black cat with onyx eyes, wearing a miniature ninja vest, in the forbidden scroll library of Konoha.
The Unlikely Partnership: Naruto and the Ninja Cat stand side by side outside the village of Konoha, ready to embark on their quest, showcasing their contrasting appearances and the beginning of their friendship.
The Twilight Forest: The dense, ancient forest shrouded in perpetual twilight, with towering trees and a floor covered in thick moss, creates an eerie and mystical atmosphere.
The Confrontation with the Beast: Naruto, in a dynamic battle stance, unleashes his powerful jutsu against a shifting, shadowy beast in the twilight forest, while the Ninja Cat prepares to strike from the shadows.
Scene name: Scene Prompt
The Defeat of the Beast: The Ninja Cat leaps with precision towards a weak spot on the beast as Naruto's jutsu illuminates the scene, highlighting the moment of triumph over the creature of smoke and shadow.
The Legendary Crystal: The radiant crystal floats gently towards Naruto, glowing with magical warmth in the aftermath of the battle, symbolizing the fulfillment of his true wish for companionship.
Reference Subjects' Portraits
Fig. 5. Qualitative comparisons of our DreamStory with SOTA approaches on the ChatGPT generated story benchmark. Ours, MuDI, and ConsiStory utilize the subject image on the bottom-left as the reference image. In contrast, StoryDiffusion references the subject image on the bottom-right. Different subjects are indicated with different colors. Please refer to the supplementary materials to watch the video.
TABLE 1. Quantitative results of different backbone for our DreamStory on the DS-500 benchmark. Red indicate the best performance.
2-Subject 3-Subject
AES↑ CLIP-T↑ DS↑ D&C-DS↑ AES↑ CLIP-T↑ DS↑ D&C-DS↑ SDXL [8] 6.52 0.3819 0.5045 0.3018 6.59 0.3900 0.4618 0.1241 SDXL [8] + Ours 6.62 0.3747 0.6048 0.3848 6.69 0.3832 0.5228 0.1778
Playground [9] 6.67 0.3818 0.5796 0.3996 6.77 0.3841 0.5194 0.1938 Playground [9] + Ours 6.72 0.3779 0.6714 0.5444 6.81 0.3791 0.5965 0.2335
to compute DreamSim similarity. This original DreamSim score is denoted as DS. Furthermore, generating multiple subjects makes it feasible to create composite subjects that blend elements from multiple others. This can lead to a single composite subject scoring high in DS with multiple subjects. Therefore, we adopted the D&C-DS [24] metric to evaluate the consistency across multiple subjects, which has been validated to align with human preference. We also evaluate the accuracy of LLM annotation in 400 synthetic benchmarks.
4.2.2 Subjective Evaluation
Due to the bias of existing metrics, a user study is conducted to assess subjective results. Given the variability of individual ratings and the broad spectrum of scores across different evaluators, we employed a pairwise comparison in our
user study. For each evaluation, two sets of images were randomly displayed, each generated by a different method and accompanied by their respective texts. Participants were asked to judge each metric by selecting one of three options: Image A is superior, Image B is superior, or both are comparable. We engaged 20 independent evaluators for the assessment. Each evaluator conducted 100 reviews per benchmark, culminating in 2000 votes in total. The final results were compiled and are presented as percentages.
4.3 Implementation Details
LLMs as Story Director. We utilize ChatGPT4 [32], currently the most advanced large-scale language model, as our story director due to its powerful interactive and long context


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
DreamStory (Ours) ConsiStory MuDI StoryDiffusion
A jolly plump chef wearing a stained apron and white hat laughs as a golden dog with a frisbee and a white cat with a blue collar play in a sunny, grassy backyard.
A golden dog with a frisbee joyfully runs towards an elegant woman in a red evening gown and a dapper man with a beard, holding a pocket watch, in a lush green park.
A mischievous boy with a slingshot winks, poised to flick, while an aristocratic white cat with a monocle and bow tie sits unamused, tail flicking.
A futuristic astronaut with their visor up is reading a book beside a Siamese cat lounging comfortably on top of another closed book in a cozy, dimly lit room.
Fig. 6. Qualitative comparisons of our DreamStory with SOTA approaches on the synthetic benchmark. Ours, MuDI, and ConsiStory utilize the subject image on the left as the reference image. In contrast, StoryDiffusion references the subject image on the right. Different subjects are indicated with different colors. Our method better maintains consistency across multiple subjects, such as the cat in the first row, the color of the man’s suit and the woman’s hair in the second row, the hair color of the boy in the third row, and the head of the astronaut in the fourth row.
TABLE 2. Quantitative comparison on benchmark. Red and blue indicate the best and the second-best performance.
2-Subject 3-Subject
AES↑ CLIP-T↑ DS↑ D&C-DS↑ AES↑ CLIP-T↑ DS↑ D&C-DS↑ MuDI [24] 6.47 0.3652 0.6578 0.4410 6.54 0.3664 0.5924 0.1988 ConsiStory [27] 6.62 0.3757 0.5988 0.4251 6.73 0.3770 0.5564 0.2038 StoryDiffusion [31] 6.56 0.3702 0.6258 0.4364 6.57 0.3707 0.5723 0.2095 DreamStory (Ours) 6.72 0.3779 0.6714 0.5444 6.81 0.3791 0.5965 0.2335
capabilities. Interaction with the LLM is conducted via their API 5.
Diffusion Model Backbone. We first conducted an ablation study on two popular T2I backbones, Playground 6 and SDXL 7. The results are presented in Tab. 1. We adopt Playground as the final T2I backbone due to its excellent performance in aesthetics and subject consistency. We utilize the default scheduler (EDMDPMSolverMultistepScheduler [93]) with 50 inference steps to ensure optimal performance during the inference phase. The guidance scale [94] is set to 7.0 in our experiments. The weight of text feature injection, λ, is fixed to 0.9 for a tradeoff between scene semantics and consistency of subjects. The evaluation of all our models focuses on generating visual content with dimensions of 1280 (width) by 768 (height).
Attention Mechanisms in MSD. Our MSD is applied
5. openai:gpt-4-turbo-preview 6. playground-v2.5-1024px-aesthetic 7. SDXL-base-1.0
across all diffusion steps to ensure multi-subject consistency. The masked mutual self-attention is applied to all decoder layers to maintain the appearance consistency, followed by previous works [26], [27]. Inspired by previous work [30], [88], the masked mutual cross-attention is applied to all layers for better cross-attention fusion. The dropout [27] strategy with a dropout rate of 0.5 is adopted to enhance layout diversity. Furthermore, we adopt the open-vocabulary segmentation model, GroundingSAM [89], to generate precise masks for the subjects. This process begins with the detection of the subject using the open-vocabulary detection model, GroundingDINO [92], followed by segmentation with the powerful SAM [95].
4.4 Comparison with SOTA Methods
To demonstrate the advantage of the proposed DreamStory, we compare our DreamStory with the state-of-the-art approaches. These methods fall into two main categories: (1) MuDI [24], fine-tuned using reference images; and


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
DreamStory (Ours) With MMSA With MMCA Baseline
An elegant woman in a red evening gown and a dapper man with a beard and pocket watch admire an aristocratic white cat wearing a monocle and bow tie, seated on a luxurious velvet cushion.
A mischievous boy with a slingshot teams up with a one-legged bearded pirate, plotting their next prank. Nearby, a Siamese cat lounges atop a dusty treasure map, indifferent to the chaos around it.
A white cat with a blue collar sits beside a pirate captain with an eye patch and a hook, both gazing out at a calm sea from the wooden deck of a ship.
A mischievous boy with a slingshot hides behind a tree, aiming at a dapper man with a beard and pocket watch, who is checking the time, unaware of the boy's antics.
Fig. 7. Ablation studies of different generation results. All settings except the baseline utilize the subject image on the left as the reference image. Different subjects are indicated with different colors. Our method better maintains consistency across multiple subjects, such as the woman and cat in the first row, the pirate and Siamese cat in the second row, the pirate and cat in the third row, and the boy in the last row.
TABLE 3. Quantitative results of ablation study on the benchmark. Red and blue indicate the best and the second-best performance. 2-Subject 3-Subject
AES↑ CLIP-T↑ DS↑ D&C-DS↑ AES↑ CLIP-T↑ DS↑ D&C-DS↑ Baseline 6.67 0.3818 0.5796 0.3996 6.77 0.3841 0.5194 0.1938 w/ MMCA 6.68 0.3791 0.6673 0.5301 6.80 0.3772 0.5888 0.2186 w/ MMSA 6.69 0.3800 0.5922 0.4233 6.76 0.3852 0.5293 0.2098 w/ MMSA+MMCA (Ours) 6.72 0.3779 0.6714 0.5444 6.81 0.3791 0.5965 0.2335
(2) training-free methods, ConsiStory [27] and StoryDiffusion [31]. All the approaches are tested under the same setting for a fair comparison.
4.4.1 Objective Comparison
The overall results are presented in Tab. 2. As can be seen from the table, our DreamStory outperforms other methods in terms of all metrics. Notably, the D&C-DS metric of ours is significantly surpassed other methods, exceeding MuDI, ConsiStory, and StoryDiffusion by margins of 0.1034(23.4%), 0.1293(25.1%), and 0.1080(24.7%) respectively in the 2-Subject of DS-500 benchmark. This pattern is mirrored in the 3-Subject benchmark, reinforcing the effectiveness of our method in maintaining multi-subject consistency. Furthermore, our method exhibits a notable advantage on the DS metric, outperforming other trainingfree SOTA methods (ConsiStory and StoryDiffusion) on the 2Subject benchmark by at least 0.05 (9.0%). This lead, however, narrows to an approximate average of 0.02 (3.9%) on the 3Subject benchmark. This is attributed to the limitations of the diffusion model when generating three subjects, leading to a
higher likelihood of subject fusion, which is also discussed in MuDI [24].
Regarding text similarity, all methods except MuDI [24] yield comparable CLIP-T scores, and our method excels in all benchmarks. In contrast, MuDI has a significant CLIP-T decline (approximately 0.01) in all settings. This decline would worsen if the training continued, leading to problems such as lack of background or single subject dominating [24]. This suggests our method’s effectiveness in maintaining scene semantic consistency while preserving subject consistency. In conclusion, all these results effectively prove that our DreamStory method performs superiorly in maintaining multi-subject consistency, enhancing aesthetics, and preserving scene semantic consistency. Notably, unlike MuDI, our training-free MSD eliminates the extra training costs and the potential for overfitting. This latter issue could notably degrade the aesthetic appeal of the generated scene images and their alignments with the corresponding text, which are crucial metrics in story visualization.


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
Once upon a time, the yellow-haired boy met a white dog, and they became good friends
Every day, the yellowhaired boy and the white dog play happily on the grass
One day, the yellowhaired boy and the white dog crossed the small river near their home
The yellow-haired boy and the white dog found a forest and decided to start a new journey
Reference Subjects Portraits
DreamStory (Ours)
Real-Style
DreamStory (Ours)
Anime-Style
StoryGen
Fig. 8. Qualitative comparisons of our DreamStory with StoryGen on their benchmark. All approaches utilize the subject image on the left as the reference image. Different subjects are indicated with different colors. The narrative text of the story is presented below and serves as the input for our DreamStory. Please refer to the supplementary materials to watch the video.
A boy walks into the playground.
The boy saw a girl on the playground and wanted to talk.
The girl walked away because she didn't like him.
The boy was very sad.
DreamStory (Ours) Real-Style
DreamStory (Ours) Anime-Style
TheaterGen
Fig. 9. Qualitative comparisons of our DreamStory with TheaterGen on their benchmark. Different subjects are indicated with different colors. The narrative text of the story is presented on the left and serves as the input for our DreamStory. Please refer to the supplementary materials to watch the video.
4.4.2 Subjective Comparison
The overall subjective comparison results are presented in Fig. 13(a)(b)(c). It can be seen from Fig. 13 that over 80% evaluators believe that our DreamStory surpasses or is comparable to ConsiStory [27], MuDI [24], and StoryDiffusion [31] in all benchmarks at all criteria. To demonstrate the performance of our overall framework, we present the visual
Scene Prompt Subject Prompt Is in
scene?
LLM Prediction A young boy with glasses and a bearded pirate with a wooden leg are examining a treasure map together under the flickering light of a lantern on a sandy beach at sunset.
Yes No
No Yes
A young boy with messy, jet-black hair, wide, curious eyes, sporting oversized glasses and a superhero t-shirt.
A mischievous boy with a slingshot teams up with a one-eyed pirate captain with a hook, plotting their next adventure on a treasure map.
A fierce, bearded pirate sporting a colorful bandana, a wooden leg, and an ornate cutlass hanging from his belt.
Fig. 10. Failure Cases of LLM. Baseline
A mischievous boy with a slingshot teams up with a distinguished man wearing a monocle, both laughing, while a golden dog with a frisbee jumps playfully between them in a sunny, grassy park.
40
DreamStory (Ours)
A Siamese cat lounges on a dusty book beside a bearded pirate with a wooden leg who chats with a dapper man sporting a beard and pocket watch, all gathered around an old wooden table in a dimly lit, cozy tavern.
Fig. 11. Failure Cases of Diffusion Model. Different subjects are indicated with different colors.
results of two complete stories compared with other SOTA methods in Fig. 4 and Fig. 5. From these tables, we can see that our framework is capable of effectively annotating the subjects in the scene that need to maintain consistency and generating images that maintain multi-subject consistency, such as the protagonist man and his daughter in Fig. 4, the Naruto and ninja cat in Fig. 5. To further illustrate the advantages of our approach in preserving multi-subject consistency, we also display more comparison visual results


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
Generated Scene Images
Subject-1: A distinguished man with a neatly trimmed silver beard, monocle perched on his right eye, dressed in a tailored three-piece velvet suit.
Subject-2: A futuristic astronaut in a sleek white suit with blue lights, visor up, revealing a face filled with wonder.
Scene: A futuristic astronaut with their visor up converses with a distinguished man wearing a monocle, both standing against the backdrop of a starry space vista.
Subject-1: A cheerful, portly chef with a towering white hat, flour-dusted apron.
Subject-2: A tall, graceful woman with silverstreaked ebony hair, wearing a sleek, red evening gown and a mysterious smile.
Scene: An elegant woman in a red evening gown and a portly chef with a white hat share a toast in a dimly lit, luxurious kitchen, surrounded by culinary delights.
Prompts Reference Subjects Portraits
Fig. 12. Qualitative results of our DreamStory with a different random seed. The first two rows display the same set of subjects in real style with different seeds; the last two rows present another set in anime style. Each row contains two subject images on the left and three scene images on the right, which are generated based on the left subjects. Different subjects are indicated with different colors. These results demonstrate the superior consistency and diversity of our DreamStory across various styles.
Aesthetic Ours is better Comparable Other is better
T-I Alignment Ours is better Comparable Other is better
Subject-Consistency Ours is better Comparable Other is better
(e) Ablation Study in 3-Subject Benchmark
(a) Comparing SOTA Methods in Real-Story Benchmark
(b) Comparing SOTA Methods in 2-Subject Benchmark
(d) Ablation Study in 2-Subject Benchmark
(c) Comparing SOTA Methods in 3-Subject Benchmark
Fig. 13. User Study on DS-500 benchmark. Dominant preferences to our full model are presented, compared with other competitive baselines (a, b, c) and ablation models (d, e). T-I Alignment means text-image relevance.
on the synthetic benchmark in Fig. 6. Furthermore, we apply our DreamStory to a case from StoryGen [25]. We control the style of the generated results by adding style prompts, e.g., anime style, as shown in Fig. 8. As seen from Fig. 8, our method maintains better consistency while achieving a higher aesthetic appeal. Our training-free approach effectively leverages existing large, high-quality datasets (e.g., LAION-5B [45]), overcoming the shortcomings of current story visualization datasets, which are smaller and lower quality. Therefore, our method outperforms StoryGen in aesthetics and can be applied to the open-domain where the subjects and styles are considerably diverse. We also conduct the qualitative comparisons with concurrent work TheaterGen [79] on their benchmark, as shown in Fig. 9. Two styles (anime style and real style) are also applied to our DreamStory to show its performance. As we can see from Fig. 9, our DreamStory can generate more aesthetically pleasing images while maintaining better consistency across multiple subjects, such as the clothes of the boy and girl in the figure. We present subject and scene images generated by our DreamStory with different seeds under both real-style and anime-style conditions, as illustrated in Fig. 12. These extensive experimental results also prove the advantages of our DreamStory.
4.5 Ablation Studies
We also conduct ablation studies in our benchmark to verify the effectiveness of each of our modules in MSD.
TABLE 4. Quantitative results of LLM accuracy (%) on benchmark 0-Subject 1-Subject 2-Subject 3-Subject LLaMA3-1B [96] 100.00 98.89 87.44 80.13 LLaMA3-3B [96] 100.00 98.88 91.44 84.67 ChatGPT4 [32] 100.00 98.86 95.29 91.28
We integrated these two modules, Masked Mutual SelfAttention (MMSA) and Masked Mutual Cross-Attention (MMCA), individually into the baseline, i.e., Playground. All the settings are compared from both subjective and objective perspectives as described in Sec. 4.2.
4.5.1 Objective Comparison
All the objective results are presented in Tab. 3. It is evident from Tab. 3 that adding MMSA and MMCA improved subject consistency, as indicated by an increase in DreamSim similarity (DS) and D&C-DS. However, a minor decline was observed in the similarity between the scene and its text. This is attributed to our generation process’s emphasis on subject consistency, which marginally affects the scene’s content. Nonetheless, this impact is negligible, i.e., the difference in CLIP-T similarity is less than 0.007. These results confirm that our method maintains subject consistency while preserving the scene’s semantics. Moreover, Tab. 3 reveals that incorporating both MMSA and MMCA modules led to our DreamStory achieving optimal performance regarding aesthetic scores and subject consistency. This conclusively validates the effectiveness of our approach.


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
Scene Subject
Maru: An energetic Ninja Monkey with a coat of earthy browns and beiges, sparkling intelligent eyes hidden behind a playful demeanor, and an agile form exuding mischievous confidence.
With ChatGPT4
Shen: A majestic Ninja Panda with fur as black as the night sky, contrasting with pristine white chest and limbs, deep wise eyes reflecting calmness, and a graceful posture.
Shen and Maru in the Forest: Ninja panda with deep wise eyes and a playful monkey stand together in a dense, lush green forest illuminated by slivers of sunlight.
The Return of the Crystal of Harmony: The Crystal of Harmony glows warmly on its pedestal in the ancient temple. Beside it, a wise ninja panda and a playful ninja monkey stand against a backdrop of lush green forest.
Maru: A nimble monkey with earthy brown fur and bright eyes, leaping through a vibrant forest canopy with a playful expression.
With LLaMA3-3B
Shen: A majestic black panda with a pristine white chest and limbs, standing proudly in a misty forest.
Introduction of Shen and Maru: A young panda and ninja monkey standing in front of a lush, verdant forest.
Retrieval of the Crystal of Harmony: A nimble monkey and a majestic black panda standing victorious, holding the Crystal of Harmony, with the forest in the background, bathed in a warm, healing light.
Maru: An energetic and playful monkey with sparkling mischievous eyes and a hint of a warm smile.
With LLaMA3-1B
Shen: A majestic and serene panda with wise eyes and a calm demeanor, blending seamlessly into the lush forest.
Shen and Maru's Journey to the Crystal of Harmony: The panda and monkey embarks on a perilous journey, facing treacherous terrain and numerous challenges that test their bond and skills.
Shen and Maru's Retrieval of the Crystal of Harmony: Using their unique skills and teamwork, the panda and monkey outmaneuver Kuro and retrieve the Crystal of Harmony.
Fig. 14. Qualitative results of our DreamStory with different LLMs (ChatGPT4, LLaMA3-3B, and LLaMA3-1B). The first two rows are the text and images of the subject, and the last two rows are those of the scenes. Different subjects are indicated with different colors. DreamStory generates visually appealing story scenes while maintaining consistency across multiple subjects, demonstrating the robustness of our framework.
Without Rewriting
Naruto and Tai, back-to-back, engage in combat with a rogue ninja amidst the dense green bamboo of the forest, with Naruto preparing a ninjutsu and Tai in a martial arts stance.
With Rewriting
The young ninja with spiky blond hair and the panda warrior, back-to-back, engage in combat with a rogue ninja amidst the dense green bamboo of the forest, with the young ninja with spiky blond hair preparing a ninjutsu and the panda warrior in a martial arts stance.
Kondo and Rex walk side by side through the lush rainforest, showcasing their growing bond amidst the vibrant greenery and diverse wildlife.
A towering gorilla and a golden-furred dog warrior walk side by side through the lush rainforest, showcasing their growing bond amidst the vibrant greenery and diverse wildlife.
Fig. 15. Ablation study of LLM rewriting. Different subjects are indicated with different colors. Without rewriting, the diffusion model may generate incorrect subjects, such as generating the panda Tai into a ninja man (first row), and turning the gorilla Kondo into a human (second row).
4.5.2 Subjective Comparison
We present the user study result of our DreamStory compared to our different settings, baseline, with MMCA and with MMSA in Fig. 13(d)(e). It can be seen from Fig. 13 that the evaluators prefer DreamStory to the other settings. We also show the visual results of ablation studies in Fig. 7 to show the effectiveness of each component. As Fig. 7 illustrates, the approach without the MMSA module has the potential to generate images with blending subjects, particularly when two subjects are close within the image, as seen in the first row with the man and woman, and the second row with the pirate and boy. Furthermore, without the MMCA module, there is a significant discrepancy in appearance between the generated subject’s portraits and the reference image, as demonstrated in the second row’s pirate and boy and the fourth row’s boy and man. This discrepancy can be attributed to two factors. Firstly, the subject’s text contains rich appearance information about the target subject, which
is aligned in the semantic space of the diffusion model. This alignment comes from the fact that the subject’s reference portrait is generated by the same diffusion model using this text. Secondly, the lack of detailed text descriptions can lead to a substantial difference between the subject and the reference image during the generation process of the scene image. This discrepancy can exacerbate the problem during the self-attention computation, as it hinders the identification of the correct patches when calculating patch similarity, resulting in a significant difference in appearance. Finally, our DreamStory achieves the best aesthetic and subject similarity performance by including both modules. This strongly validates the effectiveness of our method. We also conducted an ablation study about rewriting and showed the result in Fig. 15. As illustrated in Fig. 15, without rewriting, the diffusion model fails to understand the names, such as Kondo. This leads to generating incorrect types of subjects, such as generating gorillas as humans, as shown in the second row of Fig. 15.
4.6 Systematic Analysis
In this subsection, we provide a comprehensive analysis to validate the performance and robustness of DreamStory. This includes a series of degradation tests and time-efficiency evaluations, ensuring a thorough assessment of our approach.
4.6.1 Performance with Varying LLMs
To assess the performance of DreamStory, we replaced two smaller LLMs (LLaMA3.2 [96]) and evaluated their accuracy in subject annotation. As shown in Tab.4, all LLMs achieved over 98% accuracy on the 0-Subject and 1-Subject benchmarks. For the 2-Subject and 3-Subject benchmarks, accuracy decreased slightly with smaller model sizes but remained above 80%. Among the models tested, the largest LLM, ChatGPT4 [32], outperformed the others, achieving approximately 95% accuracy for the 2-Subject and 91% for the 3-Subject benchmarks. Additionally, all LLMs successfully generated prompts for subjects and scenes that aligned with the story, likely due to their robust capabilities in text


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14 TABLE 5. Comparison results of different segmentation models. Similar performance across various segmentation models demonstrates the robustness and efficiency of our DreamStory.
2-Subject 3-Subject
AES↑ CLIP-T↑ DS↑ D&C-DS↑ AES↑ CLIP-T↑ DS↑ D&C-DS↑ w/ Grounded-Light-HQSAM [97] 6.71 0.3774 0.6725 0.5406 6.82 0.3792 0.5962 0.2327 w/ Grounded-MobileSAM [98] 6.72 0.3789 0.6707 0.5386 6.83 0.3799 0.5929 0.2284 w/ Grounded-SAM [89] (Ours) 6.72 0.3779 0.6714 0.5444 6.81 0.3791 0.5965 0.2335
Scene Image Scene Mask Reference Mask
With GroundedLight-HQSAM
With GroundedMobileSAM
With Grounded-SAM (Ours)
Fig. 16. Qualitative comparisons of our DreamStory with different segmentation models. Despite variations in masks produced by different SAM models, the final generated scene images remain nearly identical. This demonstrates the robustness of our method, showing its adaptability to different segmentation models.
understanding and summarization—well-established tasks within LLM training data. ChatGPT, in particular, excelled at instruction-following, making it the model of choice for our approach. Fig. 14 illustrates final story images generated using different LLMs, further emphasizing the robustness of DreamStory.
4.6.2 Performance with Varying SAMs
We also evaluated DreamStory’s performance with different SAM models, specifically Light-HQSAM [97] and MobileSAM [98]. The results, presented in Tab.5, show that performance degradation with smaller SAMs across various metrics is minimal. Furthermore, Fig.16 displays example masks generated by different SAMs, along with their corresponding final scene images. While smaller SAMs occasionally miss fine details (such as the sleeves of wizard robes in the first two rows), the differences in the final generated images are negligible. These results affirm the robustness and high performance of DreamStory across varying SAM sizes.
4.6.3 Time Efficiency Analysis
The runtime of the DreamStory framework for generating a single story is primarily determined by the time required for the LLM to process requests, which typically takes 3 to 4 minutes, accounting for about 60% of the total time. This time consumption is largely due to the waiting period for network-based API responses, which could be accelerated in future implementations. In comparison, the image generation phase is faster, averaging 20 to 30 seconds per image. Additionally, the time required for SAM to generate masks is minimal, averaging less than 0.5 seconds, with negligible impact on
TABLE 6. Average time(s) Consumption on different methods. It includes the time for generating reference subjects and the final scene. Additionally, MUDI requires extra fine-tuning time for each case.
2-Subject 3-Subject MuDI [24] 5400 7200 ConsiStory [27] 30 38 StoryDiffusion [31] 21 25 DreamStory(Ours) 22 28
the overall processing time. Tab. 6 summarizes the time required to generate a scene image using different methods. For finetuning-based approaches (e.g., MuDI), approximately 1.5 hours of finetuning is required for 2-subject scenarios, and 2 hours for 3-subject scenarios. In contrast, DreamStory generates each scene image in about 25 seconds, similar to other training-free approaches, highlighting its exceptional time efficiency.
4.7 Limitations and Failure Cases
Our method relies on the abilities of both the LLM and diffusion model. Firstly, LLM may have hallucinations when labeling whether the subject is in the scene. As shown in Fig. 10, the scene includes a boy, but the LLM failed to identify the boy. In addition, the LLM may not effectively distinguish between subjects with similar descriptions, such as the pirate and pirate captain in the second row of Fig. 10. Finally, diffusion models suffer from semantic understanding issues, which may be difficult in multi-subject and multiattribute generation [24]. As shown in Fig. 11, when the diffusion model failed to generate three subjects for the first time, our method also failed to generate three subjects with consistent appearances. Despite these limitations, our framework still has promising potential as individual models evolve and progress.
5 CONCLUSION
This paper introduced an automatic training-free opendomain story visualization framework, DreamStory. It leverages Language Models (LLMs) as a story director to generate concise prompts for subjects and scenes, annotating the subjects in each scene. This information guides diffusion models in creating visually consistent content that aligns with the story narrative. We also developed a novel MultiSubject consistent Diffusion model (MSD) that leverages both the subject prompt and its corresponding portrait to maintain consistency in multiple subjects across frames. To validate our approach and promote progress in story visualization, we established an evaluation benchmark, DS-500. Our method outperforms previous methods in aesthetics, image-text alignment, and subject consistency through objective and subjective evaluations. In conclusion, our DreamStory method represents a significant step forward as a framework for open-domain


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15
story visualization. It does not require additional training and is poised to enhance its performance as the underlying models evolve. This positions our framework for promising advancements in story visualization.
REFERENCES
[1] C. Klimmt, C. Roth, I. Vermeulen, P. Vorderer, and F. S. Roth, “Forecasting the experience of future entertainment technology: “interactive storytelling” and media enjoyment,” Games and Culture, vol. 7, no. 3, pp. 187–208, 2012. [2] D. Kostons and B. B. de Koning, “Does visualization affect monitoring accuracy, restudy choice, and comprehension scores of students in primary education?” Contemporary Educational Psychology, vol. 51, pp. 1–10, 2017. [3] K. Carter, “The place of story in the study of teaching and teacher education,” Educational researcher, vol. 22, no. 1, pp. 5–18, 1993. [4] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” in ICLR, 2020. [5] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, “Score-based generative modeling through stochastic differential equations,” in ICLR, 2020. [6] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in CVPR, 2022, pp. 10 684–10 695. [7] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical text-conditional image generation with CLIP latents,” arXiv preprint arXiv:2204.06125, 2022. [8] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach, “SDXL: Improving latent diffusion models for high-resolution image synthesis,” arXiv preprint arXiv:2307.01952, 2023. [9] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi, “Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation,” 2024. [10] G. Sun, W. Liang, J. Dong, J. Li, Z. Ding, and Y. Cong, “Create your world: Lifelong text-to-image diffusion,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [11] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion models in vision: A survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [12] Z. Liu, P. Dai, R. Li, X. Qi, and C.-W. Fu, “DreamStone: Image as a stepping stone for text-guided 3d shape generation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 12, pp. 14 385–14 403, 2023. [13] F. Zhan, Y. Yu, R. Wu, J. Zhang, S. Lu, L. Liu, A. Kortylewski, C. Theobalt, and E. Xing, “Multimodal image synthesis and editing: The generative AI era,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 12, pp. 15 098–15 119, 2023. [14] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu, “MotionDiffuse: Text-driven human motion generation with diffusion model,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 6, pp. 4115–4128, 2024. [15] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia, “LongLoRA: Efficient fine-tuning of long-context large language models,” arXiv preprint arXiv:2309.12307, 2023. [16] C. Meister, S. Lazov, I. Augenstein, and R. Cotterell, “Is sparse attention more interpretable?” in ACL/IJCNLP. Association for Computational Linguistics, 2021, pp. 122–129. [17] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv preprint arXiv:2303.18223, 2023. [18] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large language models,” NeurIPS, vol. 35, pp. 24 824–24 837, 2022. [19] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-finetuned language models,” Journal of Machine Learning Research, vol. 25, no. 70, pp. 1–53, 2024. [20] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” NeurIPS, vol. 36, 2024. [21] O. Avrahami, A. Hertz, Y. Vinker, M. Arar, S. Fruchter, O. Fried, D. Cohen-Or, and D. Lischinski, “The chosen one: Consistent characters in text-to-image diffusion models,” arXiv preprint arXiv:2311.10093, 2023.
[22] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang, “IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models,” arXiv preprint arXiv:2308.06721, 2023. [23] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, “DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation,” in CVPR, 2023, pp. 22 500–22 510. [24] S. Jang, J. Jo, K. Lee, and S. J. Hwang, “Identity decoupling for multi-subject personalization of text-to-image models,” in NeurIPS, 2024. [25] C. Liu, H. Wu, Y. Zhong, X. Zhang, Y. Wang, and W. Xie, “Intelligent grimm-open-ended visual storytelling via latent diffusion models,” in CVPR, 2024, pp. 6190–6200. [26] M. Cao, X. Wang, Z. Qi, Y. Shan, X. Qie, and Y. Zheng, “MasaCtrl: Tuning-free mutual self-attention control for consistent image synthesis and editing,” in ICCV, 2023, pp. 22 560–22 570. [27] Y. Tewel, O. Kaduri, R. Gal, Y. Kasten, L. Wolf, G. Chechik, and Y. Atzmon, “Training-free consistent text-to-image generation,” TOG, vol. 43, no. 4, pp. 1–18, 2024. [28] Y. Li, Z. Gan, Y. Shen, J. Liu, Y. Cheng, Y. Wu, L. Carin, D. Carlson, and J. Gao, “StoryGAN: A sequential conditional gan for story visualization,” in CVPR, June 2019. [29] T. Gupta, D. Schwenk, A. Farhadi, D. Hoiem, and A. Kembhavi, “Imagine this! scripts to compositions to videos,” in ECCV, 2018, pp. 598–613. [30] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in ICCV, 2023, pp. 3836–3847. [31] Y. Zhou, D. Zhou, M.-M. Cheng, J. Feng, and Q. Hou, “StoryDiffusion: Consistent self-attention for long-range image and video generation,” 2024. [32] OpenAI, “GPT-4 technical report,” 2023. [33] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang et al., “Yi: Open foundation models by 01. ai,” arXiv preprint arXiv:2403.04652, 2024. [34] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [35] Y.-Z. Song, Z. Rui Tam, H.-J. Chen, H.-H. Lu, and H.-H. Shuai, “Character-preserving coherent story visualization,” in ECCV. Springer, 2020, pp. 18–33. [36] B. Li, P. H. Torr, and T. Lukasiewicz, “Clustering generative adversarial networks for story visualization,” in ACM MM, 2022, pp. 769–778. [37] Y. Ma, H. Yang, B. Liu, J. Fu, and J. Liu, “AI illustrator: Translating raw descriptions into images by prompt-based cross-modal generation,” in ACM MM, 2022, pp. 4282–4290. [38] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein, “Unrolled generative adversarial networks,” in ICLR, 2017. [39] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial networks,” in ICML, 2017, pp. 214–223. [40] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of Wasserstein GANs,” NeurIPS, vol. 30, 2017. [41] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” NeurIPS, vol. 33, pp. 6840–6851, 2020. [42] A. Q. Nichol and P. Dhariwal, “Improved denoising diffusion probabilistic models,” in ICML, 2021, pp. 8162–8171. [43] P. Dhariwal and A. Nichol, “Diffusion models beat GANs on image synthesis,” NeurIPS, vol. 34, pp. 8780–8794, 2021. [44] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet, and M. Norouzi, “Palette: Image-to-image diffusion models,” in ACM SIGGRAPH, 2022, pp. 1–10. [45] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., “LAION-5B: An open large-scale dataset for training next generation image-text models,” arXiv preprint arXiv:2210.08402, 2022. [46] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu et al., “PixArt-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis,” arXiv preprint arXiv:2310.00426, 2023. [47] A. Maharana and M. Bansal, “Integrating visuospatial, linguistic, and commonsense structure into story visualization,” in EMNLP, 2021, pp. 6772–6786. [48] A. Maharana, D. Hannan, and M. Bansal, “Improving generation and evaluation of visual stories via semantic consistency,” in NAACL HLT, 2021, pp. 2427–2442. [49] H. Chen, R. Han, T.-L. Wu, H. Nakayama, and N. Peng, “Charactercentric story visualization via visual planning and token alignment,” in EMNLP, 2022, pp. 8259–8272.


JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16
[50] T. Rahman, H.-Y. Lee, J. Ren, S. Tulyakov, S. Mahajan, and L. Sigal, “Make-a-Story: Visual memory conditioned consistent story generation,” in CVPR, June 2023, pp. 2493–2502. [51] X. Gu, Y. Sun, F. Ni, S. Chen, X. Wang, R. Song, B. Li, and X. Cao, “TeViS: Translating text synopses to video storyboards,” in ACM MM, 2023, pp. 4968–4979. [52] A. Maharana, D. Hannan, and M. Bansal, “StoryDALL-E: Adapting pretrained text-to-image transformers for story continuation,” in ECCV. Springer, 2022, pp. 70–87. [53] X. Pan, P. Qin, Y. Li, H. Xue, and W. Chen, “Synthesizing coherent story with auto-regressive latent diffusion models,” in WACV, 2024, pp. 2920–2930. [54] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable visual models from natural language supervision,” in ICML, 2021, pp. 8748–8763. [55] J. Li, D. Li, C. Xiong, and S. Hoi, “BLIP: bootstrapping languageimage pre-training for unified vision-language understanding and generation,” in ICML. PMLR, 2022, pp. 12 888–12 900. [56] J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models,” in ICML. PMLR, 2023, pp. 19 730–19 742. [57] X. Peng, J. Zhu, B. Jiang, Y. Tai, D. Luo, J. Zhang, W. Lin, T. Jin, C. Wang, and R. Ji, “PortraitBooth: A versatile portrait model for fast identity-preserved personalization,” in CVPR, June 2024, pp. 27 080–27 090. [58] C. Zhu, K. Li, Y. Ma, C. He, and L. Xiu, “MultiBooth: Towards generating all your concepts in an image from text,” arXiv preprint arXiv:2404.14239, 2024. [59] K. Lee, S. Kwak, K. Sohn, and J. Shin, “Direct consistency optimization for compositional text-to-image personalization,” arXiv preprint arXiv:2402.12004, 2024. [60] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., “LoRA: Low-rank adaptation of large language models,” in ICLR, 2021. [61] M. Arar, R. Gal, Y. Atzmon, G. Chechik, D. Cohen-Or, A. Shamir, and A. H. Bermano, “Domain-agnostic tuning-encoder for fast personalization of text-to-image models,” in SIGGRAPH Asia, 2023, pp. 1–10. [62] S. Cui, J. Guo, X. An, J. Deng, Y. Zhao, X. Wei, and Z. Feng, “IDAdapter: Learning mixed features for tuning-free personalization of text-to-image models,” in CVPR Workshops, June 2024, pp. 950–959. [63] X. Chen, L. Huang, Y. Liu, Y. Shen, D. Zhao, and H. Zhao, “AnyDoor: Zero-shot object-level image customization,” in CVPR, June 2024, pp. 6593–6602. [64] Z. Li, M. Cao, X. Wang, Z. Qi, M.-M. Cheng, and Y. Shan, “PhotoMaker: Customizing realistic human photos via stacked id embedding,” in CVPR, June 2024, pp. 8640–8650. [65] D. Li, J. Li, and S. Hoi, “BLIP-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing,” NeurIPS, vol. 36, 2024. [66] L. Tang, M. Jia, Q. Wang, C. P. Phoo, and B. Hariharan, “Emergent correspondence from image diffusion,” NeurIPS, vol. 36, pp. 13631389, 2023. [67] Z. Cao, F. Wei, W. Li, and S. Li, “Faithful to the original: Fact aware neural abstractive summarization,” in AAAI, vol. 32, no. 1, 2018. [68] M. Gui, J. Tian, R. Wang, and Z. Yang, “Attention optimization for abstractive document summarization,” in EMNLP-IJCNLP 2019, 2019, pp. 1222–1228. [69] S. Wang, M. Yu, X. Guo, Z. Wang, T. Klinger, W. Zhang, S. Chang,
G. Tesauro, B. Zhou, and J. Jiang, “R3: Reinforced ranker-reader for open-domain question answering,” in AAAI, vol. 32, no. 1, 2018. [70] S. Min, V. Zhong, R. Socher, and C. Xiong, “Efficient and robust question answering from minimal context over documents,” in ACL, 2018, pp. 1725–1735. [71] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,” NeurIPS, vol. 35, pp. 27 730–27 744, 2022. [72] E. Segalis, D. Valevski, D. Lumen, Y. Matias, and Y. Leviathan, “A picture is worth a thousand words: Principled recaptioning improves image generation,” arXiv preprint arXiv:2310.16656, 2023. [73] S. Wen, G. Fang, R. Zhang, P. Gao, H. Dong, and D. Metaxas, “Improving compositional text-to-image generation with large vision-language models,” arXiv preprint arXiv:2310.06311, 2023.
[74] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts et al., “Stable video diffusion: Scaling latent video diffusion models to large datasets,” arXiv preprint arXiv:2311.15127, 2023. [75] Y. Hao, Z. Chi, L. Dong, and F. Wei, “Optimizing prompts for text-to-image generation,” NeurIPS, vol. 36, 2024. [76] H. He, T. Wang, H. Yang, J. Fu, N. J. Yuan, J. Yin, H. Chao, and Q. Zhang, “Learning profitable NFT image diffusions via multiple visual-policy guided reinforcement learning,” in ACM MM, 2023, pp. 6831–6840. [77] J. Cheng, X. Lu, H. Li, K. L. Zai, B. Yin, Y. Cheng, Y. Yan, and X. Liang, “AutoStudio: Crafting consistent subjects in multi-turn interactive image generation,” arXiv preprint arXiv:2406.01388, 2024. [78] L. Yang, Z. Yu, C. Meng, M. Xu, S. Ermon, and C. Bin, “Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal LLMs,” in ICML, 2024. [79] J. Cheng, B. Yin, K. Cai, M. Huang, H. Li, Y. He, X. Lu, Y. Li, Y. Li, Y. Cheng et al., “TheaterGen: Character management with LLM for consistent multi-turn image generation,” arXiv preprint arXiv:2404.18919, 2024. [80] W. Ren, H. Yang, G. Zhang, C. Wei, X. Du, W. Huang, and W. Chen, “ConsistI2V: Enhancing visual consistency for imageto-video generation,” TMLR, 2024. [81] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large language models,” NeurIPS, vol. 35, pp. 24 824–24 837, 2022. [82] B. Wang, S. Min, X. Deng, J. Shen, Y. Wu, L. Zettlemoyer, and H. Sun, “Towards understanding chain-of-thought prompting: An empirical study of what matters,” in ACL, 2023, pp. 2717–2739. [83] G. Feng, B. Zhang, Y. Gu, H. Ye, D. He, and L. Wang, “Towards revealing the mystery behind chain of thought: a theoretical perspective,” NeurIPS, vol. 36, 2024. [84] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang, W. Peng, M. Liu, B. Qin, and T. Liu, “A survey of chain of thought reasoning: Advances, frontiers and future,” arXiv preprint arXiv:2309.15402, 2023. [85] Y. Alaluf, D. Garibi, O. Patashnik, H. Averbuch-Elor, and D. CohenOr, “Cross-image attention for zero-shot appearance transfer,” arXiv preprint arXiv:2311.03335, 2023. [86] H. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or, “Attendand-excite: Attention-based semantic guidance for text-to-image diffusion models,” TOG, vol. 42, no. 4, pp. 1–10, 2023. [87] D. Shen, G. Song, Z. Xue, F.-Y. Wang, and Y. Liu, “Rethinking the spatial inconsistency in classifier-free diffusion guidance,” arXiv preprint arXiv:2404.05384, 2024. [88] M. Chen, I. Laina, and A. Vedaldi, “Training-free layout control with cross-attention guidance,” in WACV, 2024, pp. 5343–5353. [89] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan et al., “Grounded SAM: Assembling open-world models for diverse visual tasks,” arXiv preprint arXiv:2401.14159, 2024. [90] N. Otsu et al., “A threshold selection method from gray-level histograms,” Automatica, vol. 11, no. 285-296, pp. 23–27, 1975. [91] S. Fu, N. Tamir, S. Sundaram, L. Chai, R. Zhang, T. Dekel, and P. Isola, “DreamSim: Learning new dimensions of human visual similarity using synthetic data,” NeurIPS, vol. 36, 2024. [92] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu et al., “Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection,” arXiv preprint arXiv:2303.05499, 2023. [93] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, “DPM-solver++: Fast solver for guided sampling of diffusion probabilistic models,” arXiv preprint arXiv:2211.01095, 2022. [94] J. Ho and T. Salimans, “Classifier-free diffusion guidance,” in NeurIPS 2021 Workshop, 2021. [95] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., “Segment anything,” in ICCV, 2023, pp. 4015–4026. [96] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of models,” arXiv preprint arXiv:2407.21783, 2024. [97] L. Ke, M. Ye, M. Danelljan, Y. Liu, Y.-W. Tai, C.-K. Tang, and F. Yu, “Segment anything in high quality,” in NeurIPS, 2023. [98] C. Zhang, D. Han, Y. Qiao, J. U. Kim, S.-H. Bae, S. Lee, and C. S. Hong, “Faster segment anything: Towards lightweight sam for mobile applications,” arXiv preprint arXiv:2306.14289, 2023.