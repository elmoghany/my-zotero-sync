Tuning-Free Multi-Event Long Video Generation
via Synchronized Coupled Sampling
Subin Kim1 Seoung Wug Oh2 Jui-Hsien Wang2 Joon-Young Lee2 Jinwoo Shin1
1KAIST 2Adobe Research
subin-kim@kaist.ac.kr
Local 1 He sweeps back his dreadlocks Local 3 He tilts his head
Local 2 He looks to the side Local 4 He puts his hand on his chest
⋯⋯⋯
12 47 132 170 272 358 422
397
Global prompt A closeup portrait of a young Jamaican man with medium-length dreadlocks, wearing a white linen shirt unbuttoned at the top, revealing a hint of his toned chest, and a thin gold chain around his neck
Local 1 Playing metal guitar, grimace Local 3 Stomping its foot on the stage
Local 2 Grinning widely Local 4 Leaning back dramatically
⋯⋯⋯
9 103 210
166 261 329 386 485
Global prompt Anthropomorphic gorilla playing a heavy metal guitar
0 sec 5 sec 10 sec 15 sec 20 sec
1st chunk 2nd chunk 3rd chunk 4th chunk
⋯⋯⋯
Local 1 The camera moves closer to the cliff’s edge Local 3 Focus on small island with the light house
Local 2 The camera pans to the right off the cliffs Local 4 The camera zooms out to a wide shot
⋯⋯⋯
9 103 166 210 329 485
Global prompt Big Sur’s Gary Point Beach with rugged cliffs and crashing waves. Golden light of the setting sun illuminating the rocky shore
3 36 127 192 257 290 471
380
Figure 1. Multi-event long video generation results using our tuning-free inference framework, SynCoS. Each example is around 21 seconds of video at 24 fps (4× longer than the base model ). Frame indices are displayed in each frame. SynCoS generates high-quality, long videos with multi-event dynamics while achieving both seamless transitions between frames and long-term semantic consistency throughout.
Abstract
While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuningfree approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this,
SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior longrange coherence, outperforming previous approaches both quantitatively and qualitatively. 1
1. Introduction
Recently, text-to-video (T2V) diffusion models have shown impressive capabilities in synthesizing high-quality videos from a single text prompt [3, 11, 20, 23, 34, 55]. However, these models primarily generate short, fixed-length videos that capture a single event, mainly due to high computational training costs and limited high-quality long video data [31, 49, 53]. In contrast, real-world videos are long and comprise multiple events that introduce dynamics, such as object motion and camera movement, all within a coherent scenario. Thus, extending T2V models for long video generation requires handling event transitions while ensuring global coherence throughout the video.
1Visualizations are available at: https://syncos2025.github.io/.
1
arXiv:2503.08605v1 [cs.CV] 11 Mar 2025


1st and 2nd chunk 3rd and 4th chunk 0 sec 30 sec
15 sec
Local 1 The astronaut on Mars walks carefully through a small puddle Local 2 The astronaut on Mars stands still, watching fireworks
Global prompt An astronaut in a white spacesuit in the surreal landscape of Mars. The setting sun casts long shadows and dust particles float in the Martian atmosphere
⋯
SynCoS (Ours) Gen-L-Video
⋯
FIFO
⋯
Overlapping artifacts
Style drift
Consistent in both content and style
Figure 2. Qualitative comparisons on CogVideoX-2B [55]. All examples are 5 times longer in duration compared to the underlying base model, generating a 30-second video. Unlike Gen-L-Video [50] and FIFO-Diffusion [21], which often struggle with overlapping artifacts and style drift, our method, SynCoS, ensures consistency in both content and style throughout the entire video. Additionally, SynCoS generates long videos where each frame faithfully follows its designated prompt while ensuring seamless transitions between frames.
.
Existing methods often struggle to achieve this balance, failing to enforce long-range consistency. Training-based approaches introduce conditioning modules [9, 47, 49] to generate video chunks autoregressively. However, relying solely on previously generated chunks makes maintaining a coherent global structure difficult, and repeated conditioning leads to error accumulation and degraded frame quality.
Alternatively, several works [5, 21, 29, 31, 36, 50] propose inference techniques to extend T2V diffusion models, avoiding additional training and error accumulation. One common and intuitive approach is fusion, which smoothly connects short videos into a longer one by dividing a long video into overlapping chunks, denoising each with different prompts, and applying local fusion to overlapped regions for seamless transitions. However, these methods focus only on local smoothness, failing to integrate information across distant chunks. This issue becomes even more pronounced in multi-prompt scenarios, where denoising paths between chunks diverge more significantly than with a single prompt, resulting in severe inconsistencies in semantics, and style across the generated video (see 1st and 2nd rows in Figure 2).
While local fusion-based inference techniques are promising as they require no additional training, they lack a mechanism to enforce long-range consistency. We argue that maintaining a shared denoising trajectory across both short- and long-distance chunks is essential for generating coherent long videos. To address this, we explore an optimizationbased sampling approach: Collaborative Score Distillation (CSD) [22], which synchronizes information across adjacent and distant samples, enforcing long-range consistency. However, when directly applied to long video generation, CSD completely fails to produce high-quality videos despite aligning denoising trajectories across chunks (see Section 3.2).
In this paper, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework for extending any T2V diffusion model to multi-event long video generation. Our key idea is to jointly preserve local smoothness and global coherence by coupling two complementary samplings. However, a direct combination misaligns their denoising trajectories, weakening prompt guidance and causing unin
tended content variations. To address this, we introduce a synchronized mechanism when coupling them. Specifically, SynCoS integrates Denoising Diffusion Implicit Models (DDIM) [43], built on fusion, to enforce smooth local transitions between adjacent chunks while leveraging CSD to maintain long-range coherence across chunks. To enable synchronous operation when coupling these sampling methods, SynCoS introduces a grounded timestep and fixed baseline noise, ensuring alignment across the entire denoising process. Furthermore, SynCoS proposes a structured prompt to ensure a coherent long scenario with dynamics, combining a global prompt for scenario-wide consistency with local prompts for fine-grained controls. This newly introduced coupled sampling, along with its key synchronization components—grounded timestep and fixed baseline noise—and the structured prompt, defines SynCoS as a new inference framework. As a unified inference framework, SynCoS ensures that local smoothness and global coherence work in tandem, enabling coherent long video generation with multi-event dynamics. We evaluate SynCoS across various challenging scenarios in long video generation, including multiple events (e.g., object motion, compositions, camera movements, background changes, etc.) across different denoising models to comprehensively verify its efficacy. Consistently, SynCoS significantly outperforms existing tuning-free methods in temporal consistency, video quality, and prompt fidelity. Contributions. Our main contributions are as follows: • We propose SynCoS, a novel inference framework that extends any T2V diffusion model for multi-event long video generation without additional tuning. • SynCoS synchronizes two complementary sampling methods to ensure both local transitions and global coherence, introducing a grounded timestep and fixed baseline noise to couple them into a new, unified sampling. • SynCoS incorporates a structured prompt for dynamic yet semantically consistent multi-event generation. • We extensively validate SynCoS on various T2V models across diverse long video generation scenarios, achieving state-of-the-art temporal consistency and prompt fidelity.
2


2. Background
In this section, we present the preliminaries (Section 2.1) and basic concept of fusion for extending T2V diffusion models in a tuning-free manner (Section 2.2).
2.1. Preliminaries
We provide a brief overview of diffusion models, score distillation samplings [22, 35] as the foundation of our method. For a detailed explanation, refer to Appendix A.
Diffusion models. Diffusion models are generative models that learn to gradually denoise random Gaussian noise into structured data by reversing a noise-adding process. This generative process can be formulated using stochastic differential equations (SDEs) [44] or their deterministic counterpart, ordinary differential equations (ODEs) [25]. A notable variant of diffusion models is Denoising Diffusion Implicit Models (DDIM) [43], which adopts a nonMarkovian approach to accelerate sampling while preserving the marginal distribution. At each step, DDIM estimates the clean data sample, ˆx0|t, following Tweedie’s formula from a noisy sample, xt, using a learned noise predictor, Φ. Then, the posterior distribution, p(xt−1|xt, x0), is computed as:
xt−1 = √α ̄t−1xˆ0|t + p1 − α ̄t−1ε ̃, (1)
where α ̄t−1 is a pre-defined coefficient that regulates noise schedule over time, and ε ̃ is a weighted combination of the predicted noise estimate term εΦ, and a stochastic noise term ε ∼ N (0, I), controlled by the hyperparameter η. For prompt-conditioned generation, classifier-free guidance (CFG) [16] is widely used. The estimated noise is adjusted as: εγ
Φ(xt; c, t) = εΦ(xt; ∅, t) + γ[εΦ(xt; c, t) − εΦ(xt; ∅, t)] , where γ controls the guidance strength, ∅ is the null-text embedding and c is the conditioning text embedding.
Score distillation sampling. Score Distillation Sampling (SDS) [35, 51] introduces a novel approach for optimizing differentiable parametric functions using diffusion models as a critic. By doing so, SDS extends the applicability of textto-image diffusion models to generate and manipulate more complex visual data, including 3D objects and scenes [7, 15, 24, 30, 46, 48]. To achieve this, SDS formulates generative sampling as an optimization problem, allowing control over the generated output by optimizing the parameters, θ, of a function, g(θ), to guide generation. Specifically, given a generated sample, x = g(θ), the gradient of the diffusion loss function with respect to θ is computed as:
∇θLSDS(Φ; x = g(θ)) ∆ = Et,ε w(t) (εγ
Φ(xt; c, t) − ε) ∂x
∂θ . (2) Collaborative score distillation sampling. SDS (Equation 2) optimizes a single sample, x. In contrast, Collaborative Score Distillation (CSD) [22] extends SDS by incorporating interactions between multiple samples, {x(i)}N
i=1,
allowing them to influence each other, thereby ensuring intersample consistency during optimization. At each iteration, CSD selects a random timestep, t, and samples Gaussian noise for each sample, ε(i) ∼ N (0, I). Then, each sample, x(i) = g(θi), is optimized using the following objective:
∇θi LCSD Φ; x(i) = g(θi)
∆ = w(t)
N
N
X
j=1
"
k(x(j)
t , x(i)
t ) εΦ(x(i)
t ; c, t) − ε(i)
+ ∇x(j)
t
k(x(j)
t , x(i)
t)
#
∂ x(i)
∂θi
,
(3)
where k is a positive definite kernel that measures similarity between samples. By leveraging inter-sample relationships, CSD adjusts gradient updates based on sample affinity, ensuring that each parameter update affects and is affected by other samples to promote global coherency.
2.2. Fusion-based tuning-free long video generation
T2V diffusion models generate short, single-event videos in one pass due to high computational costs, making long, dynamic video generation challenging. Fusion offers a practical solution by leveraging the model’s short-clip generation capability for extended multi-event video synthesis: dividing a long video into overlapping short chunks, processing each chunk with different prompts, and fusing overlapped regions for smooth transitions.
Problem formulation. Given a pre-trained T2V model, Φ, that generates a short video sample x ∈ Rf×H×W ×D, conditioned on a single text prompt embedding c, our goal is to extend it to generate a long video x′ ∈ RF ×H×W ×D, where F ≫ f . A common fusion strategy for a long video generation involves partitioning the long video, x′, into over
lapping short video chunks {x(i)}N
i=1 ∈ Rf×H×W ×D, with a temporal stride s. Each chunk is denoised independently,
conditioned on evolving text prompts {c(i)}N
i=1, to generate multi-event dynamics. These short chunks are then merged into a long video by averaging overlapping regions, normalized by the number of contributing chunks.
3. Key observations
Through fusion, adjacent chunks undergo temporal codenoising, ensuring smooth transitions. Gen-L-Video [50], CSD [22], and our proposed approach all leverage temporal co-denoising via fusion but differ in what is fused. The following sections detail these differences.
3.1. Local temporal co-denoising with Gen-L-Video
Gen-L-Video fuses posterior distributions. Gen-LVideo [50] extends T2V diffusion models by fusing the posterior distributions of overlapping chunks to maintain local
3


∎ Gen-L-Video ∎ CSD ∎ SynCoS (Ours)
collapsed
divergent
same initial point (t=1000)
1000
800
600
400
200
0
balanced
Figure 3. t-SNE visualization of CLIP [37] features for the predicted video frames xˆ0|t, at each timestep using different samplings. Faded colors indicate earlier timesteps (t ≈ 1000), while vivid colors indicate later, small timesteps (t ≈ 0), illustrating feature trajectory evolution over time (top to bottom).
Local 2 A beautiful young lady sits beside him
1st chunk 2nd chunk 0 sec 5 sec 10 sec
Local 1 He takes a slow sip enjoying his drink
Global prompt A handsome young man sits at a wooden table, enjoying a moment of relaxation with a cup of coffee.
⋯
CSD Gen-L-Video
⋯
SynCoS (Ours)
⋯
Figure 4. Qualitative comparison of sampling methods motivating SynCoS. GenL-Video [50] fails to maintain global coherence, resulting in abrupt appearance changes (e.g., a man morphing into a woman). CSD [22] retains a similar appearance of a man but shows poor adherence to local prompts, suffering from low frame quality with severe noise-like artifacts. In contrast, our method achieves a balance, ensuring high-quality generation, strong prompt fidelity, and temporal coherence.
smoothness. Each chunk undergoes independent denoising using the DDIM sampler, producing intermediate outputs
x(i)
t−1. The final x′t−1 is obtained by fusing the posteriors of
overlapping chunks x(i)
t−1, as computed in Equation 1.
Divergence in denoising paths with Gen-L-Video. While this local fusion with Gen-L-Video helps maintain smooth transitions between adjacent chunks, it neglects information across distant video chunks to enforce long-range temporal consistency. As a result, Gen-L-Video produces unnatural transitions, such as abrupt appearance changes (e.g., a man morphing into a woman, as shown in Figure 4). We verify this in Figure 3, which visualizes how denoising paths evolve over time. Here, dot color intensity transitions from faded to vivid, representing progressively later timesteps, with dots arranged top to bottom, corresponding to timesteps from t = 1000 to t = 0. The green dots, representing Gen-LVideo, gradually separate, indicating increasing divergence in its denoising paths.
3.2. Global temporal co-denoising with CSD
To establish connections between distant video chunks while preserving local smoothness, we explore an optimizationbased approach using Collaborative Score Distillation (CSD) for long video generation. Although CSD was originally designed for visual editing, we apply it to improve global temporal consistency in fusion-based long video generation.
Apply CSD for long video generation by fusing its loss. Similar to Gen-L-Video, a long video is divided into overlapping chunks. The CSD loss for each chunk is computed to synchronize denoising paths across both short- and longrange chunks. Then, the losses in overlapping regions are fused, further reinforcing smooth transitions.
Failure of CSD for long video generation. Despite its effectiveness in visual editing, CSD fails when applied to long
video generation. Unlike editing, where an existing source structure guides modifications, video synthesis starts from pure Gaussian noise with no inherent priors. This lack of structured guidance causes frames to collapse into similar states as denoising progresses (see red dots in Figure 3). As a result, while the generated video retains a similar appearance throughout, it fails to adhere to local prompts and suffers from degraded per-frame quality (see Figure 4).
4. Method
We propose Synchronized Coupled Sampling (SynCoS), a novel inference framework applicable to any T2V diffusion model for generating temporally consistent, multi-event long videos. Section 4.1 provides an overview of the stages in our coupled sampling, followed by key mechanisms that enable the synchronized coupling of the three stages in Section 4.2.
4.1. SynCoS: Synchronized Coupled Sampling
As discussed in Section 3, fusion-based temporal codenoising shows promise for long video generation, but existing methods suffer from different limitations. Temporal co-denoising with DDIM (i.e., Gen-L-Video) ensures local smoothness but results in divergent denoising paths, whereas temporal co-denoising with CSD facilitates global coherence but results in overly converged, collapsed denoising paths. To capture both local smoothness and global consistency, SynCoS combines fusion-based temporal co-denoising with DDIM and CSD at each denoising step. The key insight is treating the intermediate DDIM output of xˆ0|t as a refinement source for CSD-based optimization, enabling global refinement to build upon locally smoothed updates. This formulation reframes CSD-based optimization as a progressive refinement process rather than pure generation, where each DDIM update at every denoising step serves as an improved starting point for enforcing global coherence.
4


T2V Diffusion Model
From t = T to t = : Repeat 1, 2, and 3 From t = to t = 1 : Repeat 1, and 3
(g, l4)
(g, l2)
(g, l3)
(g, l1) (g, l2)
(g, l1)
(g, l3)
(g, l4)
⋯⋯⋯
Backpropagate to update θ
T2V Diffusion Model
DDIM ⊕
Refined
DDIM
3 Reverted to previous timestep
1 Perform diffusion forward on each short video chunk and fuse the intermediate DDIM output
2 Refine the locally fused output to enforce global coherency
Global prompt: g Local prompt : l1, l2, l3, l4
Structured prompt
Figure 5. Overall illustration of our proposed method, Synchronized Coupled Sampling (SynCoS), a tuning-free inference framework for multi-event long video generation. SynCoS performs one-step denoising in three iterative stages, repeated from t = 1000 to t = 0. In the first stage, SynCoS performs temporal co-denoising with DDIM, dividing the long video into overlapping short chunks, denoising each chunk, and applying fusion for local smoothness. In the second stage, SynCoS refines the locally fused output, enforcing global coherence by synchronizing information across both short- and long-distance chunks. Finally, in the third stage, it reverts the locally and globally refined output to the previous timestep. Through these three synchronized stages of local and global denoising, SynCoS ensures smooth transitions, global semantic coherence, and high prompt fidelity, enabling multi-event long video generation.
SynCoS consists of three stages, which are repeated at each denoising step (illustrated in Figure 5):
• 1. Perform temporal co-denoising with DDIM. Instead of fully reverting each chunk to its previous timestep, t − 1, SynCoS computes xˆ(i)
0|t for every video chunk, then applies fusion to them to produce x′0.
• 2. Refine the locally fused output to enforce global coherency. The fused output from the first stage serves as a refinement source, where temporal co-denoising with CSD
based optimization is performed by computing ∇x(i)
0
LCSD
for each chunk and applying fusion on the gradients to update x′0. This stage synchronizes denoising levels across chunks, effectively managing multi-event scenarios by ensuring coherence when varying prompts cause divergent denoising paths. By doing so, SynCoS mitigates content and style drift while preserving global semantic consistency. Crucially, SynCoS introduces key components to bridge the first and second stages and the second and final stages, as detailed in Section 4.2.
• 3. Resume the temporal co-denoising with DDIM using locally and globally refined output. SynCoS reverts the refined and fused x′0 to the previous timestep, producing
x′t−1. Here, the intermediate sample x′t is re-derived by
adding noise (from the second stage) to the x′0. Finally,
x′t−1 is computed using both x′0 and the re-derived x′t.
This one-step denoising process, comprising three stages, is iteratively repeated from t = 1000 until t = 0. We present the complete algorithm in Algorithm 1, with pseudo-code for each stage provided in Appendix E.
4.2. Align denoising paths across three stages for synchronized coupling
SynCoS introduces three stages to capture both local smoothness and global coherence. However, synchronizing denois
Algorithm 1:
Synchronized Coupled Sampling (SynCoS)
Require : Φ; // a pre-trained T2V model
{c(i)}N
i=1; // conditioning text-prompt embeddings Ensure :x′
0
Procedure Ours: x′
T ∼ N (0, I);
for t = T, ..., 1 do for i = 0, ..., N − 1 do
x(i)
t ← TakeChunk(x′
t);
ε(i)
pred ← DiffForwardΦ(x(i)
t , c(i), t);
xˆ(i)
0|t ← DDIM(ε(i)
pred, t);
x′
0 ← Fusion({xˆ(i)
0|t }N
i=1);
if t > tmin then ε′
base ∼ N (0, I); // fixed baseline noise for iter = 1 to iters do
for i = 0, ..., N − 1 do
x(i)
0 ← TakeChunk(x′
0);
ε(i)
base ← TakeChunk(ε′
base ); x(i)
t ← AddNoise(x(i)
0 , ε(i)
base ); ε(i)
pred ←
DiffForwardΦ(x(i)
t , c(i), t); // grounded timestep
for i = 0, ..., N − 1 do
∇x(i)
0
LCSD (x(i)
t , ε(i)
base, ε(i)
pred );
∇x′
0 LCSD ← Fusion({∇x(i)
0
LCSD }N
i=1);
Backpropagate ∇x′
0 LCSD;
SGD update on x′
0;
x′
t−1 ← DDIM(x′
0, εbase, t);
5


ing trajectories across stages is crucial, as alternating between them can cause misalignment, leading to artifacts and reduced prompt fidelity. In this section, we present the key components of each stage in SynCoS that ensure synchronized coupled sampling for seamless stage transitions.
Grounded timestep between the first and second stage. Aligning timesteps across stages is crucial in a three-stage process, as diffusion models establish the overall structure in earlier timesteps and refine finer details in later timesteps. If each stage operates on inconsistent temporal references, denoising trajectories become misaligned, introducing artifacts and inconsistencies in the final video (see Figure 7). To prevent this, SynCoS anchors the second-stage timestep to the first-stage sampling schedule, following the DDIM timestep progressing from t = 1000 to t = 0 based on designated sampling steps. This ensures that both stages operate within a unified temporal reference, first establishing a coherent structure and then focusing on finer details throughout the denoising process. This approach distinguishes SynCoS from standard CSD-based optimization (Equation 3), where timesteps are randomly selected at each iteration.
Fixed baseline noise for the second and third stage. In SynCoS, while ensuring global coherence, it is crucial in the second stage to preserve distinct prompt guidance for each chunk and prevent sample collapse, as discussed in Section 3.2. To achieve this, we fix a single noise and use it consistently throughout one-step denoising, stabilizing second-stage optimization by providing a consistent baseline noise, unlike Equation 3, where random noise is introduced at each step. Specifically, a fixed baseline noise ε′
base is sampled from a Gaussian distribution at the start of the second stage, matching the shape of x′0. Each chunk x(i)
0 is then
processed alongside its corresponding noise chunk ε(i)
base. Additionally, SynCoS retains this fixed noise in the third stage, where it is reintroduced into the refined x0, further maintaining aligned update directions across stages.
Coupled sampling for the early timesteps. To balance local smoothness and global coherence, SynCoS regulates the second stage using tmin, applying it only until tmin ∈ [800, 900], which we empirically found to be the optimal range. By this point, SynCoS establishes a coherent semantic layout across video chunks, preventing denoising trajectories from diverging as shown in Figure 3. Thus, beyond tmin, SynCoS performs only the first and third stages, focusing on adding fine-grained details on an already structured layout.
Structured prompt. To further enhance coherent long scenarios with dynamics, SynCoS designs a structured prompt for multi-event scenarios, consisting of a shared global prompt for scenario-wide coherence and local prompts for event-specific variations (e.g., motions, camera movement, or attributes). Naïvely using a sequence of prompts (e.g., “A teddy bear is standing” → “A teddy bear is running”)
can lead to inconsistencies in the shared entity. To reduce ambiguity and constrain the generation space, we introduce a detailed global prompt (e.g., “A brown teddy bear with button eyes and a stitched smile”), ensuring uniformity across all video chunks. Each chunk is then assigned a local prompt that builds on the global prompt, incorporating chunk-specific dynamics. See Appendix B for details.
Flexibility of SynCoS to various T2V diffusion models. Notably, SynCoS is architecture-agnostic and compatible with any T2V diffusion model, supporting various diffusion objectives (v-prediction [41], ε-prediction [12, 52]), both diffusion-based and rectified flow-based models. In addition, unlike prior works [5, 36, 45] restricted to U-Net [39] or DiT [33], SynCoS remains flexible across architectures. While we describe our method using ε-prediction networks for clarity, SynCoS easily adapts to other diffusion settings by modifying LCSD in Algorithm 1 (see Appendix A for a complete derivation on various diffusion settings). We further validate this through comprehensive experiments on various denoising models in Section 5 and Appendix C.
5. Experiments
We first provide a brief overview of the experimental setup in Section 5.1, followed by a thorough validation of our inference framework across diverse scenarios and diffusion settings in Section 5.2, Appendix D, comparing it with previous works. Additionally, we conduct comprehensive ablation studies in Section 5.3 and Appendix C.
5.1. Experimental setup
Implementation details. We evaluate on 48 long-video scenarios, forming a more extensive benchmark than previous works [21, 31], with video lengths extended by 4–5 times. These multi-event scenarios encompass challenges such as object motion control, camera control, background changes, compositional generation, and physical transformations (full prompt details in Appendix B). To demonstrate broad applicability, we implement SynCoS across four different T2V diffusion models: CogVideoX2B [55] and Open-Sora Plan (v1.3) [23] (in Section 5.2) as well as a modified Open-Sora Plan (v1.2) variant (denoted as M) and VideoCrafter2 [52] (in Appendix C and D). All experiments are conducted on a single NVIDIA H100 80GB GPU, with inference time measurements detailed in Appendix B.
Baselines. We primarily evaluate SynCoS against tuningfree, architecture-agnostic baselines (Gen-L-Video [50], FIFO-Diffusion [21]) and provide additional comparisons with architecture-specific methods (FreeNoise [36], VideoInfinity [45], DiTCtrl [5]) in Appendix D for a fair evaluation within their respective architectures. For all baselines, we follow their reported experimental setups and carefully tune hyperparameters for optimal performance.
6


Table 1. Quantitative comparison. Experimental results of SynCoS with baselines on multi-event long video generations. Bold indicates the best results. SynCoS consistently outperforms baselines across temporal consistency, per-frame quality, and prompt fidelity.
Temporal Quality Frame-wise Quality Semantics
Subject Background Motion Dynamic Num Aesthetic Imaging Glb Prompt Loc Prompt Backbone Method Consistency ↑ Consistency ↑ Smoothness ↑ Degree ↑ Scenes ↓ Quality ↑ Quality ↑ Fidelity ↑ Fidelity ↑
CogVideoX [55]
Gen-L-Video [50] 81.34% 89.46% 98.32% 50.00% 2.292 60.09% 58.52% 0.324 0.351 FIFO-Diffusion [21] 83.54% 90.72% 98.04% 70.83% 1.938 59.59% 63.18% 0.323 0.327
SynCoS (Ours) 88.92% 94.57% 98.21% 85.42% 1.208 63.37% 67.43% 0.341 0.354
Open-Sora Plan [23]
Gen-L-Video [50] 85.15% 92.63% 98.93% 43.75% 2.458 61.44% 57.19% 0.319 0.337 FIFO-Diffusion [21] 89.14% 94.34% 98.19% 27.08% 1.125 60.26% 61.23% 0.327 0.331
SynCoS (Ours) 90.19% 94.78% 99.06% 58.33% 1.646 63.79% 60.19% 0.328 0.345
1st and 2nd chunk 3rd and 4th chunk 0 sec 10 sec 20 sec
Local 1 The camel runs across a vast, snow-covered expanse Local 2 The camel stands still
Global prompt A solitary camel, with its thick coat and humps on a snow-covered landscape. The soft light of the setting sun casts long shadows.
⋯
SynCoS (Ours)
⋯
FIFO Gen-L-Video
⋯ Abrupt changes
in appearance
Noticeable noise-like artifacts
High-quality
Figure 6. Qualitative comparisons on Open-Sora Plan [23]. All examples are 4 times longer in duration compared to the underlying base model, generating a 20-second video. Gen-L-Video [50] suffers from abrupt appearance changes, while FIFO-Diffusion [21] introduces noticeable noise-like artifacts. In contrast, our proposed method, SynCoS, generates high-quality, temporally coherent videos that faithfully follow the prompt throughout the sequence.
Evaluation metrics. We evaluate multi-event long video generation results using VBench [19] to assess both temporal consistency and per-frame quality. Temporal consistency is measured by subject and background consistency, motion smoothness, and dynamic degree, while aesthetic and imaging quality evaluate per-frame fidelity. Following prior works [14, 54], we use Adaptive Detector [6] to count scene changes, where a ‘Num Scenes’ value of 1 indicates no transitions. To assess prompt-driven controllability, we measure prompt fidelity using CLIP [37], computing image-text similarity for both local and global prompts.
5.2. Main experiments
While SynCoS incorporates a structured prompt, we apply the structured prompt to all baselines for a fair comparison.
Quantitative comparisons. In Table 1, SynCoS significantly outperforms existing tuning-free baselines for long video generation, achieving higher scores in both temporal consistency and per-frame image quality. In particular, SynCoS surpasses baselines in subject and background consistency while achieving the highest dynamic degree, demonstrating its ability to generate visually dynamic but temporally consistent videos. Since a lower dynamic degree naturally improves consistency, high-scores of SynCoS in both metrics are particularly significant. Additionally, SynCoS demonstrates superior prompt fidelity, generating distinct promptcontrolled dynamics while maintaining semantic and content consistency across the entire video.
Qualitative comparisons. As shown in Figure 2 and Figure 6, SynCoS excels in high-quality multi-event long video
generation, ensuring temporal consistency throughout the video. Gen-L-Video suffers from overlapping artifacts due to diverging denoising paths across video chunks when guided by different prompts. SynCoS effectively mitigates these issues through global synchronization. FIFO-Diffusion exhibits style drift on CogVideoX-2B and suffers from degraded image quality with noise artifacts on Open-Sora Plan. This issue arises from its sequential timestep processing of adjacent frames, leading to a training-inference discrepancy. As Open-Sora Plan encodes more frames per video chunk, FIFO-Diffusion struggles with severe timestep variations between frames, resulting in pronounced artifacts. In contrast, SynCoS does not introduce any training-inference discrepancies. See Appendix D for further comparisons on baselines and Appendix F for additional qualitative results.
5.3. Ablation study
Table 2 and Figure 7 ablate the key components essential for coupling the three stages into a unified sampling, providing both quantitative and qualitative evaluations. First, grounded timesteps anchor the generation process, ensuring that each stage focuses on the same aspects of generation when alternating between stages. Without grounding, random timesteps disrupt the progressive generation from structure formation (early timesteps) to detail addition (later timesteps), causing content drift (see w/o grounded timestep in Table 2). For example, in Figure 7, the absence of grounded timesteps causes a volcano to randomly change its appearance, generating one or two peaks across frames due to misalignment between structure and detailing.
7


Table 2. Quantitative ablation study. *Abbreviations: subject consistency (SC), background consistency (BC), aesthetic quality (AQ), and prompt fidelity (PF).
Temporal Frame Semantics SC ↑ BC ↑ AQ ↑ PF ↑ SynCoS 0.864 0.927 0.643 0.381 SynCoS w/o t
min
0.724 0.854 0.632 0.373 SynCoS w/o grounded timestep 0.803 0.899 0.638 0.364 SynCoS w/o fixed baseline noise 0.817 0.904 0.643 0.382 SynCoS w/o structured prompt 0.837 0.903 0.663 0.362
1
st
chunk 2
nd
chunk 3
rd
chunk 0 sec 7 sec 14 sec 21 sec
Global prompt
The majestic volcano stands out against the rugged landscape. In the distance a flock of birds takes flight. Local 1
Volcano erupts violently under a clear sky
Local 2
Smoke rises from the summit
Local 3
Dark clouds gather, darkening the clear sky
w/o
grouned
timestep
SynCoS
w/o
fixed
baseline noise
ڮ
ڮ
ڮ
ڮ
ڮ
ڮ
Figure 7. Qualitative ablation study. Without a grounded timestep, structural inconsistencies arise (e.g., the volcano alternates between one and two peaks). Without a fixed baseline noise, it fails to follow local prompts faithfully (e.g., missing eruptions or absent smoke).
Additionally, fixed baseline noise is crucial for preserving distinct prompt guidance while ensuring cohesion between them (see w/o fixed baseline noise in Table 2). Without it, distinct prompt guidance weakens as cohesion dominates, reducing prompt fidelity (e.g., a volcano without an eruption or missing rising smoke, as shown in Figure 7). Moreover, t
min
regulates the synchronization strength (see w/o t
min
in Table 2). Lastly, structured prompts improve coherence by blending global prompts for consistency with local prompts for fine-grained control (see w/o structured prompt in Table 2). In addition to qualitative examples in Section 3, Appendix C presents quantitative ablations by skipping each stage of SynCoS to assess the efficacy of coupled stages.
6. Related work
Text-guided video diffusion models. The success of textto-image (T2I) generation with diffusion models [2, 38, 40] has inspired extensions to the more complex task of textto-video generation. Several works extended T2I models by fine-tuning them with temporal layers, adapting spatial structures to temporal dynamics for more efficient training [1, 4, 10, 13, 18, 42, 52]. Recent advancements in Diffusion Transformers (DiT) [33] and flow-based generative models [27] have further improved quality, enabling state-ofthe-art T2V diffusion models [20]. However, high computational and memory costs restrict these models to short video clips, limiting their applicability to long video generation.
Training-based long video generation. To extend video length, alternative strategies include different architecture choices: transformer-based methods, autoregressive diffusion models, and hierarchical models [8, 9, 14, 47, 49, 54, 56]. Transformer-based methods [9, 49] generate long videos in a one-shot manner from multiple prompts. However, they require extensive training from scratch and often suffer from content drift, degrading quality over time. Autoregressive diffusion models [14, 54] generate frames sequentially, conditioning each chunk on the previous one. While this improves short-term consistency, autoregressive dependencies cause errors to accumulate, resulting in visual artifacts and inconsistencies in long-form synthesis. Additionally, these methods are limited to single-prompt generation, making
them unsuitable for evolving content. Alternatively, hierarchical diffusion models [8, 56] generate keyframes first and then interpolate intermediate frames. While this approach maintains structural consistency, interpolation alone does not introduce new content, limiting dynamic scene evolution.
Tuning-free long video generation. Recent approaches extend existing T2V diffusion models for long video generation in a tuning-free manner using multiple prompts [21, 31, 36, 50]. While scalable without additional training, these approaches focus on local frame transitions, often leading to content drift and semantic inconsistencies over longer sequences. Gen-L-Video [50] fuses denoising paths in overlapping frames, but local fusion dilutes prompt guidance from different prompts between frames, degrading quality and causing divergent denoising paths. FIFO-Diffusion [21] enables infinite sampling by sequentially assigning timesteps, but discrepancies between training and inference lead to undesirable artifacts. FreeNoise [36] uses window-based attention fusion to attend to longer frame dependencies, but it is incompatible with newer diffusion models that lack separate spatial and temporal attention layers. In contrast, our method synchronizes local and global denoising paths across adjacent and distant frames, preventing content drift and preserving semantic consistency throughout long videos. Additionally, Video-Infinity [45] and DiTCtrl [5] propose architecture-specific extensions, whereas our approach is compatible with any T2V diffusion model. Please refer to Appendix D for further discussion and comparisons.
7. Conclusion
This work presents a tuning-free inference framework that extends any existing T2V diffusion model for multi-event long video generation. Unlike previous works focusing solely on local smoothing between adjacent frames, our approach simultaneously ensures smooth local transitions and global coherence by introducing three synchronously coupled stages with a structured prompt. Extensive evaluations across diverse, challenging, long video scenarios with multiple events demonstrate that our method generates high-quality, temporally coherent long videos, significantly outperforming prior works both quantitatively and qualitatively.
8


References
[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 8
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 8
[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: A space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 1–11, 2024. 1 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 8
[5] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2025. 2, 6, 8, 5 [6] Brandon Castellano. PySceneDetect. 7 [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In IEEE International Conference on Computer Vision, pages 22246–22256, 2023. 3
[8] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In International Conference on Learning Representations, 2023. 8
[9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference on Computer Vision, 2022. 2, 8 [10] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In IEEE International Conference on Computer Vision, 2023. 8
[11] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-tovideo generation by explicit image conditioning. In European Conference on Computer Vision, 2023. 1
[12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In International Conference on Learning Representations, 2023. 6
[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. 8
[14] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 7, 8
[15] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3
[16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3
[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 1
[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 8
[19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 7
[20] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. 1, 8 [21] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. In Advances in Neural Information Processing Systems, 2024. 2, 6, 7, 8, 3, 4 [22] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual editing. In Advances in Neural Information Processing Systems, pages 73232–73257, 2023. 2, 3, 4, 1 [23] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1, 6, 7, 4 [24] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to3d content creation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3
[25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2022. 3, 1 [26] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems, 2016. 1
9


[27] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. 8 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 3
[29] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. In Advances in Neural Information Processing Systems, 2025. 2, 5 [30] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from a single image. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3
[31] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, Hyeokmin Kwon, and Sangpil Kim. Mtvg: Multi-text video generation with text-to-video models. In European Conference on Computer Vision, 2023. 1, 2, 6, 8 [32] OpenAI. Hello gpt-4o, 2024. 2 [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision, 2023. 6, 8
[34] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1 [35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2022. 3, 1
[36] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. In International Conference on Learning Representations, 2023. 2, 6, 8, 4 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 4, 7 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 8
[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234–241. Springer, 2015. 6 [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep lan
guage understanding. In Advances in Neural Information Processing Systems, 2022. 8
[41] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 6
[42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In International Conference on Learning Representations, 2023. 8
[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 2, 3
[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. 3, 1 [45] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. 6, 8, 5
[46] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [47] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation. In Advances in Neural Information Processing Systems, 2024. 2, 8 [48] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: Generation of realistic 3d meshes from text prompts. In 2024 International Conference on 3D Vision (3DV), pages 15541563. IEEE, 2024. 3 [49] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. 1, 2, 8
[50] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 2, 3, 4, 6, 7, 8
[51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [52] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023. 6, 8, 5
[53] Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski,
10


and Sergey Tulyakov. Mind the time: Temporally-controlled multi-event video generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2025. 1
[54] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. arXiv preprint arXiv:2410.08151, 2024. 7, 8 [55] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 6, 7, 4, 5
[56] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 8
11


Tuning-Free Multi-Event Long Video Generation
via Synchronized Coupled Sampling
Supplementary Material
Project page: https://syncos2025.github.io/
A. Foundational concepts and derivations
A.1. Further details on preliminaries
This section provides additional details on the preliminaries introduced in Section 2 to further guide the readers.
Diffusion models. Diffusion models are generative models that learn to gradually transform a noise sample from a tractable noise distribution towards a target data distribution. This transformation is consisting of two processes: a forward process and a reverse process. In the forward process, noise is incrementally added to the data sample over a sequence of timestep t, leading to a gradual loss of structure in the data. This forward process is defined as following:
xt = √α ̄tx0 + √1 − α ̄tε (4)
where α ̄t is a pre-defined coefficient that regulates noise schedule over time, and ε ∼ N (0, I) denotes a noise sampled from a standard normal distribution. The reverse diffusion process aims to invert the forward process, gradually removing the noise and recovering the original target data distribution. This reverse transformation is modeled using a neural network (referred to as the diffusion model, εΦ), which is trained using a loss function based on denoising score matching [17, 44]. The loss function is defined as:
Ldiff(Φ; x) = Et,ε∼N (0,I) w(t)∥εΦ(xt; t) − ε∥2
2 , (5)
where w(t) is a weighting function applied to each timestep t, and t ∼ U(0, 1) is drawn from a uniform distribution.
Score distillation sampling. Score distillation sampling (SDS) [35] introduces a new way to optimize any arbitrary differentiable parametric function g using diffusion models as a critic by posing generative sampling as an optimization problem. The flexibility of SDS in optimizing diverse differentiable operators has made it a versatile tool for visual tasks. Specifically, given x = g(θ), the gradient of the diffusion loss function of Equation 5 with respect to the parameter θ is expressed as below:
∇θLSDS(Φ; x = g(θ))
∆ = Et,ε w(t) (εγ
Φ(xt; c, t) − ε) ∂εγ
Φ(xt; c, t) ∂x
∂x ∂θ
(6)
In DreamFusion [35], it has been shown that by omitting the Jacobian term of the U-Net (the middle term) in Equation 6 yields an effective gradient for estimating an update direction that follows the score function of diffusion models, thereby moving towards a higher-density region. The simplified expression is given as Equation 2 in Section 2.
Collaborative score distillation sampling. The original SDS method considers only a single sample, x, during the optimization process. Collaborative Score Distillation (CSD) [22] extends SDS to enable inter-sample consistency by synchronizing gradient updates across multiple samples,
{x(i)}N
i=1. Specifically, CSD generalizes SDS by using Stein Variational Gradient Descent (SVGD) [26] to align gradient directions across multiple samples as Equation 3 in Section 2. For a positive definite kernel, k, in Equation 3, CSD employs the standard Radial Basis Function (RBF) kernel. This approach ensures that each parameter update is influenced by other samples with scores adjusted via importance weights based on sample affinity, thereby effectively promoting intersample consistency during optimization.
A.2. SynCoS with different diffusion models
In this paper, we present our method using diffusion models with ε-prediction networks for clarity. However, our framework is compatible with any T2V denoising model. To illustrate this, we describe how applying our framework to different diffusion settings. Flow-based models. Recently, rectified flow [25], a unified ODE-based framework for generative modeling, introduces a simplified denoising process that optimizes the trajectories in diffusion space to be as straight as possible. Given a data sample x from the data distribution and a noise sample ε from a Gaussian distribution, rectified flow defines the forward process as:
xt = tε + (1 − t)x0, t ∈ [0, 1] (7)
Accordingly, the reverse process is governed by an ODE that maps ε to x0:
dxt = v(xt; t)dt, t ∈ [0, 1] (8)
where v is a velocity field estimated by a learnable neural network Φ, where the model is trained using the following objective:
Lflow(Φ; x) = Et,ε w(t)∥(ε − x0) − v(xt; t)∥2
2 . (9)
1


I would like you to act as a sentence separator, breaking down long sentences into a global prompt and a specified number of concise local prompts based on action verbs. The long sentence will be referred to as the "Scenario" and divided into a given number of short sentences, or "Local Prompts," based on the "Number of Prompts."
First, consider the context of the long sentence, replacing any pronouns with specific nouns mentioned in the sentence, using an indefinite article if necessary.
Next, break the modified long sentence into the specified number of local prompts, focusing on the main action verbs. Please follow these rules:
1. Each set should consist of one global prompt that is shared across all local prompts, along with the specified number of local prompts. 2. The global prompt should describe the shared properties across all the local prompts. 3. The global prompt must provide detailed, concise descriptions for the shared entity to avoid vagueness, enhancing clarity and context for the local prompts. 4. Each local prompt should include the global prompt. 5. Each local prompt should focus on only one action verb. The tense and form of these verbs must match the original long sentence. 6. Each local prompt should be self-contained, following the structure of subject, verb, and background. 7. Each local prompt should include all relevant background information linked to the main action verb. 8. No other verbs should be included apart from the specified main action verbs. 9. Avoid redundancy between the global prompt and local prompts; the global prompt should not contain the main actions described in the local prompts. 10. The tense of the local prompts (present, present progressive, or present participle) must align with the original long sentence.
From this point onward, I will provide prompts for you to rephrase according to these rules. The prompts may sometimes be simple, with minimal description for the entity or background. Feel free to enhance the details as needed to improve video quality.
Example:
Global prompt: An adorable kawaii cheeseball on the moon, smiling, 3D render, octane, soft lighting, dreamy bokeh, light sparkles floating in the background.
Local prompt 1: An adorable kawaii cheeseball on the moon, smiling, 3D render, octane, soft lighting, dreamy bokeh, light sparkles floating in the background. The cheeseball wiggles slightly, its rounded shape catching the soft light as it shimmers.
Local prompt 2: An adorable kawaii cheeseball on the moon, smiling, 3D render, octane, soft lighting, dreamy bokeh, light sparkles floating in the background. A soft sparkle lands on the cheeseball's surface, making it blush a gentle pink, adding to its cuteness.
Local prompt 3: An adorable kawaii cheeseball on the moon, smiling, 3D render, octane, soft lighting, dreamy bokeh, light sparkles floating in the background. The cheeseball's eyes blink slowly, a soft reflection of the stars above shimmering in them.
Local prompt 4: An adorable kawaii cheeseball on the moon, smiling, 3D render, octane, soft lighting, dreamy bokeh, light sparkles floating in the background. The cheeseball hops slightly on the moon's surface, leaving a tiny imprint in the soft, glowing moon dust.
Figure 8. Instruction for generating structured prompt. This instruction follows the guidelines to create individual local prompts and a shared global prompt based on a scenario and the number of prompts the user gives.
Score distillation with flow-based models. While SDS and CSD have been implemented on denoising diffusion models with ε-prediction networks, the core concept of score distillation—using diffusion models as generative priors for optimization—can also be extended to flow-based models. Below, we derive the equation for score distillation adapted to flow-based models. By computing the gradient of the training objective Lflow for flow-based models, Φ, with respect to θ, the score distillation sampling adapted to flow-based models can be expressed as:
∇θi LFlow−SDS(Φ, x = g(θ))
∆ = Eε,t w(t) (vΦ(xt, t) − (ε − x)) ∂x
∂θ . (10)
Here, w(t) is a time-dependent weighting function, and vΦ is estimated by the pre-trained flow-based denoising network Φ. In accordance with SDS conventions, the (transformer) Jacobian term is omitted to enhance computational efficiency, enabling the optimization of x using the rectified flow model. This loss is then could be reinterpreted within the Collaborative Score Distillation (CSD) framework [22], allowing for synchronous updates across multiple samples instead of treating each gradient independently.
B. Experimental details
Details of structured prompt. The structured prompt is designed to divide a cohesive story (i.e., scenario) into multiple prompts, enabling the generation of long videos controlled by distinct prompts. It consists of a single global prompt that describes shared properties across all local prompts (e.g., object appearances or styles) and distinct local prompts that specify changes in objects, motions, or styles. To create a structured prompt, we leverage a Large Language Model (LLM), as in previous works [31]. The process involves instructing the LLM to segment a given scenario into multiple local prompts and a detailed global prompt, as illustrated in Figure 8. We use GPT-4o [32] to generate these structured prompts, ensuring consistency across all experiments. These structured prompts are applied in all experiments, including the main evaluations of our method and the baselines, as well as in ablation studies, ensuring a fair comparison.
To extensively verify our method under various challenging scenarios, we consider both previously established scenarios [21, 29, 31, 36, 47, 50] and our new, more challenging scenarios, including: background changes, object motion changes, camera movements, compositional generation, complex scene transitions, cinematic effects, physical transformation, and storytelling. We experiment with 48 structured prompt scenarios, including 11 with two local prompts, 12
2


Table 3. Quantitative ablations study of the three coupled stages in SynCoS, omitting each stage during one-timestep denoising, demonstrates the importance of all three stages for coherent long video generation with multiple events.
Temporal Quality Frame-wise Quality Semantics
Stage Subject Background Motion Dynamic Num Aesthetic Imaging Glb Prompt Loc Prompt Backbone 1 2 3 Consistency ↑ Consistency ↑ Smoothness ↑ Degree ↑ Scenes ↓ Quality ↑ Quality ↑ Fidelity ↑ Fidelity ↑
M
✓ ✗ ✓ 80.46% 91.14% 98.55% 97.92% 1.229 53.36% 58.42% 0.318 0.348 ✗ ✓ ✗ 78.88% 91.63% 97.70% 14.58% 21.33 45.56% 42.27% 0.305 0.300
✓ ✓ ✓ 82.70% 91.85% 98.24% 100.00% 1.042 54.56% 65.53% 0.325 0.348
1st chunk
Local 1 The sky is deep orange Local 2 The sky is purple
2nd chunk
Local 3 The city glows under the night sky
3rd chunk 0 sec 5 sec 10 sec 15 sec
Global prompt The whole beautiful view of the city is shown. The setting sun casts a warm glow, with light reflecting off buildings, creating a serene and picturesque scene.
CSD
⋯⋯
SynCos (Ours)
⋯⋯
⋯⋯
Gen-L-Video
Figure 9. Qualitative visualization of the ablation study on the three coupled stages of SynCoS. All examples in the second box are 3 times longer in duration compared to the underlying base model.
with three, and 25 with four, extending video length by a factor of four or five. Full lists are provided on our project page: https://syncos2025.github.io/
Implementation details of the main experiments. DDIM sampling was performed with 50 steps, setting DDIM η to 0. The stride s is set to 4 for CogVideoX-2B and 6 for OpenSora Plan (v1.3). In second stage, we set tmin ∈ [800, 900], learning rate, lr ∈ [0.5, 1] using AdamW Optimizer [28], and iters ∈ [20, 50]. The scale of the classifier-free guidance is set to 6 for CogVideoX-2B and 7.5 for Open-Sora Plan (v1.3). For Gen-L-Video, we use the same strides and guidance scale as in our experiments. For FIFO-Diffusion, we set n = 4 for the number of partitions in latent partitioning and lookahead denoising, following their best parameter configurations.
Measurements of computation time. We measure the computation time required to generate a 4× longer video compared to the underlying base model of CogVideoX-2B, using a single H100 80GB GPU (Table 4), including both main baselines and our approach. Although our method takes 2.6× longer than the baselines, it achieves notable gains in quality and consistency, demonstrating superior qualitative and quantitative performance.
While computation time is not our primary focus, it can be adjusted based on video scenarios and the required level of synchronization. Specifically, reducing iters in the secondstage optimization can lower computation time. Nonetheless,
we think reducing computation time while maintaining quality is a promising direction in the future.
Table 4. Measurements and comparisons of computation time on CogVideoX-2B.
Gen-L-Video [50] FIFO [21] SynCoS (Ours)
21 min. 21 min. 55 min.
C. Additional ablations
Effect of the three coupled stage. In addition to Section 3 and Figure 4, we provide further quantitative evaluations (Table 3) and qualitative visualizations (Figure 9) by skipping each stage in SynCoS to assess the efficacy of its three-stage process in generating high-quality multi-event long videos. As discussed in Section 3.1, omitting the first stage in SynCoS, which corresponds to temporal co-denoising with DDIM (i.e., Gen-L-Video), causes denoising paths across chunks to diverge, leading to overlapping artifacts, as shown in Figure 4. This results in reduced temporal consistency and frame-wise quality, as quantified in Table 3, due to abrupt changes. Additionally, this variant often struggles to faithfully follow prompts, as the simple fusion of denoising paths dilutes prompt guidance unique to each chunk. This issue is evident in the second example of a city transition (Figure 9), where the scene fails to properly reflect changes in glowing, particularly in the third chunk.
3


Conversely, relying solely on the second stage, corresponding to temporal co-denoising with CSD, degrades image quality significantly. As shown in Figure 4 and Figure 9, the video exhibits noise-like artifacts, leading to a severe loss of frame-wise quality and prompt fidelity, as quantified in Table 3. While this approach integrates information across local and distant video frames, it does not produce high-quality long videos, as the resulting video suffers from low image quality, completely failing for high-quality long video generation.
In contrast, SynCoS effectively couples both stages, leveraging the output of the first stage as a refining source for the second stage, which enhances inter-sample long-range consistency. This integration enables high-quality, long video generation with multiple prompts, achieving smooth transitions, semantic consistency throughout the video, and strong prompt fidelity for each chunk.
Ablation study on stride. By default, we set the stride (step size between chunks) to 4 for CogVideoX-2B without extensive tuning for each scenario. For prompts with frequent content changes, reducing the stride improves long-term consistency by increasing information sharing across overlaps. Users can adjust the stride to balance vibrant changes (larger stride) and stronger synchronization (smaller stride), enhancing content consistency. Figure 10 provides qualitative evidence: while a stride of 4 introduces slight variations in the knight’s appearance, a stride of 1 ensures greater consistency throughout.
(a) stride=1
(b) stride=4
Figure 10. Qualitative ablation study on stride. Reducing the stride enhances content consistency, which is beneficial in scenarios like a knight running as the background transitions from grass to snow.
Although reducing the stride increases computation time slightly—from 55 minutes (Table 4) to 60 minutes—the impact is minimal. This is because stride reduction does not affect the second stage optimization time, where most of the computational overhead occurs. Instead of processing all chunks simultaneously in this stage, SynCoS uses a minibatch approach, randomly selecting B chunks from the total N chunks at each iteration. Since B remains the same for both stride 1 and stride 4, the overall optimization cost remains largely unaffected.
D. Further discussions on previous work
D.1. Limitations of previous approaches
Following the recent success of T2V models, several works have explored extending video diffusion models for longer video generation without additional training. However, we observe that current tuning-free approaches often exhibit undesirable artifacts when applied to recent video diffusion models. In the following sections, we will detail the limitations and failures of these existing methods.
Discussion on FIFO-Diffusion. In video diffusion models, all frames within a single video chunk are processed at the same timestep during both training and inference. FIFOdiffusion [21] proposes a new sampling technique that uses a queue to store a series of consecutive frames, with the timestep increasing for each frame. While this approach enables the generation of infinitely long videos, it introduces avoidable discrepancies between the timesteps used during training and those used during inference. These discrepancies become more pronounced when applied to recent video diffusion models. This is because as the number of frames processed within a single chunk increases, the timestep gap between frames in the same chunk widens. For example, FIFO-diffusion is not susceptible to per-frame artifacts on CogVideoX-2B [55], which encodes 13 frames per chunk. However, in Open-Sora Plan (v1.3) [23], which encodes 24 frames per chunk, these artifacts become significantly more noticeable, as shown in Figure 12. We note that in contrast to existing tuning-free long video generation methods, SynCoS does not introduce training-inference discrepancy and can be seamlessly applied to any video diffusion model.
Discussion on FreeNoise. The video diffusion models asis often lack the capability to maintain content consistency across different video chunks beyond a single video chunk. FreeNoise [36] addresses this limitation by fusing attention features from temporal layers to establish long-range temporal correlations between different chunks. While this method works effectively with earlier video diffusion models that separate spatial and temporal attention layers (i.e., enabling the fusion of only temporal attention features), we observe that it is not applicable to newer video diffusion models, as illustrated in Figure 12. This limitation arises because recent DiT-based models, widely used in frontier T2V approaches [23, 55], lack dedicated temporal layers. Instead, these models tokenize an entire video chunk into patches and apply attention across all patches. As a result, fusing all attention features without additional considerations severely degrades performance, demonstrating that naive fusion techniques are unsuitable for DiT-based models in long video generation.
4


D.2. Additional comparisons with architecture-specific approaches
In our main experiment (Section 5), we primarily compare SynCoS with architecture-compatible, tuning-free methods for multi-event long video generation. These approaches remain compatible with newer diffusion models, leveraging backbone improvements for enhanced generation quality. However, several existing methods [5, 29, 45] are restricted to specific diffusion backbones, making them incompatible with newer or alternative models and limiting their performance to existing architectures. Nonetheless, to ensure a comprehensive and fair comparison, we implement SynCoS using the respective architectures of existing tuning-free methods and evaluate its performance in Table 5. We compare SynCoS with VideoInfinity [45] by applying SynCoS to VideoCrafter2 [52], a U-Net-based video diffusion model. Additionally, to compare against DitCtrl [5] (built on Multi-Modal Diffusion Transformer (MM-DiT)), we evaluate SynCoS under the same backbone of CogVideoX-2B [55]. Notably, SynCoS significantly outperforms all baselines in temporal consistency and video quality while achieving comparable prompt fidelity, demonstrating its robustness across various diffusion backbones.
Table 5. Quantitative comparison with architecture-specific baselines. *Abbreviations: subject consistency (SC), background consistency (BC), aesthetic quality (AQ), and prompt fidelity (PF).
Temporal Frame Semantics Backbone Method SC ↑ BC ↑ AQ ↑ PF ↑
VideoCrafter2 [52]
Video-Infinity [45] 0.879 0.943 0.645 0.365 SynCoS 0.911 0.947 0.648 0.365
CogVideoX-2B [55]
DitCtrl [5] 0.821 0.916 0.635 0.394 SynCoS 0.864 0.927 0.643 0.381
Comparisons with autoregressive generation using I2V. Long videos can also be generated using image-to-video (I2V) models by generating a single video chunk with T2V and then conditioning the last frame of the previous chunk to the I2V model. For two reasons, we do not directly include autoregressive generation using the I2V model in the main paper. First, using an I2V model requires a different prompt structure. Unlike our structured prompts, which focus on describing changes in local chunks, I2V models require explicit descriptions of transitions between chunks. For example, to generate an artistic video of a butterfly on a flower across changing seasons, our structured prompts might include: (1) “In spring, a butterfly is on a flower,” and (2) “In summer, a butterfly is on a flower.” In contrast, I2V prompts would need to explicitly describe transitions, such as: (1) “In spring, a butterfly is on a flower,” and (2) “The season changes from spring to summer, and the butterfly is on a flower.” Second, the base models used in our experiments do not support I2V generation.
Nonetheless, to comprehensively analyze long video generation, we compare SynCoS with an I2V model (CogVideo5X-I2V) that uses a backbone supporting I2V generation and appropriately tuned prompts. As shown in Figure 13, I2V-based autoregressive generation enables smooth scene transitions (e.g., a Chihuahua floating in space transitioning to the Chihuahua swimming in the ocean). However, it often struggles to introduce new objects (e.g., a brown squirrel and a white squirrel, or a Chihuahua with fish), limiting its ability to generate long videos that naturally add or change components over time.
5


1st chunk 0 sec 15 sec 30 sec
Local 1 The astronaut snowboards down a snowy hill
Global prompt An astronaut in a white uniform, equipped with a helmet and gear.
2nd chunk 3rd chunk
FIFO
Local 2 The astronaut surfs in the sea Local 3 The astronaut surfs across desert dunes
SynCoS (Ours)
(a) Long video generation on CogVideoX-2B, where a single video chunk consists of 26 frames.
1st chunk 0 sec 10 sec 20 sec
Local 1 The astronaut snowboards down a snowy hill
Global prompt An astronaut in a white uniform, equipped with a helmet and gear.
2nd chunk 3rd chunk
FIFO
SynCoS (Ours)
Local 2 The astronaut surfs in the sea Local 3 The astronaut surfs across desert dunes
(b) Long video generation on Open-Sora Plan (v1.3), where a single video chunk consists of 49 frames.
Figure 11. Long video generation results. All generated videos are 4 times longer than the underlying base model. FIFO significantly suffers from noise-like artifacts on Open-Sora Plan (v1.3) due to inevitable training-inference discrepancy in their design.
0 sec 15 sec 30 sec
Global prompt A panda sitting on a wooden stole is playing an acoustic guitar. The background includes a flowing stream and vibrant green foliage.
FreeNoise
Figure 12. Long video generation result of FreeNoise on CogVideoX-2B. This generated video is 4 times longer than the underlying base model. Each frame is generated well in the early frames, where no fusion is applied. However, as soon as the fusion of attentional features is applied, the generated video shows stagnant results of repeated object motion without any scene changes, eventually leading to the entire failure of video generation.
⋯
SynCoS
(Ours)
Prompt 1 A cute brown squirrel is in the icy environment of Antarctica, perched on a pile of hazelnuts. Prompt 2 Alongside it is a cute white squirrel, and the two sit together on the pile of nuts.
⋯
I2V
(Autoregressive)
SynCoS
(Ours)
I2V
(Autoregressive)
Prompt 2 A Chihuahua wearing an astronaut is swimming in the ocean. Prompt 3 A Chihuahua is swims alongside colorful fish.
⋯⋯
Prompt 1 A Chihuahua wearing an astronaut floats in space.
⋯⋯
Figure 13. Qualitative comparison with autoregressive generation using image-to-video (I2V) model for long videos. While autoregressive generation with I2V models effectively handles scene transitions, it often struggles to introduce new components into the video.
6


E. Pseudo-code implementation of each stage.
We describe each stage with pseudo-code implementations. In the first stage of temporal co-denoising with DDIM, the video is divided into overlapping chunks, where each chunk undergoes a diffusion forward pass and a DDIM update to compute x(i)
0 . The chunks are then fused to obtain x′0, as
detailed in Figure 14. In the second stage, the fused x′0 is further refined by re-dividing it into overlapping chunks and applying CSD-based optimization. While we use CSDbased optimization to enforce global coherence, it differs from the original CSD, as the second stage is adjusted using a grounded timestep and a fixed baseline noise, ensuring synchronous coupling across all three stages. The iterative refinement process is illustrated in Figure 15. Lastly, the refined x′0 from the second stage is converted using baseline noise to prepare for subsequent steps in the diffusion pipeline.
F. Additional qualitative results
We consider long videos of multiple events with the following challenging scenarios: object motion control, cinematic effects, storytelling, camera control, background changes, physical transformations, complex scene transitions, and compositional generation. For generated videos handling object motion control and camera control scenes, please refer to Figure 1, and other video examples are visualized in Figure 16 and Figure 17. To view all generated videos, refer to our project page: https://syncos2025.github.io/.
7


1 def first_stage( 2 videos , # video tensor of shape [T, C, H, W] 3 prompt_embeds_for_chunks , # Text prompt embeddings for each chunk 4 model , # The denoising model 5 scheduler , # Scheduler to refine videos 6 guidance_scale , # Scale factor for classifier -free guidance 7 timestep , # Current timestep in the diffusion process 8 stride # Stride size for dividing videos 9 ): 10
11 # Initialize list to store video chunks and count tensor 12 videos_for_chunks = [] 13 count = torch . zeros_like ( videos ) 14
15 # Split videos into strides for diffusion forward 16 for start in range (0 , videos . shape [0] , stride ) : 17 # Determine the end of the current stride 18 end = start + stride 19
20 # Slice the videos for the current stride 21 chunks = videos [ start : end ] 22
23 # Increment count for the covered range 24 count [ start : end ] += 1 25
26 # Append the current stride to the list for processing 27 videos_for_chunks . append ( chunks ) 28
29 # Concatenate all processed video chunks into a single tensor for inference 30 videos_for_chunks_input = torch . cat ( videos_for_chunks , dim =0) 31
32 # Diffusion forward 33 noise_pred = predict_noise ( model , videos_for_chunks_input , prompt_embeds_for_chunks , timestep ) 34
35 # Apply classifier - free guidance to refine noise predictions 36 noise_pred = apply_classifier_free_guidance ( noise_pred , guidance_scale ) 37
38 # Perform a scheduler step to update videos and extract the denoised sample 39 res = scheduler_step ( scheduler , noise_pred , timestep , videos_for_chunks ) 40 videos_x0 = res . pred_original_sample 41
42 # Initialize the aggregated result tensor 43 value_x0 = torch . zeros_like ( videos ) 44
45 # Aggregate the results into ` value_x0 ` 46 idx = 0 47 for start in range (0 , videos . shape [0] , stride ) : 48 # Define the range for the current stride 49 start + stride 50
51 # Accumulate the processed values for the corresponding stride 52 value_x0 [ start : end ] += videos_x0 [ idx ] 53 idx += 1 54
55 # Compute the final denoised prediction by fusing 56 # Wherever ` count > 0`, compute the averaged value , else fallback to ` value_x0 ` 57 pred_original_sample = torch . where ( count > 0 , value_x0 / count , value_x0 ) 58
59 return pred_original_sample
Figure 14. Pseudo-code implementation of the first stage of SynCoS.
8


1 def second_stage( 2 pred_original_sample , # Initial video prediction 3 prompt_embeds_for_chunks , # Text prompt embeddings for each chunk 4 timestep , # A grounded timestep 5 cfg_scale , # Guidance scale for classifier -free guidance 6 opt # Optimization hyperparameters and options 7 ): 8 9
10 # Initialize target predictions with gradients enabled 11 tgt_pred_sample = pred_original_sample . clone () . detach () 12 tgt_pred_sample . requires_grad = True 13
14 # Optimization hyperparameters 15 lr = opt . lr 16 wd = opt . wd 17 decay_iter = opt . decay_iter 18 decay_rate = opt . decay_rate 19 num_steps = opt . num_steps 20 batch_size = opt . batch_size 21 stride = opt . stride 22
23 # Initialize optimizer and learning rate scheduler 24 optimizer = opt . optimizer ([ tgt_pred_sample ] , lr = lr , weight_decay = wd ) 25 scheduler = opt . scheduler ( optimizer , step_size = decay_iter , gamma = decay_rate ) 26
27 # Fix baseline noise for the entire tensor 28 base_noise_all = torch . randn_like ( tgt_pred_sample ) 29
30 for step in range ( num_steps ) : 31 optimizer . zero_grad () 32
33 # Initialize tensors for counting and score accumulation 34 count = torch . zeros_like ( tgt_pred_sample , dtype = tgt_pred_sample . dtype ) 35 score_value = torch . zeros_like ( tgt_pred_sample , dtype = tgt_pred_sample . dtype ) 36
37 # Initialize lists to store videos and noise chunks 38 videos_for_chunks = [] 39 basenoise_chunks = [] 40 prompt_embeds_chunks = [] 41
42 # Determine valid subset indices for this iteration 43 num_chunks = ( tgt_pred_sample . shape [0] + stride - 1) // stride 44 subset_indices = torch . randperm ( num_chunks ) [: batch_size ] 45
46 # Process selected chunks 47 for i , start in enumerate ( range (0 , tgt_pred_sample . shape [0] , stride ) ) : 48 if i not in subset_indices : 49 continue 50
51 # Determine the end of the current stride 52 end = start + stride 53
54 # Slice the videos for the current stride and add noise 55 chunks = tgt_pred_sample [ start : end ] 56 noisy_chunks = add_noise ( chunks , base_noise_all [ start : end ] , timestep ) 57
58 basenoise_chunks . append ( base_noise_all [ start : end ]. unsqueeze (0) ) 59 prompt_embeds_chunks . append ( prompt_embeds_for_chunks [ i ]) 60
61 # Update count and append to chunks list 62 count [ start : end ] += 1 63 videos_for_chunks . append ( noisy_chunks . unsqueeze (0) ) 64
65 # Concatenate videos for model inference 66 videos_for_chunks_input = torch . cat ( videos_for_chunks , dim =0) 67 basenoise_chunks = torch . cat ( basenoise_chunks , dim =0) 68 prompt_embeds_chunks = torch . cat ( prompt_embeds_chunks , dim =0) 69
70 # Predict noise using the model 71 noise_pred = predict_noise ( opt . model , videos_for_chunks_input , prompt_embeds_chunks , timestep ) 72
73 # Apply classifier - free guidance 74 noise_pred = apply_classifier_free_guidance ( noise_pred , cfg_scale ) 75
76 # Compute CSD scores 77 scores = calculate_csd_loss ( videos_for_chunks_input , noise_pred , basenoise_chunks ) 78
79 # Aggregate scores back to the appropriate positions 80 for i , start in enumerate ( range (0 , tgt_pred_sample . shape [0] , stride ) ) : 81 if i not in subset_indices : 82 continue 83
84 # Determine the end of the current stride 85 end = start + stride 86
87 # Accumulate scores for the current range 88 score_value [ start : end ] += scores [ start : end ] 89
90 # Compute gradients using scores normalized by count 91 grad_all = torch . where ( count > 0 , score_value / count , score_value ) 92 tgt_pred_sample . backward ( gradient = grad_all ) 93
94 # Perform optimization step 95 optimizer . step () 96 scheduler . step () 97
98 # Detach and return the updated target prediction 99 tgt_pred_sample = tgt_pred_sample . clone () . detach () . requires_grad_ ( False ) 100
101 return tgt_pred_sample , base_noise_all
Figure 15. Pseudo-code implementation of the second stage of SynCoS.
9


0 sec 5 sec 10 sec 15 sec
1
st
chunk 2
nd
chunk
Local 1
The figure continues its stride
Local 2
Lightning flashes in the background
Local 3
Purple smoke emits from the water figure
Global prompt
A humanoid figure made entirely of water walks forward
3
rd
chunk
ڮ
ڮ
0 sec 5 sec 10 sec
1
st
chunk
2
nd
chunk
Local 1
The rainbow blob bursts with vibrant energy
Local 2
An apple emerges from the explosion of rainbow paint
Global prompt
A large blob of splashing rainbow paint explodes in stunning detail, 8k
ڮ
0 sec 5 sec 10 sec
1
st
chunk
2
nd
chunk
ڮ
Local 1
The astronaut dances energetically
Local 2
Colorful fireworks explode in the background
Global prompt
An astronaut dances on Mars
(a) Cinematic effects
0 sec 5 sec 10 sec 15 sec 20 sec
1
st
chunk 2
nd
chunk 3
rd
chunk 4
th
chunk
⋯
⋯
⋯
Local 1
It appears in
colorful comic book panels
Local 3
Dances
Local 2
Leaps from the comics to the real world
Local 4
Sits down
and
rests
⋯
⋯
⋯
9 103 166 210 261 329 386 485
Global prompt 1
A cartoon-style bear
Global prompt 2
A realistic bear
(b) Storytelling
0 sec 10 sec 20 sec
2
nd
chunk
Local 1
In spring͕ ͕͙ ŽŶ Ă blooming flowers
Local 2
In summer͕ ͕͙ ŽŶ Ă bright flowers
Local 3
In autumn͕ ͕͙ ŽŶ Ă fading flowers
Local 4
In winter͕ ͕͙ ŽŶ Ă frosty flowers
Global prompt
A delicate white butterfly sits on a flower
1
st
chunk 3
rd
chunk 4
sth
chunk 10 sec 15 sec
ڮ ڮ ڮ
0 sec 5 sec 10 sec 15 sec
1
st
chunk 2
nd
chunk
Local 1
The astronaut snowboards down a snowy hill
Local 2
The astronaut surfs in the sea
Local 3
The astronaut rides across desert dunes like a surfer
Global prompt
An astronaut in a white suit with a reflective helmet
3
rd
chunk
ڮ
ڮ
(c) Background changes
Figure 16. Multi-event long video generation results showcasing challenging scenarios, including cinematic effects, storytelling, and background changes. Each example is 2-4 times longer in duration compared to the underlying base model, resulting in 11 to 21-second videos at 24 fps, with a total of 256 to 512 frames.
10


0 sec 5 sec 10 sec 15 sec
1
st
chunk 2
nd
chunk
ڮ
Local 1
The candle is brightly lit
Local 2
Smoke rises gently and the flame flickers
Local 3
The candle is extinguished and remains unlit with no flame or light 9 103
Global prompt
Closed shot of a candle presented in a dark room. The camera locked down.
3
rd
chunk
ڮ
ڮ
(d) Physical transformation
0 sec 10 sec 20 sec
2
nd
chunk
Local 1
The Chihuahua floats weightlessly in space
Local 2
The Chihuahua dances, playfully stepping
Local 3
The Chihuahua swims underwater
Local 4
The Chihuahua swims alongside colorful fish
Global prompt
A Chihuahua wearing an astronaut suit
1
st
chunk 3
rd
chunk 4
sth
chunk 10 sec 15 sec
ڮ ڮ ڮ
0 sec 10 sec 20 sec
2
nd
chunk
Local 1
It paddles gently
Local 2
It swims alongside colorful fish
Local 3
It nestles among coral reefs
Local 4
Large sharks suddenly appears
Global prompt
A fluffy brown teddy bear with soft fur swims underwater in a serene, magical ocean
1
st
chunk 3
rd
chunk 4
sth
chunk 10 sec 15 sec
ڮ ڮ ڮ
(e) Complex scene transitions
0 sec 5 sec 10 sec
1
st
chunk
2
nd
chunk
Local 1
The astronaut dances energetically.
Local 2
Beside it, a cute white squirrel sits
Global prompt
A cute brown squirrel sits in the vast, icy environment of Antarctica, perched on a pile of hazelnuts
ڮ
0 sec 5 sec 10 sec
1
st
chunk
2
nd
chunk
Local 1
The knight strides confidently, his armor clinking softly
Local 2
Accompanying him is a wise wizard, walking beside him
Global prompt
A brave young knight journeys through a dense forest, the trees towering above as sunlight filters through the leaves
ڮ
(f) Compositional generation
Figure 17. Multi-event long video generation results showcasing challenging scenarios, including physical transformation, complex scene transitions, and compositional generation. Each example is 2-4 times longer in duration compared to the underlying base model, resulting in 11 to 21-second videos at 24 fps, with a total of 256 to 512 frames.
11