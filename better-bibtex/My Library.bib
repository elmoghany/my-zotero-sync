@misc{446South2nd,
  title = {446 {{South}} 2nd {{Street}}, {{San Jose}}, {{CA}} 95113 - {{Google Search}}},
  urldate = {2025-04-23},
  howpublished = {https://www.google.com/search?q=446+South+2nd+Street\%2C+San+Jose\%2C+CA+95113\&rlz=1C1CHBD\_en-GBEG1020EG1020\&sourceid=chrome\&ie=UTF-8},
  file = {C:\Users\melmoghany\Zotero\storage\8EDUP9S4\search.html}
}

@misc{aiMAGI1AutoregressiveVideo2025,
  title = {{{MAGI-1}}: {{Autoregressive Video Generation}} at {{Scale}}},
  shorttitle = {{{MAGI-1}}},
  author = {{ai}, Sand and Teng, Hansi and Jia, Hongyu and Sun, Lei and Li, Lingzhi and Li, Maolin and Tang, Mingqiu and Han, Shuai and Zhang, Tianning and Zhang, W. Q. and Luo, Weifeng and Kang, Xiaoyang and Sun, Yuchen and Cao, Yue and Huang, Yunpeng and Lin, Yutong and Fang, Yuxin and Tao, Zewei and Zhang, Zheng and Wang, Zhongshu and Liu, Zixun and Shi, Dai and Su, Guoli and Sun, Hanwen and Pan, Hong and Wang, Jie and Sheng, Jiexin and Cui, Min and Hu, Min and Yan, Ming and Yin, Shucheng and Zhang, Siran and Liu, Tingting and Yin, Xianping and Yang, Xiaoyu and Song, Xin and Hu, Xuan and Zhang, Yankai and Li, Yuqiao},
  year = {2025},
  month = may,
  number = {arXiv:2505.13211},
  eprint = {2505.13211},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13211},
  urldate = {2025-05-31},
  abstract = {We present MAGI-1, a world model that generates videos by autoregressively predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 facilitates controllable generation via chunk-wise prompting and supports real-time, memory-efficient deployment by maintaining constant peak inference cost, regardless of video length. The largest variant of MAGI-1 comprises 24 billion parameters and supports context lengths of up to 4 million tokens, demonstrating the scalability and robustness of our approach. The code and models are available at https://github.com/SandAI-org/MAGI-1 and https://github.com/SandAI-org/MagiAttention. The product can be accessed at https://sand.ai.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\X8LFQ4NC\\ai et al. - 2025 - MAGI-1 Autoregressive Video Generation at Scale.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\HIHDDCJP\\2505.html}
}

@misc{aiMAGI1AutoregressiveVideo2025a,
  title = {{{MAGI-1}}: {{Autoregressive Video Generation}} at {{Scale}}},
  shorttitle = {{{MAGI-1}}},
  author = {{ai}, Sand and Teng, Hansi and Jia, Hongyu and Sun, Lei and Li, Lingzhi and Li, Maolin and Tang, Mingqiu and Han, Shuai and Zhang, Tianning and Zhang, W. Q. and Luo, Weifeng and Kang, Xiaoyang and Sun, Yuchen and Cao, Yue and Huang, Yunpeng and Lin, Yutong and Fang, Yuxin and Tao, Zewei and Zhang, Zheng and Wang, Zhongshu and Liu, Zixun and Shi, Dai and Su, Guoli and Sun, Hanwen and Pan, Hong and Wang, Jie and Sheng, Jiexin and Cui, Min and Hu, Min and Yan, Ming and Yin, Shucheng and Zhang, Siran and Liu, Tingting and Yin, Xianping and Yang, Xiaoyu and Song, Xin and Hu, Xuan and Zhang, Yankai and Li, Yuqiao},
  year = {2025},
  month = may,
  number = {arXiv:2505.13211},
  eprint = {2505.13211},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13211},
  urldate = {2025-06-18},
  abstract = {We present MAGI-1, a world model that generates videos by autoregressively predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 facilitates controllable generation via chunk-wise prompting and supports real-time, memory-efficient deployment by maintaining constant peak inference cost, regardless of video length. The largest variant of MAGI-1 comprises 24 billion parameters and supports context lengths of up to 4 million tokens, demonstrating the scalability and robustness of our approach. The code and models are available at https://github.com/SandAI-org/MAGI-1 and https://github.com/SandAI-org/MagiAttention. The product can be accessed at https://sand.ai.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\JP8D6CXU\\ai et al. - 2025 - MAGI-1 Autoregressive Video Generation at Scale.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\PVNEUKPM\\2505.html}
}

@misc{ararPALPPromptAligned2024,
  title = {{{PALP}}: {{Prompt Aligned Personalization}} of {{Text-to-Image Models}}},
  shorttitle = {{{PALP}}},
  author = {Arar, Moab and Voynov, Andrey and Hertz, Amir and Avrahami, Omri and Fruchter, Shlomi and Pritch, Yael and {Cohen-Or}, Daniel and Shamir, Ariel},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06105},
  eprint = {2401.06105},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.06105},
  urldate = {2025-03-20},
  abstract = {Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a {\textbackslash}emph\{single\} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our method in multi- and single-shot settings and further show that it can compose multiple subjects or use inspiration from reference images, such as artworks. We compare our approach quantitatively and qualitatively with existing baselines and state-of-the-art techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  note = {Comment: Project page available at https://prompt-aligned.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\ALALK32S\\Arar et al. - 2024 - PALP Prompt Aligned Personalization of Text-to-Image Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\UPE59K7D\\2401.html}
}

@misc{atzmonMotionQueriesIdentityMotion2025,
  title = {Motion by {{Queries}}: {{Identity-Motion Trade-offs}} in {{Text-to-Video Generation}}},
  shorttitle = {Motion by {{Queries}}},
  author = {Atzmon, Yuval and Gal, Rinon and Tewel, Yoad and Kasten, Yoni and Chechik, Gal},
  year = {2025},
  month = mar,
  number = {arXiv:2412.07750},
  eprint = {2412.07750},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.07750},
  urldate = {2025-03-19},
  abstract = {Text-to-video diffusion models have shown remarkable progress in generating coherent video clips from textual descriptions. However, the interplay between motion, structure, and identity representations in these models remains under-explored. Here, we investigate how self-attention query features (a.k.a. Q features) simultaneously govern motion, structure, and identity and examine the challenges arising when these representations interact. Our analysis reveals that Q affects not only layout, but that during denoising Q also has a strong effect on subject identity, making it hard to transfer motion without the side-effect of transferring identity. Understanding this dual role enabled us to control query feature injection (Q injection) and demonstrate two applications: (1) a zero-shot motion transfer method that is 20 times more efficient than existing approaches, and (2) a training-free technique for consistent multi-shot video generation, where characters maintain identity across multiple video shots while Q injection enhances motion fidelity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: (1) Project page: https://research.nvidia.com/labs/par/MotionByQueries/ (2) The methods and results in section 5, "Consistent multi-shot video generation", are based on the arXiv version 1 (v1) of this work. Here, in version 2 (v2), we extend and further analyze those findings to efficient motion transfer},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\AD2VR3JF\\Atzmon et al. - 2025 - Motion by Queries Identity-Motion Trade-offs in Text-to-Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\7GTRJH3X\\2412.html}
}

@misc{biCustomTTTMotionAppearance2024,
  title = {{{CustomTTT}}: {{Motion}} and {{Appearance Customized Video Generation}} via {{Test-Time Training}}},
  shorttitle = {{{CustomTTT}}},
  author = {Bi, Xiuli and Lu, Jian and Liu, Bo and Cun, Xiaodong and Zhang, Yong and Li, Weisheng and Xiao, Bin},
  year = {2024},
  month = dec,
  number = {arXiv:2412.15646},
  eprint = {2412.15646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.15646},
  urldate = {2025-03-19},
  abstract = {Benefiting from large-scale pre-training of text-video pairs, current text-to-video (T2V) diffusion models can generate high-quality videos from the text description. Besides, given some reference images or videos, the parameter-efficient fine-tuning method, i.e. LoRA, can generate high-quality customized concepts, e.g., the specific subject or the motions from a reference video. However, combining the trained multiple concepts from different references into a single network shows obvious artifacts. To this end, we propose CustomTTT, where we can joint custom the appearance and the motion of the given video easily. In detail, we first analyze the prompt influence in the current video diffusion model and find the LoRAs are only needed for the specific layers for appearance and motion customization. Besides, since each LoRA is trained individually, we propose a novel test-time training technique to update parameters after combination utilizing the trained customized models. We conduct detailed experiments to verify the effectiveness of the proposed methods. Our method outperforms several state-of-the-art works in both qualitative and quantitative evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted in AAAI 2025. Project Page: https://customttt.github.io/ Code: https://github.com/RongPiKing/CustomTTT},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\7MV28QFN\\Bi et al. - 2024 - CustomTTT Motion and Appearance Customized Video Generation via Test-Time Training.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\IJ3U4EYQ\\2412.html}
}

@misc{blattmannStableVideoDiffusion2023,
  title = {Stable {{Video Diffusion}}: {{Scaling Latent Video Diffusion Models}} to {{Large Datasets}}},
  shorttitle = {Stable {{Video Diffusion}}},
  author = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15127},
  eprint = {2311.15127},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.15127},
  urldate = {2025-04-24},
  abstract = {We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\I8MU7HH2\\Blattmann et al. - 2023 - Stable Video Diffusion Scaling Latent Video Diffusion Models to Large Datasets.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\9674CY62\\2311.html}
}

@misc{chenDocumentHaystacksVisionLanguage2024,
  title = {Document {{Haystacks}}: {{Vision-Language Reasoning Over Piles}} of 1000+ {{Documents}}},
  shorttitle = {Document {{Haystacks}}},
  author = {Chen, Jun and Xu, Dannong and Fei, Junjie and Feng, Chun-Mei and Elhoseiny, Mohamed},
  year = {2024},
  month = dec,
  number = {arXiv:2411.16740},
  eprint = {2411.16740},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.16740},
  urldate = {2025-03-23},
  abstract = {Large multimodal models (LMMs) have achieved impressive progress in vision-language understanding, yet they face limitations in real-world applications requiring complex reasoning over a large number of images. Existing benchmarks for multi-image question-answering are limited in scope, each question is paired with only up to 30 images, which does not fully capture the demands of large-scale retrieval tasks encountered in the real-world usages. To reduce these gaps, we introduce two document haystack benchmarks, dubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on large-scale visual document retrieval and understanding. Additionally, we propose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG) framework that leverages a suite of multimodal vision encoders, each optimized for specific strengths, and a dedicated question-document relevance module. V-RAG sets a new standard, with a 9\% and 11\% improvement in Recall@1 on the challenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively, compared to the previous best baseline models. Additionally, integrating V-RAG with LMMs enables them to efficiently operate across thousands of images, yielding significant improvements on our DocHaystack and InfoHaystack benchmarks. Our code and datasets are available at https://github.com/Vision-CAIR/dochaystacks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: the correct arxiv version},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\KY55R9PE\\Chen et al. - 2024 - Document Haystacks Vision-Language Reasoning Over Piles of 1000+ Documents.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\3W7V77I2\\2411.html}
}

@misc{chenMultisubjectOpensetPersonalization2025,
  title = {Multi-Subject {{Open-set Personalization}} in {{Video Generation}}},
  author = {Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Fang, Yuwei and Lee, Kwot Sin and Skorokhodov, Ivan and Aberman, Kfir and Zhu, Jun-Yan and Yang, Ming-Hsuan and Tulyakov, Sergey},
  year = {2025},
  month = mar,
  number = {arXiv:2501.06187},
  eprint = {2501.06187},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.06187},
  urldate = {2025-06-18},
  abstract = {Video personalization methods allow us to synthesize videos with specific concepts such as people, pets, and places. However, existing methods often focus on limited domains, require time-consuming optimization per subject, or support only a single subject. We present Video Alchemist \$-\$ a video model with built-in multi-subject, open-set personalization capabilities for both foreground objects and background, eliminating the need for time-consuming test-time optimization. Our model is built on a new Diffusion Transformer module that fuses each conditional reference image and its corresponding subject-level text prompt with cross-attention layers. Developing such a large model presents two main challenges: dataset and evaluation. First, as paired datasets of reference images and videos are extremely hard to collect, we sample selected video frames as reference images and synthesize a clip of the target video. However, while models can easily denoise training videos given reference frames, they fail to generalize to new contexts. To mitigate this issue, we design a new automatic data construction pipeline with extensive image augmentations. Second, evaluating open-set video personalization is a challenge in itself. To address this, we introduce a personalization benchmark that focuses on accurate subject fidelity and supports diverse personalization scenarios. Finally, our extensive experiments show that our method significantly outperforms existing personalization methods in both quantitative and qualitative evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: CVPR 2025. Project page: https://snap-research.github.io/open-set-video-personalization/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\RJ7ZU2LQ\\Chen et al. - 2025 - Multi-subject Open-set Personalization in Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZVCV4TS3\\2501.html}
}

@misc{chenOuroborosDiffusionExploringConsistent2025,
  title = {Ouroboros-{{Diffusion}}: {{Exploring Consistent Content Generation}} in {{Tuning-free Long Video Diffusion}}},
  shorttitle = {Ouroboros-{{Diffusion}}},
  author = {Chen, Jingyuan and Long, Fuchen and An, Jie and Qiu, Zhaofan and Yao, Ting and Luo, Jiebo and Mei, Tao},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09019},
  eprint = {2501.09019},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09019},
  urldate = {2025-03-20},
  abstract = {The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\D8P9EYR8\\Chen et al. - 2025 - Ouroboros-Diffusion Exploring Consistent Content Generation in Tuning-free Long Video Diffusion.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZCEJIGZF\\2501.html}
}

@misc{chenSkyReelsV2InfinitelengthFilm2025,
  title = {{{SkyReels-V2}}: {{Infinite-length Film Generative Model}}},
  shorttitle = {{{SkyReels-V2}}},
  author = {Chen, Guibin and Lin, Dixuan and Yang, Jiangping and Lin, Chunze and Zhu, Junchen and Fan, Mingyuan and Zhang, Hao and Chen, Sheng and Chen, Zheng and Ma, Chengcheng and Xiong, Weiming and Wang, Wei and Pang, Nuo and Kang, Kang and Xu, Zhiheng and Jin, Yuzhe and Liang, Yupeng and Song, Yubing and Zhao, Peng and Xu, Boyuan and Qiu, Di and Li, Debang and Fei, Zhengcong and Li, Yang and Zhou, Yahui},
  year = {2025},
  month = apr,
  number = {arXiv:2504.13074},
  eprint = {2504.13074},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.13074},
  urldate = {2025-06-18},
  abstract = {Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at https://github.com/SkyworkAI/SkyReels-V2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 31 pages,10 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\WQSM3WQK\\Chen et al. - 2025 - SkyReels-V2 Infinite-length Film Generative Model.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\8EF3VI5H\\2504.html}
}

@misc{choSoraAGIWorld2024,
  title = {Sora as an {{AGI World Model}}? {{A Complete Survey}} on {{Text-to-Video Generation}}},
  shorttitle = {Sora as an {{AGI World Model}}?},
  author = {Cho, Joseph and Puspitasari, Fachrina Dewi and Zheng, Sheng and Zheng, Jingyao and Lee, Lik-Hang and Kim, Tae-Ho and Hong, Choong Seon and Zhang, Chaoning},
  year = {2024},
  month = jun,
  number = {arXiv:2403.05131},
  eprint = {2403.05131},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.05131},
  urldate = {2025-06-02},
  abstract = {The evolution of video generation from text, starting with animating MNIST numbers to simulating the physical world with Sora, has progressed at a breakneck speed over the past seven years. While often seen as a superficial expansion of the predecessor text-to-image generation model, text-to-video generation models are developed upon carefully engineered constituents. Here, we systematically discuss these elements consisting of but not limited to core building blocks (vision, language, and temporal) and supporting features from the perspective of their contributions to achieving a world model. We employ the PRISMA framework to curate 97 impactful research articles from renowned scientific databases primarily studying video synthesis using text conditions. Upon minute exploration of these manuscripts, we observe that text-to-video generation involves more intricate technologies beyond the plain extension of text-to-image generation. Our additional review into the shortcomings of Sora-generated videos pinpoints the call for more in-depth studies in various enabling aspects of video generation such as dataset, evaluation metric, efficient architecture, and human-controlled generation. Finally, we conclude that the study of the text-to-video generation may still be in its infancy, requiring contribution from the cross-discipline research community towards its advancement as the first step to realize artificial general intelligence (AGI).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: First complete survey on Text-to-Video Generation, 44 pages, 20 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\IDY5YX6Q\\Cho et al. - 2024 - Sora as an AGI World Model A Complete Survey on Text-to-Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\8GUG28X5\\2403.html}
}

@misc{feiSkyReelsA2ComposeAnything2025,
  title = {{{SkyReels-A2}}: {{Compose Anything}} in {{Video Diffusion Transformers}}},
  shorttitle = {{{SkyReels-A2}}},
  author = {Fei, Zhengcong and Li, Debang and Qiu, Di and Wang, Jiahua and Dou, Yikun and Wang, Rui and Xu, Jingtao and Fan, Mingyuan and Chen, Guibin and Li, Yang and Zhou, Yahui},
  year = {2025},
  month = apr,
  number = {arXiv:2504.02436},
  eprint = {2504.02436},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.02436},
  urldate = {2025-04-11},
  abstract = {This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\TXK9A546\\Fei et al. - 2025 - SkyReels-A2 Compose Anything in Video Diffusion Transformers.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\EVCQIJFA\\2504.html}
}

@misc{gaoSeedance10Exploring2025,
  title = {Seedance 1.0: {{Exploring}} the {{Boundaries}} of {{Video Generation Models}}},
  shorttitle = {Seedance 1.0},
  author = {Gao, Yu and Guo, Haoyuan and Hoang, Tuyen and Huang, Weilin and Jiang, Lu and Kong, Fangyuan and Li, Huixia and Li, Jiashi and Li, Liang and Li, Xiaojie and Li, Xunsong and Li, Yifu and Lin, Shanchuan and Lin, Zhijie and Liu, Jiawei and Liu, Shu and Nie, Xiaonan and Qing, Zhiwu and Ren, Yuxi and Sun, Li and Tian, Zhi and Wang, Rui and Wang, Sen and Wei, Guoqiang and Wu, Guohong and Wu, Jie and Xia, Ruiqi and Xiao, Fei and Xiao, Xuefeng and Yan, Jiangqiao and Yang, Ceyuan and Yang, Jianchao and Yang, Runkai and Yang, Tao and Yang, Yihang and Ye, Zilyu and Zeng, Xuejiao and Zeng, Yan and Zhang, Heng and Zhao, Yang and Zheng, Xiaozheng and Zhu, Peihao and Zou, Jiaxin and Zuo, Feilong},
  year = {2025},
  month = jun,
  number = {arXiv:2506.09113},
  eprint = {2506.09113},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.09113},
  urldate = {2025-06-18},
  abstract = {Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving {\textasciitilde}10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Seedance 1.0 Technical Report},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\PZQCAKNI\\Gao et al. - 2025 - Seedance 1.0 Exploring the Boundaries of Video Generation Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\MJQKSB2H\\2506.html}
}

@misc{guoLongContextTuning2025,
  title = {Long {{Context Tuning}} for {{Video Generation}}},
  author = {Guo, Yuwei and Yang, Ceyuan and Yang, Ziyan and Ma, Zhibei and Lin, Zhijie and Yang, Zhenheng and Lin, Dahua and Jiang, Lu},
  year = {2025},
  month = mar,
  number = {arXiv:2503.10589},
  eprint = {2503.10589},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.10589},
  urldate = {2025-03-19},
  abstract = {Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Page: https://guoyww.github.io/projects/long-context-video/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\M9AUCSAZ\\Guo et al. - 2025 - Long Context Tuning for Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\TWJBEEFQ\\2503.html}
}

@misc{guROICtrlBoostingInstance2024,
  title = {{{ROICtrl}}: {{Boosting Instance Control}} for {{Visual Generation}}},
  shorttitle = {{{ROICtrl}}},
  author = {Gu, Yuchao and Zhou, Yipin and Ye, Yunfan and Nie, Yixin and Yu, Licheng and Ma, Pingchuan and Lin, Kevin Qinghong and Shou, Mike Zheng},
  year = {2024},
  month = nov,
  number = {arXiv:2411.17949},
  eprint = {2411.17949},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.17949},
  urldate = {2025-04-11},
  abstract = {Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons ({\textbackslash}eg, ControlNet, T2I-Adapter) and embedding-based add-ons ({\textbackslash}eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page at https://roictrl.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\87UGBN5Y\\Gu et al. - 2024 - ROICtrl Boosting Instance Control for Visual Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\7RQQT766\\2411.html}
}

@misc{guROICtrlBoostingInstance2024a,
  title = {{{ROICtrl}}: {{Boosting Instance Control}} for {{Visual Generation}}},
  shorttitle = {{{ROICtrl}}},
  author = {Gu, Yuchao and Zhou, Yipin and Ye, Yunfan and Nie, Yixin and Yu, Licheng and Ma, Pingchuan and Lin, Kevin Qinghong and Shou, Mike Zheng},
  year = {2024},
  month = nov,
  number = {arXiv:2411.17949},
  eprint = {2411.17949},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.17949},
  urldate = {2025-04-24},
  abstract = {Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons ({\textbackslash}eg, ControlNet, T2I-Adapter) and embedding-based add-ons ({\textbackslash}eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page at https://roictrl.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\CDXDWXB6\\Gu et al. - 2024 - ROICtrl Boosting Instance Control for Visual Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\H57TYKL2\\2411.html}
}

@misc{hacohenLTXVideoRealtimeVideo2024,
  title = {{{LTX-Video}}: {{Realtime Video Latent Diffusion}}},
  shorttitle = {{{LTX-Video}}},
  author = {HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  year = {2024},
  month = dec,
  number = {arXiv:2501.00103},
  eprint = {2501.00103},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.00103},
  urldate = {2025-06-18},
  abstract = {We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\YU9PQJ7V\\HaCohen et al. - 2024 - LTX-Video Realtime Video Latent Diffusion.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\66V5WIHW\\2501.html}
}

@misc{harveyFlexibleDiffusionModeling2022,
  title = {Flexible {{Diffusion Modeling}} of {{Long Videos}}},
  author = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  year = {2022},
  month = dec,
  number = {arXiv:2205.11495},
  eprint = {2205.11495},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.11495},
  urldate = {2025-05-30},
  abstract = {We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\2V62ET2C\\Harvey et al. - 2022 - Flexible Diffusion Modeling of Long Videos.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\DHF5EMFN\\2205.html}
}

@misc{heAnyStoryUnifiedSingle2025,
  title = {{{AnyStory}}: {{Towards Unified Single}} and {{Multiple Subject Personalization}} in {{Text-to-Image Generation}}},
  shorttitle = {{{AnyStory}}},
  author = {He, Junjie and Tuo, Yuxiang and Chen, Binghui and Zhong, Chongyang and Geng, Yifeng and Bo, Liefeng},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09503},
  eprint = {2501.09503},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09503},
  urldate = {2025-03-20},
  abstract = {Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an "encode-then-route" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Tech report; Project page: https://aigcdesigngroup.github.io/AnyStory/
\par
Dataset: 
\par
1) Laion
\par
[53] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 6
\par
[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, pages 25278--25294, 2022. 6
\par
2) DeepFashion
\par
[17] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images. In CVPR, pages 5337--5345, 2019. 6
\par
3) Objaverse
\par
Objaverse: A universe of annotated 3d objects. In CVPR, pages 1314213153, 2023
\par
``3D data (about 5,600k) is obtained from the Objaverse [11]'' (He et al., 2025, p. 6)
\par
No Code},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\DAQ9XME6\\He et al. - 2025 - AnyStory Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\2MF859LB\\2501.html}
}

@misc{heDreamStoryOpenDomainStory2025,
  title = {{{DreamStory}}: {{Open-Domain Story Visualization}} by {{LLM-Guided Multi-Subject Consistent Diffusion}}},
  shorttitle = {{{DreamStory}}},
  author = {He, Huiguo and Yang, Huan and Tuo, Zixi and Zhou, Yuan and Wang, Qiuyue and Zhang, Yuhang and Liu, Zeyu and Huang, Wenhao and Chao, Hongyang and Yin, Jian},
  year = {2025},
  month = mar,
  number = {arXiv:2407.12899},
  eprint = {2407.12899},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.12899},
  urldate = {2025-03-20},
  abstract = {Story visualization aims to create visually compelling images or videos corresponding to textual narratives. Despite recent advances in diffusion models yielding promising results, existing methods still struggle to create a coherent sequence of subject-consistent frames based solely on a story. To this end, we propose DreamStory, an automatic open-domain story visualization framework by leveraging the LLMs and a novel multi-subject consistent diffusion model. DreamStory consists of (1) an LLM acting as a story director and (2) an innovative Multi-Subject consistent Diffusion model (MSD) for generating consistent multi-subject across the images. First, DreamStory employs the LLM to generate descriptive prompts for subjects and scenes aligned with the story, annotating each scene's subjects for subsequent subject-consistent generation. Second, DreamStory utilizes these detailed subject descriptions to create portraits of the subjects, with these portraits and their corresponding textual information serving as multimodal anchors (guidance). Finally, the MSD uses these multimodal anchors to generate story scenes with consistent multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules ensure appearance and semantic consistency with reference images and text, respectively. Both modules employ masking mechanisms to prevent subject blending. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, which can assess the overall performance of the story visualization framework, subject-identification accuracy, and the consistency of the generation model. Extensive experiments validate the effectiveness of DreamStory in both subjective and objective evaluations. Please visit our project homepage at https://dream-xyz.github.io/dreamstory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  note = {Datasets
\par
1) Story
\par
``free-short-stories 2, and 50 short stories generated by ChatGPT'' (He et al., 2025, p. 7)
\par
2) Training-free
\par
``training-free approach effectively leverages existing large, high-quality datasets (e.g., LAION-5B [45]'' (He et al., 2025, p. 12)
\par
LAION-5B: An open large-scale dataset for training next generation image-text models,'' arXiv preprint arXiv:2210.08402, 2022.
\par
No Code},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\R4AY38PC\\He et al. - 2025 - DreamStory Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\DRCX7T46\\2407.html}
}

@misc{heImprovingMultiSubjectConsistency2025,
  title = {Improving {{Multi-Subject Consistency}} in {{Open-Domain Image Generation}} with {{Isolation}} and {{Reposition Attention}}},
  author = {He, Huiguo and Wang, Qiuyue and Zhou, Yuan and Cai, Yuxuan and Chao, Hongyang and Yin, Jian and Yang, Huan},
  year = {2025},
  month = mar,
  number = {arXiv:2411.19261},
  eprint = {2411.19261},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.19261},
  urldate = {2025-03-20},
  abstract = {Training-free diffusion models have achieved remarkable progress in generating multi-subject consistent images within open-domain scenarios. The key idea of these methods is to incorporate reference subject information within the attention layer. However, existing methods still obtain suboptimal performance when handling numerous subjects. This paper reveals two primary issues contributing to this deficiency. Firstly, the undesired internal attraction between different subjects within the target image can lead to the convergence of multiple subjects into a single entity. Secondly, tokens tend to reference nearby tokens, which reduces the effectiveness of the attention mechanism when there is a significant positional difference between subjects in reference and target images. To address these issues, we propose a training-free diffusion model with Isolation and Reposition Attention, named IR-Diffusion. Specifically, Isolation Attention ensures that multiple subjects in the target image do not reference each other, effectively eliminating the subject convergence. On the other hand, Reposition Attention involves scaling and repositioning subjects in both reference and target images to the same position within the images. This ensures that subjects in the target image can better reference those in the reference image, thereby maintaining better consistency. Extensive experiments demonstrate that IR-Diffusion significantly enhances multi-subject consistency, outperforming all existing methods in open-domain scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\5SCLNSCC\\He et al. - 2025 - Improving Multi-Subject Consistency in Open-Domain Image Generation with Isolation and Reposition At.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\JPH669ZN\\2411.html}
}

@misc{heLatentVideoDiffusion2023,
  title = {Latent {{Video Diffusion Models}} for {{High-Fidelity Long Video Generation}}},
  author = {He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  year = {2023},
  month = mar,
  number = {arXiv:2211.13221},
  eprint = {2211.13221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.13221},
  urldate = {2025-05-30},
  abstract = {AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models have shown remarkable results recently but require significant computational resources. To address this, we introduce lightweight video diffusion models by leveraging a low-dimensional 3D latent space, significantly outperforming previous pixel-space video diffusion models under a limited computational budget. In addition, we propose hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced. To further overcome the performance degradation issue for long video generation, we propose conditional latent perturbation and unconditional guidance that effectively mitigate the accumulated errors during the extension of video length. Extensive experiments on small domain datasets of different categories suggest that our framework generates more realistic and longer videos than previous strong baselines. We additionally provide an extension to large-scale text-to-video generation to demonstrate the superiority of our work. Our code and models will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Page: https://yingqinghe.github.io/LVDM/ Github: https://github.com/YingqingHe/LVDM},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\57U7FBLW\\He et al. - 2023 - Latent Video Diffusion Models for High-Fidelity Long Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\XMAMJHCD\\2211.html}
}

@misc{heLLMsMeetMultimodal2024,
  title = {{{LLMs Meet Multimodal Generation}} and {{Editing}}: {{A Survey}}},
  shorttitle = {{{LLMs Meet Multimodal Generation}} and {{Editing}}},
  author = {He, Yingqing and Liu, Zhaoyang and Chen, Jingye and Tian, Zeyue and Liu, Hongyu and Chi, Xiaowei and Liu, Runtao and Yuan, Ruibin and Xing, Yazhou and Wang, Wenhai and Dai, Jifeng and Zhang, Yong and Xue, Wei and Liu, Qifeng and Guo, Yike and Chen, Qifeng},
  year = {2024},
  month = jun,
  number = {arXiv:2405.19334},
  eprint = {2405.19334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.19334},
  urldate = {2025-06-02},
  abstract = {With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Computer Science - Sound},
  note = {Comment: 52 Pages with 16 Figures, 12 Tables, and 545 References. GitHub Repository at: https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\PDMQZE6H\\He et al. - 2024 - LLMs Meet Multimodal Generation and Editing A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\4WN4P25C\\2405.html}
}

@misc{heLLMsMeetMultimodal2024a,
  title = {{{LLMs Meet Multimodal Generation}} and {{Editing}}: {{A Survey}}},
  shorttitle = {{{LLMs Meet Multimodal Generation}} and {{Editing}}},
  author = {He, Yingqing and Liu, Zhaoyang and Chen, Jingye and Tian, Zeyue and Liu, Hongyu and Chi, Xiaowei and Liu, Runtao and Yuan, Ruibin and Xing, Yazhou and Wang, Wenhai and Dai, Jifeng and Zhang, Yong and Xue, Wei and Liu, Qifeng and Guo, Yike and Chen, Qifeng},
  year = {2024},
  month = jun,
  number = {arXiv:2405.19334},
  eprint = {2405.19334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.19334},
  urldate = {2025-06-02},
  abstract = {With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Computer Science - Sound},
  note = {Comment: 52 Pages with 16 Figures, 12 Tables, and 545 References. GitHub Repository at: https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\9H7XDQ82\\He et al. - 2024 - LLMs Meet Multimodal Generation and Editing A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\U4J7EGS5\\2405.html}
}

@misc{heLLMsMeetMultimodal2024b,
  title = {{{LLMs Meet Multimodal Generation}} and {{Editing}}: {{A Survey}}},
  shorttitle = {{{LLMs Meet Multimodal Generation}} and {{Editing}}},
  author = {He, Yingqing and Liu, Zhaoyang and Chen, Jingye and Tian, Zeyue and Liu, Hongyu and Chi, Xiaowei and Liu, Runtao and Yuan, Ruibin and Xing, Yazhou and Wang, Wenhai and Dai, Jifeng and Zhang, Yong and Xue, Wei and Liu, Qifeng and Guo, Yike and Chen, Qifeng},
  year = {2024},
  month = jun,
  number = {arXiv:2405.19334},
  eprint = {2405.19334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.19334},
  urldate = {2025-06-03},
  abstract = {With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Computer Science - Sound},
  note = {Comment: 52 Pages with 16 Figures, 12 Tables, and 545 References. GitHub Repository at: https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\VFNVXZ9W\\He et al. - 2024 - LLMs Meet Multimodal Generation and Editing A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\Y5PRQ8F4\\2405.html}
}

@misc{heUniPortraitUnifiedFramework2024,
  title = {{{UniPortrait}}: {{A Unified Framework}} for {{Identity-Preserving Single-}} and {{Multi-Human Image Personalization}}},
  shorttitle = {{{UniPortrait}}},
  author = {He, Junjie and Geng, Yifeng and Bo, Liefeng},
  year = {2024},
  month = sep,
  number = {arXiv:2408.05939},
  eprint = {2408.05939},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05939},
  urldate = {2025-03-23},
  abstract = {This paper presents UniPortrait, an innovative human image personalization framework that unifies single- and multi-ID customization with high face fidelity, extensive facial editability, free-form input description, and diverse layout generation. UniPortrait consists of only two plug-and-play modules: an ID embedding module and an ID routing module. The ID embedding module extracts versatile editable facial features with a decoupling strategy for each ID and embeds them into the context space of diffusion models. The ID routing module then combines and distributes these embeddings adaptively to their respective regions within the synthesized image, achieving the customization of single and multiple IDs. With a carefully designed two-stage training scheme, UniPortrait achieves superior performance in both single- and multi-ID customization. Quantitative and qualitative experiments demonstrate the advantages of our method over existing approaches as well as its good scalability, e.g., the universal compatibility with existing generative control tools. The project page is at https://aigcdesigngroup.github.io/UniPortrait-Page/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Basis for AnyStory paper
\par
Comment: Tech report; Project page: https://aigcdesigngroup.github.io/UniPortrait-Page/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\XSG2C33U\\He et al. - 2024 - UniPortrait A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalizati.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\XHS3DAZP\\2408.html}
}

@misc{heUniPortraitUnifiedFramework2024a,
  title = {{{UniPortrait}}: {{A Unified Framework}} for {{Identity-Preserving Single-}} and {{Multi-Human Image Personalization}}},
  shorttitle = {{{UniPortrait}}},
  author = {He, Junjie and Geng, Yifeng and Bo, Liefeng},
  year = {2024},
  month = sep,
  number = {arXiv:2408.05939},
  eprint = {2408.05939},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05939},
  urldate = {2025-04-11},
  abstract = {This paper presents UniPortrait, an innovative human image personalization framework that unifies single- and multi-ID customization with high face fidelity, extensive facial editability, free-form input description, and diverse layout generation. UniPortrait consists of only two plug-and-play modules: an ID embedding module and an ID routing module. The ID embedding module extracts versatile editable facial features with a decoupling strategy for each ID and embeds them into the context space of diffusion models. The ID routing module then combines and distributes these embeddings adaptively to their respective regions within the synthesized image, achieving the customization of single and multiple IDs. With a carefully designed two-stage training scheme, UniPortrait achieves superior performance in both single- and multi-ID customization. Quantitative and qualitative experiments demonstrate the advantages of our method over existing approaches as well as its good scalability, e.g., the universal compatibility with existing generative control tools. The project page is at https://aigcdesigngroup.github.io/UniPortrait-Page/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Tech report; Project page: https://aigcdesigngroup.github.io/UniPortrait-Page/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\KCS4HQSG\\He et al. - 2024 - UniPortrait A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalizati.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ETXVBRSH\\2408.html}
}

@misc{hoVideoDiffusionModels2022,
  title = {Video {{Diffusion Models}}},
  author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J.},
  year = {2022},
  month = jun,
  number = {arXiv:2204.03458},
  eprint = {2204.03458},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.03458},
  urldate = {2025-05-30},
  abstract = {Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\NHT4WAN3\\Ho et al. - 2022 - Video Diffusion Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\5G3ZFFCF\\2204.html}
}

@misc{huangConceptMasterMultiConceptVideo2025,
  title = {{{ConceptMaster}}: {{Multi-Concept Video Customization}} on {{Diffusion Transformer Models Without Test-Time Tuning}}},
  shorttitle = {{{ConceptMaster}}},
  author = {Huang, Yuzhou and Yuan, Ziyang and Liu, Quande and Wang, Qiulin and Wang, Xintao and Zhang, Ruimao and Wan, Pengfei and Zhang, Di and Gai, Kun},
  year = {2025},
  month = may,
  number = {arXiv:2501.04698},
  eprint = {2501.04698},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.04698},
  urldate = {2025-06-18},
  abstract = {Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges for this task: 1) the identity decoupling issue, where directly adopting existing customization methods inevitably mix identity attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training a model that can well represent and decouple various customized concepts in video generation. To address these challenges, we introduce ConceptMaster, a novel framework that effectively addresses the identity decoupling issues while maintaining concept fidelity in video customization. Specifically, we propose to learn decoupled multi-concept embeddings and inject them into diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To overcome the scarcity of high-quality MCVC data, we establish a data construction pipeline, which enables collection of high-quality multi-concept video-entity data pairs across diverse scenarios. A multi-concept video evaluation set is further devised to comprehensively validate our method from three dimensions, including concept fidelity, identity decoupling ability, and video generation quality, across six different concept composition scenarios. Extensive experiments demonstrate that ConceptMaster significantly outperforms previous methods for video customization tasks, showing great potential to generate personalized and semantically accurate content for video diffusion models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Page: https://yuzhou914.github.io/ConceptMaster/. Update and release MCVC Evaluation Set},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\UIWA4U8L\\Huang et al. - 2025 - ConceptMaster Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time T.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\TGMTAP47\\2501.html}
}

@misc{huDynamicIDZeroShotMultiID2025,
  title = {{{DynamicID}}: {{Zero-Shot Multi-ID Image Personalization}} with {{Flexible Facial Editability}}},
  shorttitle = {{{DynamicID}}},
  author = {Hu, Xirui and Wang, Jiahao and Chen, Hao and Zhang, Weizhan and Wang, Benqi and Li, Yikun and Nan, Haishun},
  year = {2025},
  month = mar,
  number = {arXiv:2503.06505},
  eprint = {2503.06505},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.06505},
  urldate = {2025-04-11},
  abstract = {Recent advancements in text-to-image generation have spurred interest in personalized human image generation, which aims to create novel images featuring specific human identities as reference images indicate. Although existing methods achieve high-fidelity identity preservation, they often struggle with limited multi-ID usability and inadequate facial editability. We present DynamicID, a tuning-free framework supported by a dual-stage training paradigm that inherently facilitates both single-ID and multi-ID personalized generation with high fidelity and flexible facial editability. Our key innovations include: 1) Semantic-Activated Attention (SAA), which employs query-level activation gating to minimize disruption to the original model when injecting ID features and achieve multi-ID personalization without requiring multi-ID samples during training. 2) Identity-Motion Reconfigurator (IMR), which leverages contrastive learning to effectively disentangle and re-entangle facial motion and identity features, thereby enabling flexible facial editing. Additionally, we have developed a curated VariFace-10k facial dataset, comprising 10k unique individuals, each represented by 35 distinct facial images. Experimental results demonstrate that DynamicID outperforms state-of-the-art methods in identity fidelity, facial editability, and multi-ID personalization capability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 17 pages, 16 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\6YWXJATP\\Hu et al. - 2025 - DynamicID Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\BJASWBEZ\\2503.html}
}

@misc{huGAIA1GenerativeWorld2023,
  title = {{{GAIA-1}}: {{A Generative World Model}} for {{Autonomous Driving}}},
  shorttitle = {{{GAIA-1}}},
  author = {Hu, Anthony and Russell, Lloyd and Yeo, Hudson and Murez, Zak and Fedoseev, George and Kendall, Alex and Shotton, Jamie and Corrado, Gianluca},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17080},
  eprint = {2309.17080},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.17080},
  urldate = {2025-05-30},
  abstract = {Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves. To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: Technical Report},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\IKSIUDRX\\Hu et al. - 2023 - GAIA-1 A Generative World Model for Autonomous Driving.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ENRWSCCR\\2309.html}
}

@misc{huHunyuanCustomMultimodalDrivenArchitecture2025,
  title = {{{HunyuanCustom}}: {{A Multimodal-Driven Architecture}} for {{Customized Video Generation}}},
  shorttitle = {{{HunyuanCustom}}},
  author = {Hu, Teng and Yu, Zhentao and Zhou, Zhengguang and Liang, Sen and Zhou, Yuan and Lin, Qin and Lu, Qinglin},
  year = {2025},
  month = may,
  number = {arXiv:2505.04512},
  eprint = {2505.04512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.04512},
  urldate = {2025-05-31},
  abstract = {Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\49PKSIYD\\Hu et al. - 2025 - HunyuanCustom A Multimodal-Driven Architecture for Customized Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZJNFHPXH\\2505.html}
}

@misc{huHunyuanCustomMultimodalDrivenArchitecture2025a,
  title = {{{HunyuanCustom}}: {{A Multimodal-Driven Architecture}} for {{Customized Video Generation}}},
  shorttitle = {{{HunyuanCustom}}},
  author = {Hu, Teng and Yu, Zhentao and Zhou, Zhengguang and Liang, Sen and Zhou, Yuan and Lin, Qin and Lu, Qinglin},
  year = {2025},
  month = may,
  number = {arXiv:2505.04512},
  eprint = {2505.04512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.04512},
  urldate = {2025-05-31},
  abstract = {Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\IH4S5J3P\\Hu et al. - 2025 - HunyuanCustom A Multimodal-Driven Architecture for Customized Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\C5VZ7N5H\\2505.html}
}

@misc{huStoryAgentCustomizedStorytelling2024,
  title = {{{StoryAgent}}: {{Customized Storytelling Video Generation}} via {{Multi-Agent Collaboration}}},
  shorttitle = {{{StoryAgent}}},
  author = {Hu, Panwen and Jiang, Jin and Chen, Jianqi and Han, Mingfei and Liao, Shengcai and Chang, Xiaojun and Liang, Xiaodan},
  year = {2024},
  month = nov,
  number = {arXiv:2411.04925},
  eprint = {2411.04925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.04925},
  urldate = {2025-04-24},
  abstract = {The advent of AI-Generated Content (AIGC) has spurred research into automated video generation to streamline conventional processes. However, automating storytelling video production, particularly for customized narratives, remains challenging due to the complexity of maintaining subject consistency across shots. While existing approaches like Mora and AesopAgent integrate multiple agents for Story-to-Video (S2V) generation, they fall short in preserving protagonist consistency and supporting Customized Storytelling Video Generation (CSVG). To address these limitations, we propose StoryAgent, a multi-agent framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks assigned to specialized agents, mirroring the professional production process. Notably, our framework includes agents for story design, storyboard generation, video creation, agent coordination, and result evaluation. Leveraging the strengths of different models, StoryAgent enhances control over the generation process, significantly improving character consistency. Specifically, we introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance intra-shot temporal consistency, while a novel storyboard generation pipeline is proposed to maintain subject consistency across shots. Extensive experiments demonstrate the effectiveness of our approach in synthesizing highly consistent storytelling videos, outperforming state-of-the-art methods. Our contributions include the introduction of StoryAgent, a versatile framework for video generation tasks, and novel techniques for preserving protagonist consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multiagent Systems},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\N9PWR5TL\\Hu et al. - 2024 - StoryAgent Customized Storytelling Video Generation via Multi-Agent Collaboration.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\6ZLLE2KH\\2411.html}
}

@misc{jeongVideoRAGRetrievalAugmentedGeneration2025,
  title = {{{VideoRAG}}: {{Retrieval-Augmented Generation}} over {{Video Corpus}}},
  shorttitle = {{{VideoRAG}}},
  author = {Jeong, Soyeong and Kim, Kangsan and Baek, Jinheon and Hwang, Sung Ju},
  year = {2025},
  month = mar,
  number = {arXiv:2501.05874},
  eprint = {2501.05874},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.05874},
  urldate = {2025-04-22},
  abstract = {Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. While very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions losing multimodal richness. To tackle these, we introduce VideoRAG, a framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Code is available at https://github.com/starsuzi/VideoRAG.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\WFLMNF9D\\Jeong et al. - 2025 - VideoRAG Retrieval-Augmented Generation over Video Corpus.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\RWF2PHXW\\2501.html}
}

@misc{jiangAniSoraExploringFrontiers2024,
  title = {{{AniSora}}: {{Exploring}} the {{Frontiers}} of {{Animation Video Generation}} in the {{Sora Era}}},
  shorttitle = {{{AniSora}}},
  author = {Jiang, Yudong and Xu, Baohan and Yang, Siqian and Yin, Mingyu and Liu, Jing and Xu, Chao and Wang, Siqi and Wu, Yidi and Zhu, Bingwen and Zhang, Xinwen and Zheng, Xingyu and Xu, Jixuan and Zhang, Yue and Hou, Jinlong and Sun, Huyang},
  year = {2024},
  month = dec,
  number = {arXiv:2412.10255},
  eprint = {2412.10255},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.10255},
  urldate = {2025-03-20},
  abstract = {Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation. Our evaluation benchmark will be publicly available at https://github.com/bilibili/Index-anisora.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Graphics},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\IU8KQDF8\\Jiang et al. - 2024 - AniSora Exploring the Frontiers of Animation Video Generation in the Sora Era.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\7TR5KYK8\\2412.html}
}

@misc{jiangVACEAllinOneVideo2025,
  title = {{{VACE}}: {{All-in-One Video Creation}} and {{Editing}}},
  shorttitle = {{{VACE}}},
  author = {Jiang, Zeyinzi and Han, Zhen and Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Liu, Yu},
  year = {2025},
  month = mar,
  number = {arXiv:2503.07598},
  eprint = {2503.07598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.07598},
  urldate = {2025-06-18},
  abstract = {Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked video-to-video editing. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://ali-vilab.github.io/VACE-Page/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\62DQWWGY\\Jiang et al. - 2025 - VACE All-in-One Video Creation and Editing.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\W6894FVU\\2503.html}
}

@misc{jinPyramidalFlowMatching2025,
  title = {Pyramidal {{Flow Matching}} for {{Efficient Video Generative Modeling}}},
  author = {Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  year = {2025},
  month = mar,
  number = {arXiv:2410.05954},
  eprint = {2410.05954},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.05954},
  urldate = {2025-03-22},
  abstract = {Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models are open-sourced at https://pyramid-flow.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: ICLR 2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\H9THSGNZ\\Jin et al. - 2025 - Pyramidal Flow Matching for Efficient Video Generative Modeling.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\M8B77586\\2410.html}
}

@misc{jinPyramidalFlowMatching2025a,
  title = {Pyramidal {{Flow Matching}} for {{Efficient Video Generative Modeling}}},
  author = {Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  year = {2025},
  month = mar,
  number = {arXiv:2410.05954},
  eprint = {2410.05954},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.05954},
  urldate = {2025-06-18},
  abstract = {Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models are open-sourced at https://pyramid-flow.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: ICLR 2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\TQXB55BH\\Jin et al. - 2025 - Pyramidal Flow Matching for Efficient Video Generative Modeling.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\B7DBRA6B\\2410.html}
}

@misc{kimPersonaCraftPersonalizedControllable2025,
  title = {{{PersonaCraft}}: {{Personalized}} and {{Controllable Full-Body Multi-Human Scene Generation Using Occlusion-Aware 3D-Conditioned Diffusion}}},
  shorttitle = {{{PersonaCraft}}},
  author = {Kim, Gwanghyun and Jeon, Suh Yoon and Lee, Seunggyu and Chun, Se Young},
  year = {2025},
  month = mar,
  number = {arXiv:2411.18068},
  eprint = {2411.18068},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.18068},
  urldate = {2025-04-11},
  abstract = {We present PersonaCraft, a framework for controllable and occlusion-robust full-body personalized image synthesis of multiple individuals in complex scenes. Current methods struggle with occlusion-heavy scenarios and complete body personalization, as 2D pose conditioning lacks 3D geometry, often leading to ambiguous occlusions and anatomical distortions, and many approaches focus solely on facial identity. In contrast, our PersonaCraft integrates diffusion models with 3D human modeling, employing SMPLx-ControlNet, to utilize 3D geometry like depth and normal maps for robust 3D-aware pose conditioning and enhanced anatomical coherence. To handle fine-grained occlusions, we propose Occlusion Boundary Enhancer Network that exploits depth edge signals with occlusion-focused training, and Occlusion-Aware Classifier-Free Guidance strategy that selectively reinforces conditioning in occluded regions without affecting unoccluded areas. PersonaCraft can seamlessly be combined with Face Identity ControlNet, achieving full-body multi-human personalization and thus marking a significant advancement beyond prior approaches that concentrate only on facial identity. Our dual-pathway body shape representation with SMPLx-based shape parameters and textual refinement, enables precise full-body personalization and flexible user-defined body shape adjustments. Extensive quantitative experiments and user studies demonstrate that PersonaCraft significantly outperforms existing methods in generating high-quality, multi-person images with accurate personalization and robust occlusion handling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://gwang-kim.github.io/persona\_craft},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\U7LJYZJV\\Kim et al. - 2025 - PersonaCraft Personalized and Controllable Full-Body Multi-Human Scene Generation Using Occlusion-A.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\6G632HMU\\2411.html}
}

@misc{kimTuningFreeMultiEventLong2025,
  title = {Tuning-{{Free Multi-Event Long Video Generation}} via {{Synchronized Coupled Sampling}}},
  author = {Kim, Subin and Oh, Seoung Wug and Wang, Jui-Hsien and Lee, Joon-Young and Shin, Jinwoo},
  year = {2025},
  month = mar,
  number = {arXiv:2503.08605},
  eprint = {2503.08605},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.08605},
  urldate = {2025-03-19},
  abstract = {While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Project page with visuals: https://syncos2025.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\BJSD9Y3G\\Kim et al. - 2025 - Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\5H6MFWGP\\2503.html}
}

@misc{kondratyukVideoPoetLargeLanguage2024,
  title = {{{VideoPoet}}: {{A Large Language Model}} for {{Zero-Shot Video Generation}}},
  shorttitle = {{{VideoPoet}}},
  author = {Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and Somandepalli, Krishna and Akbari, Hassan and Alon, Yair and Cheng, Yong and Dillon, Josh and Gupta, Agrim and Hahn, Meera and Hauth, Anja and Hendon, David and Martinez, Alonso and Minnen, David and Sirotenko, Mikhail and Sohn, Kihyuk and Yang, Xuan and Adam, Hartwig and Yang, Ming-Hsuan and Essa, Irfan and Wang, Huisheng and Ross, David A. and Seybold, Bryan and Jiang, Lu},
  year = {2024},
  month = jun,
  number = {arXiv:2312.14125},
  eprint = {2312.14125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.14125},
  urldate = {2025-04-24},
  abstract = {We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To appear at ICML 2024; Project page: http://sites.research.google/videopoet/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\NIT7ITHL\\Kondratyuk et al. - 2024 - VideoPoet A Large Language Model for Zero-Shot Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\5STHV2XT\\2312.html}
}

@misc{kondratyukVideoPoetLargeLanguage2024a,
  title = {{{VideoPoet}}: {{A Large Language Model}} for {{Zero-Shot Video Generation}}},
  shorttitle = {{{VideoPoet}}},
  author = {Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and Somandepalli, Krishna and Akbari, Hassan and Alon, Yair and Cheng, Yong and Dillon, Josh and Gupta, Agrim and Hahn, Meera and Hauth, Anja and Hendon, David and Martinez, Alonso and Minnen, David and Sirotenko, Mikhail and Sohn, Kihyuk and Yang, Xuan and Adam, Hartwig and Yang, Ming-Hsuan and Essa, Irfan and Wang, Huisheng and Ross, David A. and Seybold, Bryan and Jiang, Lu},
  year = {2024},
  month = jun,
  number = {arXiv:2312.14125},
  eprint = {2312.14125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.14125},
  urldate = {2025-04-24},
  abstract = {We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To appear at ICML 2024; Project page: http://sites.research.google/videopoet/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\2BQBIGDG\\Kondratyuk et al. - 2024 - VideoPoet A Large Language Model for Zero-Shot Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\TJNFMLY6\\2312.html}
}

@misc{kongHunyuanVideoSystematicFramework2025,
  title = {{{HunyuanVideo}}: {{A Systematic Framework For Large Video Generative Models}}},
  shorttitle = {{{HunyuanVideo}}},
  author = {Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and Wu, Kathrina and Lin, Qin and Yuan, Junkun and Long, Yanxin and Wang, Aladdin and Wang, Andong and Li, Changlin and Huang, Duojun and Yang, Fang and Tan, Hao and Wang, Hongmei and Song, Jacob and Bai, Jiawang and Wu, Jianbing and Xue, Jinbao and Wang, Joey and Wang, Kai and Liu, Mengyang and Li, Pengyu and Li, Shuai and Wang, Weiyan and Yu, Wenqing and Deng, Xinchi and Li, Yang and Chen, Yi and Cui, Yutao and Peng, Yuanbo and Yu, Zhentao and He, Zhiyu and Xu, Zhiyong and Zhou, Zixiang and Xu, Zunnan and Tao, Yangyu and Lu, Qinglin and Liu, Songtao and Zhou, Dax and Wang, Hongfa and Yang, Yong and Wang, Di and Liu, Yuhong and Jiang, Jie and Zhong, Caesar},
  year = {2025},
  month = mar,
  number = {arXiv:2412.03603},
  eprint = {2412.03603},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.03603},
  urldate = {2025-04-11},
  abstract = {Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\AQKXTXFY\\Kong et al. - 2025 - HunyuanVideo A Systematic Framework For Large Video Generative Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\U48IT7SK\\2412.html}
}

@misc{kongHunyuanVideoSystematicFramework2025a,
  title = {{{HunyuanVideo}}: {{A Systematic Framework For Large Video Generative Models}}},
  shorttitle = {{{HunyuanVideo}}},
  author = {Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and Wu, Kathrina and Lin, Qin and Yuan, Junkun and Long, Yanxin and Wang, Aladdin and Wang, Andong and Li, Changlin and Huang, Duojun and Yang, Fang and Tan, Hao and Wang, Hongmei and Song, Jacob and Bai, Jiawang and Wu, Jianbing and Xue, Jinbao and Wang, Joey and Wang, Kai and Liu, Mengyang and Li, Pengyu and Li, Shuai and Wang, Weiyan and Yu, Wenqing and Deng, Xinchi and Li, Yang and Chen, Yi and Cui, Yutao and Peng, Yuanbo and Yu, Zhentao and He, Zhiyu and Xu, Zhiyong and Zhou, Zixiang and Xu, Zunnan and Tao, Yangyu and Lu, Qinglin and Liu, Songtao and Zhou, Dax and Wang, Hongfa and Yang, Yong and Wang, Di and Liu, Yuhong and Jiang, Jie and Zhong, Caesar},
  year = {2025},
  month = mar,
  number = {arXiv:2412.03603},
  eprint = {2412.03603},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.03603},
  urldate = {2025-04-24},
  abstract = {Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\N2HX2NW5\\Kong et al. - 2025 - HunyuanVideo A Systematic Framework For Large Video Generative Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\MJW6HFEK\\2412.html}
}

@misc{kongHunyuanVideoSystematicFramework2025b,
  title = {{{HunyuanVideo}}: {{A Systematic Framework For Large Video Generative Models}}},
  shorttitle = {{{HunyuanVideo}}},
  author = {Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and Wu, Kathrina and Lin, Qin and Yuan, Junkun and Long, Yanxin and Wang, Aladdin and Wang, Andong and Li, Changlin and Huang, Duojun and Yang, Fang and Tan, Hao and Wang, Hongmei and Song, Jacob and Bai, Jiawang and Wu, Jianbing and Xue, Jinbao and Wang, Joey and Wang, Kai and Liu, Mengyang and Li, Pengyu and Li, Shuai and Wang, Weiyan and Yu, Wenqing and Deng, Xinchi and Li, Yang and Chen, Yi and Cui, Yutao and Peng, Yuanbo and Yu, Zhentao and He, Zhiyu and Xu, Zhiyong and Zhou, Zixiang and Xu, Zunnan and Tao, Yangyu and Lu, Qinglin and Liu, Songtao and Zhou, Dax and Wang, Hongfa and Yang, Yong and Wang, Di and Liu, Yuhong and Jiang, Jie and Zhong, Caesar},
  year = {2025},
  month = mar,
  number = {arXiv:2412.03603},
  eprint = {2412.03603},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.03603},
  urldate = {2025-06-18},
  abstract = {Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\EB4THBLM\\Kong et al. - 2025 - HunyuanVideo A Systematic Framework For Large Video Generative Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\KQJL2C8J\\2412.html}
}

@misc{leiAnimateAnythingConsistentControllable2024,
  title = {{{AnimateAnything}}: {{Consistent}} and {{Controllable Animation}} for {{Video Generation}}},
  shorttitle = {{{AnimateAnything}}},
  author = {Lei, Guojun and Wang, Chi and Li, Hong and Zhang, Rong and Wang, Yikai and Xu, Weiwei},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10836},
  eprint = {2411.10836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10836},
  urldate = {2025-03-22},
  abstract = {We present a unified controllable video generation approach AnimateAnything that facilitates precise and consistent video manipulation across various conditions, including camera trajectories, text prompts, and user motion annotations. Specifically, we carefully design a multi-scale control feature fusion network to construct a common motion representation for different conditions. It explicitly converts all control information into frame-by-frame optical flows. Then we incorporate the optical flows as motion priors to guide final video generation. In addition, to reduce the flickering issues caused by large-scale motion, we propose a frequency-based stabilization module. It can enhance temporal coherence by ensuring the video's frequency domain consistency. Experiments demonstrate that our method outperforms the state-of-the-art approaches. For more details and videos, please refer to the webpage: https://yu-shaonian.github.io/Animate\_Anything/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\LWG9R9CC\\Lei et al. - 2024 - AnimateAnything Consistent and Controllable Animation for Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\K3IFDIIT\\2411.html}
}

@misc{leiComprehensiveSurveyHuman2024,
  title = {A {{Comprehensive Survey}} on {{Human Video Generation}}: {{Challenges}}, {{Methods}}, and {{Insights}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Human Video Generation}}},
  author = {Lei, Wentao and Wang, Jinting and Ma, Fengji and Huang, Guanjie and Liu, Li},
  year = {2024},
  month = jul,
  number = {arXiv:2407.08428},
  eprint = {2407.08428},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.08428},
  urldate = {2025-06-02},
  abstract = {Human video generation is a dynamic and rapidly evolving task that aims to synthesize 2D human body video sequences with generative models given control conditions such as text, audio, and pose. With the potential for wide-ranging applications in film, gaming, and virtual communication, the ability to generate natural and realistic human video is critical. Recent advancements in generative models have laid a solid foundation for the growing interest in this area. Despite the significant progress, the task of human video generation remains challenging due to the consistency of characters, the complexity of human motion, and difficulties in their relationship with the environment. This survey provides a comprehensive review of the current state of human video generation, marking, to the best of our knowledge, the first extensive literature review in this domain. We start with an introduction to the fundamentals of human video generation and the evolution of generative models that have facilitated the field's growth. We then examine the main methods employed for three key sub-tasks within human video generation: text-driven, audio-driven, and pose-driven motion generation. These areas are explored concerning the conditions that guide the generation process. Furthermore, we offer a collection of the most commonly utilized datasets and the evaluation metrics that are crucial in assessing the quality and realism of generated videos. The survey concludes with a discussion of the current challenges in the field and suggests possible directions for future research. The goal of this survey is to offer the research community a clear and holistic view of the advancements in human video generation, highlighting the milestones achieved and the challenges that lie ahead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\GBHTQZJV\\Lei et al. - 2024 - A Comprehensive Survey on Human Video Generation Challenges, Methods, and Insights.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\KA4PZXAW\\2407.html}
}

@misc{liangMovieWeaverTuningFree2025,
  title = {Movie {{Weaver}}: {{Tuning-Free Multi-Concept Video Personalization}} with {{Anchored Prompts}}},
  shorttitle = {Movie {{Weaver}}},
  author = {Liang, Feng and Ma, Haoyu and He, Zecheng and Hou, Tingbo and Hou, Ji and Li, Kunpeng and Dai, Xiaoliang and {Juefei-Xu}, Felix and Azadi, Samaneh and Sinha, Animesh and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
  year = {2025},
  month = feb,
  number = {arXiv:2502.07802},
  eprint = {2502.07802},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.07802},
  urldate = {2025-04-11},
  abstract = {Video personalization, which generates customized videos using reference images, has gained significant attention. However, prior methods typically focus on single-concept personalization, limiting broader applications that require multi-concept integration. Attempts to extend these models to multiple concepts often lead to identity blending, which results in composite characters with fused attributes from multiple sources. This challenge arises due to the lack of a mechanism to link each concept with its specific reference image. We address this with anchored prompts, which embed image anchors as unique tokens within text prompts, guiding accurate referencing during generation. Additionally, we introduce concept embeddings to encode the order of reference images. Our approach, Movie Weaver, seamlessly weaves multiple concepts-including face, body, and animal images-into one video, allowing flexible combinations in a single model. The evaluation shows that Movie Weaver outperforms existing methods for multi-concept video personalization in identity preservation and overall quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  note = {Comment: Project page: https://jeff-liangf.github.io/projects/movieweaver/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\UAB6K6J3\\Liang et al. - 2025 - Movie Weaver Tuning-Free Multi-Concept Video Personalization with Anchored Prompts.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\MDQMUP3S\\2502.html}
}

@misc{linExploringEvolutionPhysics2025,
  title = {Exploring the {{Evolution}} of {{Physics Cognition}} in {{Video Generation}}: {{A Survey}}},
  shorttitle = {Exploring the {{Evolution}} of {{Physics Cognition}} in {{Video Generation}}},
  author = {Lin, Minghui and Wang, Xiang and Wang, Yishan and Wang, Shu and Dai, Fengqi and Ding, Pengxiang and Wang, Cunxiang and Zuo, Zhengrong and Sang, Nong and Huang, Siteng and Wang, Donglin},
  year = {2025},
  month = mar,
  number = {arXiv:2503.21765},
  eprint = {2503.21765},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.21765},
  urldate = {2025-06-02},
  abstract = {Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: A comprehensive list of papers studied in this survey is available at https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\TGC3XXE7\\Lin et al. - 2025 - Exploring the Evolution of Physics Cognition in Video Generation A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZKA5W99C\\2503.html}
}

@misc{linOpenSoraPlanOpenSource2024,
  title = {Open-{{Sora Plan}}: {{Open-Source Large Video Generation Model}}},
  shorttitle = {Open-{{Sora Plan}}},
  author = {Lin, Bin and Ge, Yunyang and Cheng, Xinhua and Li, Zongjian and Zhu, Bin and Wang, Shaodong and He, Xianyi and Ye, Yang and Yuan, Shenghai and Chen, Liuhan and Jia, Tanghui and Zhang, Junwu and Tang, Zhenyu and Pang, Yatian and She, Bin and Yan, Cen and Hu, Zhiheng and Dong, Xiaoyi and Chen, Lin and Pan, Zhang and Zhou, Xing and Dong, Shaoling and Tian, Yonghong and Yuan, Li},
  year = {2024},
  month = nov,
  number = {arXiv:2412.00131},
  eprint = {2412.00131},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.00131},
  urldate = {2025-06-18},
  abstract = {We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at {\textbackslash}url\{https://github.com/PKU-YuanGroup/Open-Sora-Plan\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: v1.3},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\5EX8Q2RE\\Lin et al. - 2024 - Open-Sora Plan Open-Source Large Video Generation Model.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\2AWPCRJV\\2412.html}
}

@misc{liSurveyLongVideo2024,
  title = {A {{Survey}} on {{Long Video Generation}}: {{Challenges}}, {{Methods}}, and {{Prospects}}},
  shorttitle = {A {{Survey}} on {{Long Video Generation}}},
  author = {Li, Chengxuan and Huang, Di and Lu, Zeyu and Xiao, Yang and Pei, Qingqi and Bai, Lei},
  year = {2024},
  month = mar,
  number = {arXiv:2403.16407},
  eprint = {2403.16407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.16407},
  urldate = {2025-06-02},
  abstract = {Video generation is a rapidly advancing research area, garnering significant attention due to its broad range of applications. One critical aspect of this field is the generation of long-duration videos, which presents unique challenges and opportunities. This paper presents the first survey of recent advancements in long video generation and summarises them into two key paradigms: divide and conquer temporal autoregressive. We delve into the common models employed in each paradigm, including aspects of network design and conditioning techniques. Furthermore, we offer a comprehensive overview and classification of the datasets and evaluation metrics which are crucial for advancing long video generation research. Concluding with a summary of existing studies, we also discuss the emerging challenges and future directions in this dynamic field. We hope that this survey will serve as an essential reference for researchers and practitioners in the realm of long video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\CCDFNA8V\\Li et al. - 2024 - A Survey on Long Video Generation Challenges, Methods, and Prospects.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\A39IWVGF\\2403.html}
}

@misc{liuPhantomSubjectconsistentVideo2025,
  title = {Phantom: {{Subject-consistent}} Video Generation via Cross-Modal Alignment},
  shorttitle = {Phantom},
  author = {Liu, Lijie and Ma, Tianxiang and Li, Bingchuan and Chen, Zhuowei and Liu, Jiawei and Li, Gen and Zhou, Siyu and He, Qian and Wu, Xinglong},
  year = {2025},
  month = apr,
  number = {arXiv:2502.11079},
  eprint = {2502.11079},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.11079},
  urldate = {2025-06-01},
  abstract = {The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent videos following textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single- and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. The proposed method achieves high-fidelity subject-consistent video generation while addressing issues of image content leakage and multi-subject confusion. Evaluation results indicate that our method outperforms other state-of-the-art closed-source commercial solutions. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\IUFG7QV3\\Liu et al. - 2025 - Phantom Subject-consistent video generation via cross-modal alignment.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\TPFIQ8WA\\2502.html}
}

@misc{liuPhantomSubjectconsistentVideo2025a,
  title = {Phantom: {{Subject-consistent}} Video Generation via Cross-Modal Alignment},
  shorttitle = {Phantom},
  author = {Liu, Lijie and Ma, Tianxiang and Li, Bingchuan and Chen, Zhuowei and Liu, Jiawei and Li, Gen and Zhou, Siyu and He, Qian and Wu, Xinglong},
  year = {2025},
  month = apr,
  number = {arXiv:2502.11079},
  eprint = {2502.11079},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.11079},
  urldate = {2025-06-18},
  abstract = {The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent videos following textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single- and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. The proposed method achieves high-fidelity subject-consistent video generation while addressing issues of image content leakage and multi-subject confusion. Evaluation results indicate that our method outperforms other state-of-the-art closed-source commercial solutions. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\XM3NP8VU\\Liu et al. - 2025 - Phantom Subject-consistent video generation via cross-modal alignment.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\9CLGZMSJ\\2502.html}
}

@misc{liuSurveyAIGeneratedVideo2024,
  title = {A {{Survey}} of {{AI-Generated Video Evaluation}}},
  author = {Liu, Xiao and Xiang, Xinhao and Li, Zizhong and Wang, Yongheng and Li, Zhuoheng and Liu, Zhuosheng and Zhang, Weidi and Ye, Weiqi and Zhang, Jiawei},
  year = {2024},
  month = oct,
  number = {arXiv:2410.19884},
  eprint = {2410.19884},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.19884},
  urldate = {2025-06-02},
  abstract = {The growing capabilities of AI in generating video content have brought forward significant challenges in effectively evaluating these videos. Unlike static images or text, video content involves complex spatial and temporal dynamics which may require a more comprehensive and systematic evaluation of its contents in aspects like video presentation quality, semantic information delivery, alignment with human intentions, and the virtual-reality consistency with our physical world. This survey identifies the emerging field of AI-Generated Video Evaluation (AIGVE), highlighting the importance of assessing how well AI-generated videos align with human perception and meet specific instructions. We provide a structured analysis of existing methodologies that could be potentially used to evaluate AI-generated videos. By outlining the strengths and gaps in current approaches, we advocate for the development of more robust and nuanced evaluation frameworks that can handle the complexities of video content, which include not only the conventional metric-based evaluations, but also the current human-involved evaluations, and the future model-centered evaluations. This survey aims to establish a foundational knowledge base for both researchers from academia and practitioners from the industry, facilitating the future advancement of evaluation methods for AI-generated video content.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\MVZB8QI6\\Liu et al. - 2024 - A Survey of AI-Generated Video Evaluation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZXPJSI5N\\2410.html}
}

@misc{liuVideoColorizationPretrained2023,
  title = {Video {{Colorization}} with {{Pre-trained Text-to-Image Diffusion Models}}},
  author = {Liu, Hanyuan and Xie, Minshan and Xing, Jinbo and Li, Chengze and Wong, Tien-Tsin},
  year = {2023},
  month = jun,
  number = {arXiv:2306.01732},
  eprint = {2306.01732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.01732},
  urldate = {2025-05-30},
  abstract = {Video colorization is a challenging task that involves inferring plausible and temporally consistent colors for grayscale frames. In this paper, we present ColorDiffuser, an adaptation of a pre-trained text-to-image latent diffusion model for video colorization. With the proposed adapter-based approach, we repropose the pre-trained text-to-image model to accept input grayscale video frames, with the optional text description, for video colorization. To enhance the temporal coherence and maintain the vividness of colorization across frames, we propose two novel techniques: the Color Propagation Attention and Alternated Sampling Strategy. Color Propagation Attention enables the model to refine its colorization decision based on a reference latent frame, while Alternated Sampling Strategy captures spatiotemporal dependencies by using the next and previous adjacent latent frames alternatively as reference during the generative diffusion sampling steps. This encourages bidirectional color information propagation between adjacent video frames, leading to improved color consistency across frames. We conduct extensive experiments on benchmark datasets, and the results demonstrate the effectiveness of our proposed framework. The evaluations show that ColorDiffuser achieves state-of-the-art performance in video colorization, surpassing existing methods in terms of color fidelity, temporal consistency, and visual quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {Comment: project page: https://colordiffuser.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\HV45V56M\\Liu et al. - 2023 - Video Colorization with Pre-trained Text-to-Image Diffusion Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\BUFR6MLB\\2306.html}
}

@misc{longVideoStudioGeneratingConsistentContent2024,
  title = {{{VideoStudio}}: {{Generating Consistent-Content}} and {{Multi-Scene Videos}}},
  shorttitle = {{{VideoStudio}}},
  author = {Long, Fuchen and Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  year = {2024},
  month = sep,
  number = {arXiv:2401.01256},
  eprint = {2401.01256},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01256},
  urldate = {2025-03-20},
  abstract = {The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoStudio, for consistent-content and multi-scene video generation. Technically, VideoStudio leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoStudio identifies the common entities throughout the script and asks LLM to detail each entity. The resultant entity description is then fed into a text-to-image model to generate a reference image for each entity. Finally, VideoStudio outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account. The diffusion model incorporates the reference images as the condition and alignment to strengthen the content consistency of multi-scene videos. Extensive experiments demonstrate that VideoStudio outperforms the SOTA video generation models in terms of visual quality, content consistency, and user preference. Source code is available at {\textbackslash}url\{https://github.com/FuchenUSTC/VideoStudio\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ECCV 2024. Source code is available at https://github.com/FuchenUSTC/VideoStudio},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\4R72A5AA\\Long et al. - 2024 - VideoStudio Generating Consistent-Content and Multi-Scene Videos.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZC8HSX2Z\\2401.html}
}

@misc{luoVideoFusionDecomposedDiffusion2023,
  title = {{{VideoFusion}}: {{Decomposed Diffusion Models}} for {{High-Quality Video Generation}}},
  shorttitle = {{{VideoFusion}}},
  author = {Luo, Zhengxiong and Chen, Dayou and Zhang, Yingya and Huang, Yan and Wang, Liang and Shen, Yujun and Zhao, Deli and Zhou, Jingren and Tan, Tieniu},
  year = {2023},
  month = oct,
  number = {arXiv:2303.08320},
  eprint = {2303.08320},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08320},
  urldate = {2025-05-30},
  abstract = {A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution. Despite its recent success in image synthesis, applying DPMs to video generation is still challenging due to high-dimensional data spaces. Previous methods usually adopt a standard diffusion process, where frames in the same video clip are destroyed with independent noises, ignoring the content redundancy and temporal correlation. This work presents a decomposed diffusion process via resolving the per-frame noise into a base noise that is shared among all frames and a residual noise that varies along the time axis. The denoising pipeline employs two jointly-learned networks to match the noise decomposition accordingly. Experiments on various datasets confirm that our approach, termed as VideoFusion, surpasses both GAN-based and diffusion-based alternatives in high-quality video generation. We further show that our decomposed formulation can benefit from pre-trained image diffusion models and well-support text-conditioned video creation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted to CVPR2023},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\7D9MBRKK\\Luo et al. - 2023 - VideoFusion Decomposed Diffusion Models for High-Quality Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\4Q2CNGMT\\2303.html}
}

@misc{luoVideoRAGVisuallyalignedRetrievalAugmented2024,
  title = {Video-{{RAG}}: {{Visually-aligned Retrieval-Augmented Long Video Comprehension}}},
  shorttitle = {Video-{{RAG}}},
  author = {Luo, Yongdong and Zheng, Xiawu and Yang, Xiao and Li, Guilin and Lin, Haojia and Huang, Jinfa and Ji, Jiayi and Chao, Fei and Luo, Jiebo and Ji, Rongrong},
  year = {2024},
  month = dec,
  number = {arXiv:2411.13093},
  eprint = {2411.13093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.13093},
  urldate = {2025-04-22},
  abstract = {Existing large video-language models (LVLMs) struggle to comprehend long videos correctly due to limited context. To address this problem, fine-tuning long-context LVLMs and employing GPT-based agents have emerged as promising solutions. However, fine-tuning LVLMs would require extensive high-quality data and substantial GPU resources, while GPT-based agents would rely on proprietary models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented Generation (Video-RAG), a training-free and cost-effective pipeline that employs visually-aligned auxiliary texts to help facilitate cross-modality alignment while providing additional information beyond the visual content. Specifically, we leverage open-source external tools to extract visually-aligned information from pure video data (e.g., audio, optical character, and object detection), and incorporate the extracted information into an existing LVLM as auxiliary texts, alongside video frames and queries, in a plug-and-play manner. Our Video-RAG offers several key advantages: (i) lightweight with low computing overhead due to single-turn retrieval; (ii) easy implementation and compatibility with any LVLM; and (iii) significant, consistent performance gains across long video understanding benchmarks, including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o when utilized with a 72B model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 10 pages, 6 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\RJEGAHZP\\Luo et al. - 2024 - Video-RAG Visually-aligned Retrieval-Augmented Long Video Comprehension.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\9GFHI9DI\\2411.html}
}

@misc{maMagicMeIdentitySpecificVideo2024,
  title = {Magic-{{Me}}: {{Identity-Specific Video Customized Diffusion}}},
  shorttitle = {Magic-{{Me}}},
  author = {Ma, Ze and Zhou, Daquan and Yeh, Chun-Hsiao and Wang, Xue-She and Li, Xiuyu and Yang, Huanrui and Dong, Zhen and Keutzer, Kurt and Feng, Jiashi},
  year = {2024},
  month = mar,
  number = {arXiv:2402.09368},
  eprint = {2402.09368},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.09368},
  urldate = {2025-04-24},
  abstract = {Creating content with specified identities (ID) has attracted significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven creation has achieved great progress with the identity controlled via reference images. However, its extension to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified identity defined by a few images, VCD reinforces the identity characteristics and injects frame-wise correlation at the initialization stage for stable video outputs. To achieve this, we propose three novel components that are essential for high-quality identity preservation and stable video generation: 1) a noise initialization method with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID module based on extended Textual Inversion trained with the cropped identity to disentangle the ID information from the background 3) Face VCD and Tiled VCD modules to reinforce faces and upscale the video to higher resolution while preserving the identity's features. We conducted extensive experiments to verify that VCD is able to generate stable videos with better ID over the baselines. Besides, with the transferability of the encoded identity in the ID module, VCD is also working well with personalized text-to-image models available publicly. The codes are available at https://github.com/Zhen-Dong/Magic-Me.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Page at https://magic-me-webpage.github.io},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\BYPXS4BL\\Ma et al. - 2024 - Magic-Me Identity-Specific Video Customized Diffusion.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\EB583WQ9\\2402.html}
}

@misc{maStepVideoT2VTechnicalReport2025,
  title = {Step-{{Video-T2V Technical Report}}: {{The Practice}}, {{Challenges}}, and {{Future}} of {{Video Foundation Model}}},
  shorttitle = {Step-{{Video-T2V Technical Report}}},
  author = {Ma, Guoqing and Huang, Haoyang and Yan, Kun and Chen, Liangyu and Duan, Nan and Yin, Shengming and Wan, Changyi and Ming, Ranchen and Song, Xiaoniu and Chen, Xing and Zhou, Yu and Sun, Deshan and Zhou, Deyu and Zhou, Jian and Tan, Kaijun and An, Kang and Chen, Mei and Ji, Wei and Wu, Qiling and Sun, Wen and Han, Xin and Wei, Yanan and Ge, Zheng and Li, Aojie and Wang, Bin and Huang, Bizhu and Wang, Bo and Li, Brian and Miao, Changxing and Xu, Chen and Wu, Chenfei and Yu, Chenguang and Shi, Dapeng and Hu, Dingyuan and Liu, Enle and Yu, Gang and Yang, Ge and Huang, Guanzhe and Yan, Gulin and Feng, Haiyang and Nie, Hao and Jia, Haonan and Hu, Hanpeng and Chen, Hanqi and Yan, Haolong and Wang, Heng and Guo, Hongcheng and Xiong, Huilin and Xiong, Huixin and Gong, Jiahao and Wu, Jianchang and Wu, Jiaoren and Wu, Jie and Yang, Jie and Liu, Jiashuai and Li, Jiashuo and Zhang, Jingyang and Guo, Junjing and Lin, Junzhe and Li, Kaixiang and Liu, Lei and Xia, Lei and Zhao, Liang and Tan, Liguo and Huang, Liwen and Shi, Liying and Li, Ming and Li, Mingliang and Cheng, Muhua and Wang, Na and Chen, Qiaohui and He, Qinglin and Liang, Qiuyan and Sun, Quan and Sun, Ran and Wang, Rui and Pang, Shaoliang and Yang, Shiliang and Liu, Sitong and Liu, Siqi and Gao, Shuli and Cao, Tiancheng and Wang, Tianyu and Ming, Weipeng and He, Wenqing and Zhao, Xu and Zhang, Xuelin and Zeng, Xianfang and Liu, Xiaojia and Yang, Xuan and Dai, Yaqi and Yu, Yanbo and Li, Yang and Deng, Yineng and Wang, Yingming and Wang, Yilei and Lu, Yuanwei and Chen, Yu and Luo, Yu and Luo, Yuchu and Yin, Yuhe and Feng, Yuheng and Yang, Yuxiang and Tang, Zecheng and Zhang, Zekai and Yang, Zidong and Jiao, Binxing and Chen, Jiansheng and Li, Jing and Zhou, Shuchang and Zhang, Xiangyu and Zhang, Xinhao and Zhu, Yibo and Shum, Heung-Yeung and Jiang, Daxin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.10248},
  eprint = {2502.10248},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.10248},
  urldate = {2025-06-18},
  abstract = {We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 36 pages, 14 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\IDWUWERG\\Ma et al. - 2025 - Step-Video-T2V Technical Report The Practice, Challenges, and Future of Video Foundation Model.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\3B5496BW\\2502.html}
}

@misc{melnikVideoDiffusionModels2024,
  title = {Video {{Diffusion Models}}: {{A Survey}}},
  shorttitle = {Video {{Diffusion Models}}},
  author = {Melnik, Andrew and Ljubljanac, Michal and Lu, Cong and Yan, Qi and Ren, Weiming and Ritter, Helge},
  year = {2024},
  month = nov,
  number = {arXiv:2405.03150},
  eprint = {2405.03150},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.03150},
  urldate = {2025-05-30},
  abstract = {Diffusion generative models have recently become a powerful technique for creating and modifying high-quality, coherent video content. This survey provides a comprehensive overview of the critical components of diffusion models for video generation, including their applications, architectural design, and temporal dynamics modeling. The paper begins by discussing the core principles and mathematical formulations, then explores various architectural choices and methods for maintaining temporal consistency. A taxonomy of applications is presented, categorizing models based on input modalities such as text prompts, images, videos, and audio signals. Advancements in text-to-video generation are discussed to illustrate the state-of-the-art capabilities and limitations of current approaches. Additionally, the survey summarizes recent developments in training and evaluation practices, including the use of diverse video and image datasets and the adoption of various evaluation metrics to assess model performance. The survey concludes with an examination of ongoing challenges, such as generating longer videos and managing computational costs, and offers insights into potential future directions for the field. By consolidating the latest research and developments, this survey aims to serve as a valuable resource for researchers and practitioners working with video diffusion models. Website: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\H2SPLEPF\\Melnik et al. - 2024 - Video Diffusion Models A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\238WE2BQ\\2405.html}
}

@misc{melnikVideoDiffusionModels2024a,
  title = {Video {{Diffusion Models}}: {{A Survey}}},
  shorttitle = {Video {{Diffusion Models}}},
  author = {Melnik, Andrew and Ljubljanac, Michal and Lu, Cong and Yan, Qi and Ren, Weiming and Ritter, Helge},
  year = {2024},
  month = nov,
  number = {arXiv:2405.03150},
  eprint = {2405.03150},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.03150},
  urldate = {2025-06-02},
  abstract = {Diffusion generative models have recently become a powerful technique for creating and modifying high-quality, coherent video content. This survey provides a comprehensive overview of the critical components of diffusion models for video generation, including their applications, architectural design, and temporal dynamics modeling. The paper begins by discussing the core principles and mathematical formulations, then explores various architectural choices and methods for maintaining temporal consistency. A taxonomy of applications is presented, categorizing models based on input modalities such as text prompts, images, videos, and audio signals. Advancements in text-to-video generation are discussed to illustrate the state-of-the-art capabilities and limitations of current approaches. Additionally, the survey summarizes recent developments in training and evaluation practices, including the use of diverse video and image datasets and the adoption of various evaluation metrics to assess model performance. The survey concludes with an examination of ongoing challenges, such as generating longer videos and managing computational costs, and offers insights into potential future directions for the field. By consolidating the latest research and developments, this survey aims to serve as a valuable resource for researchers and practitioners working with video diffusion models. Website: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\NZZHF88V\\Melnik et al. - 2024 - Video Diffusion Models A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\RK67C6MB\\2405.html}
}

@misc{mengAniDocAnimationCreation2025,
  title = {{{AniDoc}}: {{Animation Creation Made Easier}}},
  shorttitle = {{{AniDoc}}},
  author = {Meng, Yihao and Ouyang, Hao and Wang, Hanlin and Wang, Qiuyu and Wang, Wen and Cheng, Ka Leong and Liu, Zhiheng and Shen, Yujun and Qu, Huamin},
  year = {2025},
  month = jan,
  number = {arXiv:2412.14173},
  eprint = {2412.14173},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14173},
  urldate = {2025-03-22},
  abstract = {The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: https://yihao-meng.github.io/AniDoc\_demo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page and code: https://yihao-meng.github.io/AniDoc\_demo},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\PPEHLVN3\\Meng et al. - 2025 - AniDoc Animation Creation Made Easier.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\LB43MZFD\\2412.html}
}

@misc{meralMotionFlowAttentionDrivenMotion2024,
  title = {{{MotionFlow}}: {{Attention-Driven Motion Transfer}} in {{Video Diffusion Models}}},
  shorttitle = {{{MotionFlow}}},
  author = {Meral, Tuna Han Salih and Yesiltepe, Hidir and Dunlop, Connor and Yanardag, Pinar},
  year = {2024},
  month = dec,
  number = {arXiv:2412.05275},
  eprint = {2412.05275},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05275},
  urldate = {2025-03-19},
  abstract = {Text-to-video models have demonstrated impressive capabilities in producing diverse and captivating video content, showcasing a notable advancement in generative AI. However, these models generally lack fine-grained control over motion patterns, limiting their practical applicability. We introduce MotionFlow, a novel framework designed for motion transfer in video diffusion models. Our method utilizes cross-attention maps to accurately capture and manipulate spatial and temporal dynamics, enabling seamless motion transfers across various contexts. Our approach does not require training and works on test-time by leveraging the inherent capabilities of pre-trained video diffusion models. In contrast to traditional approaches, which struggle with comprehensive scene changes while maintaining consistent motion, MotionFlow successfully handles such complex transformations through its attention-based mechanism. Our qualitative and quantitative experiments demonstrate that MotionFlow significantly outperforms existing models in both fidelity and versatility even during drastic scene alterations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Page: https://motionflow-diffusion.github.io},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\7FXI9P5P\\Meral et al. - 2024 - MotionFlow Attention-Driven Motion Transfer in Video Diffusion Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\TEXMDNZH\\2412.html}
}

@misc{patashnikNestedAttentionSemanticaware2025,
  title = {Nested {{Attention}}: {{Semantic-aware Attention Values}} for {{Concept Personalization}}},
  shorttitle = {Nested {{Attention}}},
  author = {Patashnik, Or and Gal, Rinon and Ostashev, Daniil and Tulyakov, Sergey and Aberman, Kfir and {Cohen-Or}, Daniel},
  year = {2025},
  month = jan,
  number = {arXiv:2501.01407},
  eprint = {2501.01407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.01407},
  urldate = {2025-03-20},
  abstract = {Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  note = {Comment: Project page at https://snap-research.github.io/NestedAttention/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\37PW25CA\\Patashnik et al. - 2025 - Nested Attention Semantic-aware Attention Values for Concept Personalization.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\44HTHX4T\\2501.html}
}

@misc{pengOpenSora20Training2025,
  title = {Open-{{Sora}} 2.0: {{Training}} a {{Commercial-Level Video Generation Model}} in \$200k},
  shorttitle = {Open-{{Sora}} 2.0},
  author = {Peng, Xiangyu and Zheng, Zangwei and Shen, Chenhui and Young, Tom and Guo, Xinying and Wang, Binluo and Xu, Hang and Liu, Hongxin and Jiang, Mingyan and Li, Wenjun and Wang, Yuhui and Ye, Anbang and Ren, Gang and Ma, Qianran and Liang, Wanying and Lian, Xiang and Wu, Xiwen and Zhong, Yuting and Li, Zhuangyan and Gong, Chaoyu and Lei, Guojun and Cheng, Leijun and Zhang, Limin and Li, Minghao and Zhang, Ruijie and Hu, Silan and Huang, Shijie and Wang, Xiaokang and Zhao, Yuanheng and Wang, Yuqi and Wei, Ziang and You, Yang},
  year = {2025},
  month = mar,
  number = {arXiv:2503.09642},
  eprint = {2503.09642},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.09642},
  urldate = {2025-06-18},
  abstract = {Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only \$200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Graphics},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\PBMQB6KQ\\Peng et al. - 2025 - Open-Sora 2.0 Training a Commercial-Level Video Generation Model in $200k.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\I6ETQFTY\\2503.html}
}

@misc{polyakMovieGenCast2025,
  title = {Movie {{Gen}}: {{A Cast}} of {{Media Foundation Models}}},
  shorttitle = {Movie {{Gen}}},
  author = {Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and Yan, David and Choudhary, Dhruv and Wang, Dingkang and Sethi, Geet and Pang, Guan and Ma, Haoyu and Misra, Ishan and Hou, Ji and Wang, Jialiang and Jagadeesh, Kiran and Li, Kunpeng and Zhang, Luxin and Singh, Mannat and Williamson, Mary and Le, Matt and Yu, Matthew and Singh, Mitesh Kumar and Zhang, Peizhao and Vajda, Peter and Duval, Quentin and Girdhar, Rohit and Sumbaly, Roshan and Rambhatla, Sai Saketh and Tsai, Sam and Azadi, Samaneh and Datta, Samyak and Chen, Sanyuan and Bell, Sean and Ramaswamy, Sharadh and Sheynin, Shelly and Bhattacharya, Siddharth and Motwani, Simran and Xu, Tao and Li, Tianhe and Hou, Tingbo and Hsu, Wei-Ning and Yin, Xi and Dai, Xiaoliang and Taigman, Yaniv and Luo, Yaqiao and Liu, Yen-Cheng and Wu, Yi-Chiao and Zhao, Yue and Kirstain, Yuval and He, Zecheng and He, Zijian and Pumarola, Albert and Thabet, Ali and Sanakoyeu, Artsiom and Mallya, Arun and Guo, Baishan and Araya, Boris and Kerr, Breena and Wood, Carleigh and Liu, Ce and Peng, Cen and Vengertsev, Dimitry and Schonfeld, Edgar and Blanchard, Elliot and {Juefei-Xu}, Felix and Nord, Fraylie and Liang, Jeff and Hoffman, John and Kohler, Jonas and Fire, Kaolin and Sivakumar, Karthik and Chen, Lawrence and Yu, Licheng and Gao, Luya and Georgopoulos, Markos and Moritz, Rashel and Sampson, Sara K. and Li, Shikai and Parmeggiani, Simone and Fine, Steve and Fowler, Tara and Petrovic, Vladan and Du, Yuming},
  year = {2025},
  month = feb,
  number = {arXiv:2410.13720},
  eprint = {2410.13720},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13720},
  urldate = {2025-04-24},
  abstract = {We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\FNNU7FP7\\Polyak et al. - 2025 - Movie Gen A Cast of Media Foundation Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\VEQ3Q44B\\2410.html}
}

@misc{polyakMovieGenCast2025a,
  title = {Movie {{Gen}}: {{A Cast}} of {{Media Foundation Models}}},
  shorttitle = {Movie {{Gen}}},
  author = {Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and Yan, David and Choudhary, Dhruv and Wang, Dingkang and Sethi, Geet and Pang, Guan and Ma, Haoyu and Misra, Ishan and Hou, Ji and Wang, Jialiang and Jagadeesh, Kiran and Li, Kunpeng and Zhang, Luxin and Singh, Mannat and Williamson, Mary and Le, Matt and Yu, Matthew and Singh, Mitesh Kumar and Zhang, Peizhao and Vajda, Peter and Duval, Quentin and Girdhar, Rohit and Sumbaly, Roshan and Rambhatla, Sai Saketh and Tsai, Sam and Azadi, Samaneh and Datta, Samyak and Chen, Sanyuan and Bell, Sean and Ramaswamy, Sharadh and Sheynin, Shelly and Bhattacharya, Siddharth and Motwani, Simran and Xu, Tao and Li, Tianhe and Hou, Tingbo and Hsu, Wei-Ning and Yin, Xi and Dai, Xiaoliang and Taigman, Yaniv and Luo, Yaqiao and Liu, Yen-Cheng and Wu, Yi-Chiao and Zhao, Yue and Kirstain, Yuval and He, Zecheng and He, Zijian and Pumarola, Albert and Thabet, Ali and Sanakoyeu, Artsiom and Mallya, Arun and Guo, Baishan and Araya, Boris and Kerr, Breena and Wood, Carleigh and Liu, Ce and Peng, Cen and Vengertsev, Dimitry and Schonfeld, Edgar and Blanchard, Elliot and {Juefei-Xu}, Felix and Nord, Fraylie and Liang, Jeff and Hoffman, John and Kohler, Jonas and Fire, Kaolin and Sivakumar, Karthik and Chen, Lawrence and Yu, Licheng and Gao, Luya and Georgopoulos, Markos and Moritz, Rashel and Sampson, Sara K. and Li, Shikai and Parmeggiani, Simone and Fine, Steve and Fowler, Tara and Petrovic, Vladan and Du, Yuming},
  year = {2025},
  month = feb,
  number = {arXiv:2410.13720},
  eprint = {2410.13720},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13720},
  urldate = {2025-06-18},
  abstract = {We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\H8NB938G\\Polyak et al. - 2025 - Movie Gen A Cast of Media Foundation Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\U2IBN87H\\2410.html}
}

@misc{qiuAnimeShooterMultiShotAnimation2025,
  title = {{{AnimeShooter}}: {{A Multi-Shot Animation Dataset}} for {{Reference-Guided Video Generation}}},
  shorttitle = {{{AnimeShooter}}},
  author = {Qiu, Lu and Li, Yizhuo and Ge, Yuying and Ge, Yixiao and Shan, Ying and Liu, Xihui},
  year = {2025},
  month = jun,
  number = {arXiv:2506.03126},
  eprint = {2506.03126},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.03126},
  urldate = {2025-06-18},
  abstract = {Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project released at: https://qiulu66.github.io/animeshooter/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\F5XHVBVX\\Qiu et al. - 2025 - AnimeShooter A Multi-Shot Animation Dataset for Reference-Guided Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\QK26YWWN\\2506.html}
}

@misc{renVideoRAGRetrievalAugmentedGeneration2025,
  title = {{{VideoRAG}}: {{Retrieval-Augmented Generation}} with {{Extreme Long-Context Videos}}},
  shorttitle = {{{VideoRAG}}},
  author = {Ren, Xubin and Xu, Lingrui and Xia, Long and Wang, Shuaiqiang and Yin, Dawei and Huang, Chao},
  year = {2025},
  month = feb,
  number = {arXiv:2502.01549},
  eprint = {2502.01549},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.01549},
  urldate = {2025-04-28},
  abstract = {Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in enhancing Large Language Models (LLMs) through external knowledge integration, yet its application has primarily focused on textual content, leaving the rich domain of multi-modal video knowledge predominantly unexplored. This paper introduces VideoRAG, the first retrieval-augmented generation framework specifically designed for processing and understanding extremely long-context videos. Our core innovation lies in its dual-channel architecture that seamlessly integrates (i) graph-based textual knowledge grounding for capturing cross-video semantic relationships, and (ii) multi-modal context encoding for efficiently preserving visual features. This novel design empowers VideoRAG to process unlimited-length videos by constructing precise knowledge graphs that span multiple videos while maintaining semantic dependencies through specialized multi-modal retrieval paradigms. Through comprehensive empirical evaluation on our proposed LongerVideos benchmark-comprising over 160 videos totaling 134+ hours across lecture, documentary, and entertainment categories-VideoRAG demonstrates substantial performance compared to existing RAG alternatives and long video understanding methods. The source code of VideoRAG implementation and the benchmark dataset are openly available at: https://github.com/HKUDS/VideoRAG.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\XV9HNZSN\\Ren et al. - 2025 - VideoRAG Retrieval-Augmented Generation with Extreme Long-Context Videos.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\HAAQVJAK\\2502.html}
}

@misc{shenStoryGPTVLargeLanguage2023,
  title = {{{StoryGPT-V}}: {{Large Language Models}} as {{Consistent Story Visualizers}}},
  shorttitle = {{{StoryGPT-V}}},
  author = {Shen, Xiaoqian and Elhoseiny, Mohamed},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02252},
  eprint = {2312.02252},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.02252},
  urldate = {2025-03-22},
  abstract = {Recent generative models have demonstrated impressive capabilities in generating realistic and visually pleasing images grounded on textual prompts. Nevertheless, a significant challenge remains in applying these models for the more intricate task of story visualization. Since it requires resolving pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution, and ensuring consistent characters and background synthesis across frames. Yet, the emerging Large Language Model (LLM) showcases robust reasoning abilities to navigate through ambiguous references and process extensive sequences. Therefore, we introduce {\textbackslash}textbf\{StoryGPT-V\}, which leverages the merits of the latent diffusion (LDM) and LLM to produce images with consistent and high-quality characters grounded on given story descriptions. First, we train a character-aware LDM, which takes character-augmented semantic embedding as input and includes the supervision of the cross-attention map using character segmentation masks, aiming to enhance character generation accuracy and faithfulness. In the second stage, we enable an alignment between the output of LLM and the character-augmented embedding residing in the input space of the first-stage model. This harnesses the reasoning ability of LLM to address ambiguous references and the comprehension capability to memorize the context. We conduct comprehensive experiments on two visual story visualization benchmarks. Our model reports superior quantitative results and consistently generates accurate characters of remarkable quality with low memory consumption. Our code will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://xiaoqian-shen.github.io/StoryGPT-V},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\3K93A4LD\\Shen and Elhoseiny - 2023 - StoryGPT-V Large Language Models as Consistent Story Visualizers.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\2LTJQU6M\\2312.html}
}

@misc{shiAniMakerAutomatedMultiAgent2025,
  title = {{{AniMaker}}: {{Automated Multi-Agent Animated Storytelling}} with {{MCTS-Driven Clip Generation}}},
  shorttitle = {{{AniMaker}}},
  author = {Shi, Haoyuan and Li, Yunxin and Chen, Xinyu and Wang, Longyue and Hu, Baotian and Zhang, Min},
  year = {2025},
  month = jun,
  number = {arXiv:2506.10540},
  eprint = {2506.10540},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.10540},
  urldate = {2025-06-18},
  abstract = {Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multiagent Systems},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\54FD7I29\\Shi et al. - 2025 - AniMaker Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\P2NBD327\\2506.html}
}

@misc{shinLargeScaleTexttoImageModel2024,
  title = {Large-{{Scale Text-to-Image Model}} with {{Inpainting}} Is a {{Zero-Shot Subject-Driven Image Generator}}},
  author = {Shin, Chaehun and Choi, Jooyoung and Kim, Heeseung and Yoon, Sungroh},
  year = {2024},
  month = nov,
  number = {arXiv:2411.15466},
  eprint = {2411.15466},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.15466},
  urldate = {2025-04-11},
  abstract = {Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\PU9C9IIQ\\Shin et al. - 2024 - Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\2MXNS584\\2411.html}
}

@misc{ShowlabMovieAgentMovieAgent,
  title = {Showlab/{{MovieAgent}}: {{MovieAgent}}: {{Automated Movie Generation}} via {{Multi-Agent CoT Planning}}},
  urldate = {2025-04-22},
  howpublished = {https://github.com/showlab/MovieAgent/tree/main},
  file = {C:\Users\melmoghany\Zotero\storage\5TXHGPZ8\main.html}
}

@inproceedings{singhSurveyAITexttoImage2023,
  title = {A {{Survey}} of {{AI Text-to-Image}} and {{AI Text-to-Video Generators}}},
  booktitle = {2023 4th {{International Conference}} on {{Artificial Intelligence}}, {{Robotics}} and {{Control}} ({{AIRC}})},
  author = {Singh, Aditi},
  year = {2023},
  month = may,
  eprint = {2311.06329},
  primaryclass = {cs},
  pages = {32--36},
  doi = {10.1109/AIRC57904.2023.10303174},
  urldate = {2025-06-02},
  abstract = {Text-to-Image and Text-to-Video AI generation models are revolutionary technologies that use deep learning and natural language processing (NLP) techniques to create images and videos from textual descriptions. This paper investigates cutting-edge approaches in the discipline of Text-to-Image and Text-to-Video AI generations. The survey provides an overview of the existing literature as well as an analysis of the approaches used in various studies. It covers data preprocessing techniques, neural network types, and evaluation metrics used in the field. In addition, the paper discusses the challenges and limitations of Text-to-Image and Text-to-Video AI generations, as well as future research directions. Overall, these models have promising potential for a wide range of applications such as video production, content creation, and digital marketing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {Comment: 4 pages, 2 tables, 4th International Conference on Artificial Intelligence, Robotics and Control (AIRC 2023)},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\TQHYNURP\\Singh - 2023 - A Survey of AI Text-to-Image and AI Text-to-Video Generators.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\JALZINEK\\2311.html}
}

@misc{songMoMAMultimodalLLM2024,
  title = {{{MoMA}}: {{Multimodal LLM Adapter}} for {{Fast Personalized Image Generation}}},
  shorttitle = {{{MoMA}}},
  author = {Song, Kunpeng and Zhu, Yizhe and Liu, Bingchen and Yan, Qing and Elgammal, Ahmed and Yang, Xiao},
  year = {2024},
  month = apr,
  number = {arXiv:2404.05674},
  eprint = {2404.05674},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.05674},
  urldate = {2025-03-20},
  abstract = {In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\BAG2ZZIY\\Song et al. - 2024 - MoMA Multimodal LLM Adapter for Fast Personalized Image Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\2HKMX7KK\\2404.html}
}

@misc{sunGLOBERCoherentNonautoregressive2023,
  title = {{{GLOBER}}: {{Coherent Non-autoregressive Video Generation}} via {{GLOBal Guided Video DecodER}}},
  shorttitle = {{{GLOBER}}},
  author = {Sun, Mingzhen and Wang, Weining and Qin, Zihan and Sun, Jiahui and Chen, Sihan and Liu, Jing},
  year = {2023},
  month = sep,
  number = {arXiv:2309.13274},
  eprint = {2309.13274},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.13274},
  urldate = {2025-05-30},
  abstract = {Video generation necessitates both global coherence and local realism. This work presents a novel non-autoregressive method GLOBER, which first generates global features to obtain comprehensive global guidance and then synthesizes video frames based on the global features to generate coherent videos. Specifically, we propose a video auto-encoder, where a video encoder encodes videos into global features, and a video decoder, built on a diffusion model, decodes the global features and synthesizes video frames in a non-autoregressive manner. To achieve maximum flexibility, our video decoder perceives temporal information through normalized frame indexes, which enables it to synthesize arbitrary sub video clips with predetermined starting and ending frame indexes. Moreover, a novel adversarial loss is introduced to improve the global coherence and local realism between the synthesized video frames. Finally, we employ a diffusion-based video generator to fit the global features outputted by the video encoder for video generation. Extensive experimental results demonstrate the effectiveness and efficiency of our proposed method, and new state-of-the-art results have been achieved on multiple benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\YGAYR6WH\\Sun et al. - 2023 - GLOBER Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ETJWU2PV\\2309.html}
}

@misc{sunSoraWhatWe2024,
  title = {From {{Sora What We Can See}}: {{A Survey}} of {{Text-to-Video Generation}}},
  shorttitle = {From {{Sora What We Can See}}},
  author = {Sun, Rui and Zhang, Yumin and Shah, Tejal and Sun, Jiahao and Zhang, Shuoying and Li, Wenqi and Duan, Haoran and Wei, Bo and Ranjan, Rajiv},
  year = {2024},
  month = may,
  number = {arXiv:2405.10674},
  eprint = {2405.10674},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.10674},
  urldate = {2025-06-02},
  abstract = {With impressive achievements made, artificial intelligence is on the path forward to artificial general intelligence. Sora, developed by OpenAI, which is capable of minute-level world-simulative abilities can be considered as a milestone on this developmental path. However, despite its notable successes, Sora still encounters various obstacles that need to be resolved. In this survey, we embark from the perspective of disassembling Sora in text-to-video generation, and conducting a comprehensive review of literature, trying to answer the question, {\textbackslash}textit\{From Sora What We Can See\}. Specifically, after basic preliminaries regarding the general algorithms are introduced, the literature is categorized from three mutually perpendicular dimensions: evolutionary generators, excellent pursuit, and realistic panorama. Subsequently, the widely used datasets and metrics are organized in detail. Last but more importantly, we identify several challenges and open problems in this domain and propose potential future directions for research and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: A comprehensive list of text-to-video generation studies in this survey is available at https://github.com/soraw-ai/Awesome-Text-to-Video-Generation},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\I35D2JHK\\Sun et al. - 2024 - From Sora What We Can See A Survey of Text-to-Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\AH5AVMKY\\2405.html}
}

@misc{tangGenerativeAICelAnimation2025,
  title = {Generative {{AI}} for {{Cel-Animation}}: {{A Survey}}},
  shorttitle = {Generative {{AI}} for {{Cel-Animation}}},
  author = {Tang, Yunlong and Guo, Junjia and Liu, Pinxin and Wang, Zhiyuan and Hua, Hang and Zhong, Jia-Xing and Xiao, Yunzhong and Huang, Chao and Song, Luchuan and Liang, Susan and Song, Yizhi and He, Liu and Bi, Jing and Feng, Mingqian and Li, Xinyang and Zhang, Zeliang and Xu, Chenliang},
  year = {2025},
  month = jan,
  number = {arXiv:2501.06250},
  eprint = {2501.06250},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.06250},
  urldate = {2025-03-20},
  abstract = {Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, issues such as maintaining visual consistency, ensuring stylistic coherence, and addressing ethical considerations continue to pose challenges. Furthermore, this paper discusses future directions and explores potential advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction},
  note = {Comment: 20 pages},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\C9AXK4DD\\Tang et al. - 2025 - Generative AI for Cel-Animation A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\IGT2SI4N\\2501.html}
}

@misc{tangGenerativeAICelAnimation2025a,
  title = {Generative {{AI}} for {{Cel-Animation}}: {{A Survey}}},
  shorttitle = {Generative {{AI}} for {{Cel-Animation}}},
  author = {Tang, Yunlong and Guo, Junjia and Liu, Pinxin and Wang, Zhiyuan and Hua, Hang and Zhong, Jia-Xing and Xiao, Yunzhong and Huang, Chao and Song, Luchuan and Liang, Susan and Song, Yizhi and He, Liu and Bi, Jing and Feng, Mingqian and Li, Xinyang and Zhang, Zeliang and Xu, Chenliang},
  year = {2025},
  month = jan,
  number = {arXiv:2501.06250},
  eprint = {2501.06250},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.06250},
  urldate = {2025-06-02},
  abstract = {Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, issues such as maintaining visual consistency, ensuring stylistic coherence, and addressing ethical considerations continue to pose challenges. Furthermore, this paper discusses future directions and explores potential advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction},
  note = {Comment: 20 pages},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\SEMYJP3T\\Tang et al. - 2025 - Generative AI for Cel-Animation A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\G3ZG8UXV\\2501.html}
}

@misc{tanOminiControl2EfficientConditioning2025,
  title = {{{OminiControl2}}: {{Efficient Conditioning}} for {{Diffusion Transformers}}},
  shorttitle = {{{OminiControl2}}},
  author = {Tan, Zhenxiong and Xue, Qiaochu and Yang, Xingyi and Liu, Songhua and Wang, Xinchao},
  year = {2025},
  month = mar,
  number = {arXiv:2503.08280},
  eprint = {2503.08280},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.08280},
  urldate = {2025-03-20},
  abstract = {Fine-grained control of text-to-image diffusion transformer models (DiT) remains a critical challenge for practical deployment. While recent advances such as OminiControl and others have enabled a controllable generation of diverse control signals, these methods face significant computational inefficiency when handling long conditional inputs. We present OminiControl2, an efficient framework that achieves efficient image-conditional image generation. OminiControl2 introduces two key innovations: (1) a dynamic compression strategy that streamlines conditional inputs by preserving only the most semantically relevant tokens during generation, and (2) a conditional feature reuse mechanism that computes condition token features only once and reuses them across denoising steps. These architectural improvements preserve the original framework's parameter efficiency and multi-modal versatility while dramatically reducing computational costs. Our experiments demonstrate that OminiControl2 reduces conditional processing overhead by over 90\% compared to its predecessor, achieving an overall 5.9\${\textbackslash}times\$ speedup in multi-conditional generation scenarios. This efficiency enables the practical implementation of complex, multi-modal control for high-quality image synthesis with DiT models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\52F6VWY3\\Tan et al. - 2025 - OminiControl2 Efficient Conditioning for Diffusion Transformers.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\DRPLUXNV\\2503.html}
}

@misc{tanOminiControlMinimalUniversal2025,
  title = {{{OminiControl}}: {{Minimal}} and {{Universal Control}} for {{Diffusion Transformer}}},
  shorttitle = {{{OminiControl}}},
  author = {Tan, Zhenxiong and Liu, Songhua and Yang, Xingyi and Xue, Qiaochu and Wang, Xinchao},
  year = {2025},
  month = mar,
  number = {arXiv:2411.15098},
  eprint = {2411.15098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.15098},
  urldate = {2025-03-20},
  abstract = {We present OminiControl, a novel approach that rethinks how image conditions are integrated into Diffusion Transformer (DiT) architectures. Current image conditioning methods either introduce substantial parameter overhead or handle only specific control tasks effectively, limiting their practical versatility. OminiControl addresses these limitations through three key innovations: (1) a minimal architectural design that leverages the DiT's own VAE encoder and transformer blocks, requiring just 0.1\% additional parameters; (2) a unified sequence processing strategy that combines condition tokens with image tokens for flexible token interactions; and (3) a dynamic position encoding mechanism that adapts to both spatially-aligned and non-aligned control tasks. Our extensive experiments show that this streamlined approach not only matches but surpasses the performance of specialized methods across multiple conditioning tasks. To overcome data limitations in subject-driven generation, we also introduce Subjects200K, a large-scale dataset of identity-consistent image pairs synthesized using DiT models themselves. This work demonstrates that effective image control can be achieved without architectural complexity, opening new possibilities for efficient and versatile image generation systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\8RNTQ6UF\\Tan et al. - 2025 - OminiControl Minimal and Universal Control for Diffusion Transformer.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\6XRH9LWM\\2411.html}
}

@inproceedings{taoCoInLightweightEffective2024,
  title = {{{CoIn}}: {{A Lightweight}} and {{Effective Framework}} for {{Story Visualization}} and {{Continuation}}},
  shorttitle = {{{CoIn}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Multimedia}}},
  author = {Tao, Ming and Bao, Bing-Kun and Tang, Hao and Wang, Yaowei and Xu, Changsheng},
  year = {2024},
  month = oct,
  pages = {10659--10668},
  publisher = {ACM},
  address = {Melbourne VIC Australia},
  doi = {10.1145/3664647.3680873},
  urldate = {2025-03-22},
  isbn = {979-8-4007-0686-8},
  langid = {english}
}

@misc{taoStoryImagerUnifiedEfficient2024,
  title = {{{StoryImager}}: {{A Unified}} and {{Efficient Framework}} for {{Coherent Story Visualization}} and {{Completion}}},
  shorttitle = {{{StoryImager}}},
  author = {Tao, Ming and Bao, Bing-Kun and Tang, Hao and Wang, Yaowei and Xu, Changsheng},
  year = {2024},
  month = apr,
  number = {arXiv:2404.05979},
  eprint = {2404.05979},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.05979},
  urldate = {2025-03-20},
  abstract = {Story visualization aims to generate a series of realistic and coherent images based on a storyline. Current models adopt a frame-by-frame architecture by transforming the pre-trained text-to-image model into an auto-regressive manner. Although these models have shown notable progress, there are still three flaws. 1) The unidirectional generation of auto-regressive manner restricts the usability in many scenarios. 2) The additional introduced story history encoders bring an extremely high computational cost. 3) The story visualization and continuation models are trained and inferred independently, which is not user-friendly. To these ends, we propose a bidirectional, unified, and efficient framework, namely StoryImager. The StoryImager enhances the storyboard generative ability inherited from the pre-trained text-to-image model for a bidirectional generation. Specifically, we introduce a Target Frame Masking Strategy to extend and unify different story image generation tasks. Furthermore, we propose a Frame-Story Cross Attention Module that decomposes the cross attention for local fidelity and global coherence. Moreover, we design a Contextual Feature Extractor to extract contextual information from the whole storyline. The extensive experimental results demonstrate the excellent performance of our StoryImager. The code is available at https://github.com/tobran/StoryImager.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 17 pages},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\XYC5IYWS\\Tao et al. - 2024 - StoryImager A Unified and Efficient Framework for Coherent Story Visualization and Completion.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\I6EPXBCR\\2404.html}
}

@misc{tewelTrainingFreeConsistentTexttoImage2024,
  title = {Training-{{Free Consistent Text-to-Image Generation}}},
  author = {Tewel, Yoad and Kaduri, Omri and Gal, Rinon and Kasten, Yoni and Wolf, Lior and Chechik, Gal and Atzmon, Yuval},
  year = {2024},
  month = may,
  number = {arXiv:2402.03286},
  eprint = {2402.03286},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03286},
  urldate = {2025-03-10},
  abstract = {Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.},
  archiveprefix = {arXiv},
  note = {Comment: Accepted to journal track of SIGGRAPH 2024 (TOG). Project page is at https://consistory-paper.github.io},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\RF26FHQD\\Tewel et al. - 2024 - Training-Free Consistent Text-to-Image Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\XM64MDMS\\2402.html}
}

@misc{tewelTrainingFreeConsistentTexttoImage2024a,
  title = {Training-{{Free Consistent Text-to-Image Generation}}},
  author = {Tewel, Yoad and Kaduri, Omri and Gal, Rinon and Kasten, Yoni and Wolf, Lior and Chechik, Gal and Atzmon, Yuval},
  year = {2024},
  month = may,
  number = {arXiv:2402.03286},
  eprint = {2402.03286},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03286},
  urldate = {2025-04-25},
  abstract = {Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  note = {Comment: Accepted to journal track of SIGGRAPH 2024 (TOG). Project page is at https://consistory-paper.github.io},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\MIYJTG9T\\Tewel et al. - 2024 - Training-Free Consistent Text-to-Image Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\KQS2F9UN\\2402.html}
}

@misc{wangDocVideoQAComprehensiveUnderstanding2025,
  title = {{{DocVideoQA}}: {{Towards Comprehensive Understanding}} of {{Document-Centric Videos}} through {{Question Answering}}},
  shorttitle = {{{DocVideoQA}}},
  author = {Wang, Haochen and Hu, Kai and Gao, Liangcai},
  year = {2025},
  month = mar,
  number = {arXiv:2503.15887},
  eprint = {2503.15887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.15887},
  urldate = {2025-03-23},
  abstract = {Remote work and online courses have become important methods of knowledge dissemination, leading to a large number of document-based instructional videos. Unlike traditional video datasets, these videos mainly feature rich-text images and audio that are densely packed with information closely tied to the visual content, requiring advanced multimodal understanding capabilities. However, this domain remains underexplored due to dataset availability and its inherent complexity. In this paper, we introduce the DocVideoQA task and dataset for the first time, comprising 1454 videos across 23 categories with a total duration of about 828 hours. The dataset is annotated with 154k question-answer pairs generated manually and via GPT, assessing models' comprehension, temporal awareness, and modality integration capabilities. Initially, we establish a baseline using open-source MLLMs. Recognizing the challenges in modality comprehension for document-centric videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances unimodal feature extraction with diverse instruction-tuning data and employs contrastive learning to strengthen modality integration. Through fine-tuning, the LLM is equipped with audio-visual capabilities, leading to significant improvements in document-centric video understanding. Extensive testing on the DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing models. We'll release the code and dataset to facilitate future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\AG2VT5FK\\Wang et al. - 2025 - DocVideoQA Towards Comprehensive Understanding of Document-Centric Videos through Question Answerin.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\RKZFZ35A\\2503.html}
}

@misc{wangDreamRunnerFineGrainedCompositional2025,
  title = {{{DreamRunner}}: {{Fine-Grained Compositional Story-to-Video Generation}} with {{Retrieval-Augmented Motion Adaptation}}},
  shorttitle = {{{DreamRunner}}},
  author = {Wang, Zun and Li, Jialu and Lin, Han and Yoon, Jaehong and Bansal, Mohit},
  year = {2025},
  month = mar,
  number = {arXiv:2411.16657},
  eprint = {2411.16657},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.16657},
  urldate = {2025-03-22},
  abstract = {Storytelling video generation (SVG) aims to produce coherent and visually rich multi-scene videos that follow a structured narrative. Existing methods primarily employ LLM for high-level planning to decompose a story into scene-level descriptions, which are then independently generated and stitched together. However, these approaches struggle with generating high-quality videos aligned with the complex single-scene description, as visualizing such complex description involves coherent composition of multiple characters and events, complex motion synthesis and muti-character customization. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project website: https://zunwang1.github.io/DreamRunner},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\8IE2CU8H\\Wang et al. - 2025 - DreamRunner Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Ad.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\6NFZ79FN\\2411.html}
}

@misc{wangGenLVideoMultiTextLong2023,
  title = {Gen-{{L-Video}}: {{Multi-Text}} to {{Long Video Generation}} via {{Temporal Co-Denoising}}},
  shorttitle = {Gen-{{L-Video}}},
  author = {Wang, Fu-Yun and Chen, Wenshuo and Song, Guanglu and Ye, Han-Jia and Liu, Yu and Li, Hongsheng},
  year = {2023},
  month = may,
  number = {arXiv:2305.18264},
  eprint = {2305.18264},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18264},
  urldate = {2025-05-30},
  abstract = {Leveraging large-scale image-text datasets and advancements in diffusion models, text-driven generative models have made remarkable strides in the field of image generation and editing. This study explores the potential of extending the text-driven ability to the generation and editing of multi-text conditioned long videos. Current methodologies for video generation and editing, while innovative, are often confined to extremely short videos (typically less than 24 frames) and are limited to a single text condition. These constraints significantly limit their applications given that real-world videos usually consist of multiple segments, each bearing different semantic information. To address this challenge, we introduce a novel paradigm dubbed as Gen-L-Video, capable of extending off-the-shelf short video diffusion models for generating and editing videos comprising hundreds of frames with diverse semantic segments without introducing additional training, all while preserving content consistency. We have implemented three mainstream text-driven video generation and editing methodologies and extended them to accommodate longer videos imbued with a variety of semantic segments with our proposed paradigm. Our experimental outcomes reveal that our approach significantly broadens the generative and editing capabilities of video diffusion models, offering new possibilities for future research and applications. The code is available at https://github.com/G-U-N/Gen-L-Video.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: The code is available at https://github.com/G-U-N/Gen-L-Video},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\GL7MJTQI\\Wang et al. - 2023 - Gen-L-Video Multi-Text to Long Video Generation via Temporal Co-Denoising.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\2KW4XJWF\\2305.html}
}

@misc{wangMotionInversionVideo2024,
  title = {Motion {{Inversion}} for {{Video Customization}}},
  author = {Wang, Luozhou and Mai, Ziyang and Shen, Guibao and Liang, Yixun and Tao, Xin and Wan, Pengfei and Zhang, Di and Li, Yijun and Chen, Yingcong},
  year = {2024},
  month = oct,
  number = {arXiv:2403.20193},
  eprint = {2403.20193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.20193},
  urldate = {2025-03-20},
  abstract = {In this work, we present a novel approach for motion customization in video generation, addressing the widespread gap in the exploration of motion representation within video generative models. Recognizing the unique challenges posed by the spatiotemporal nature of video, our method introduces Motion Embeddings, a set of explicit, temporally coherent embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach provides a compact and efficient solution to motion representation, utilizing two types of embeddings: a Motion Query-Key Embedding to modulate the temporal attention map and a Motion Value Embedding to modulate the attention values. Additionally, we introduce an inference strategy that excludes spatial dimensions from the Motion Query-Key Embedding and applies a differential operation to the Motion Value Embedding, both designed to debias appearance and ensure the embeddings focus solely on motion. Our contributions include the introduction of a tailored motion embedding for customization tasks and a demonstration of the practical advantages and effectiveness of our method through extensive experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: https://wileewang.github.io/MotionInversion/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\2GXEYEXB\\Wang et al. - 2024 - Motion Inversion for Video Customization.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\PRTV6PWB\\2403.html}
}

@misc{wangSlideSpeechLargeScaleSlideEnriched2023,
  title = {{{SlideSpeech}}: {{A Large-Scale Slide-Enriched Audio-Visual Corpus}}},
  shorttitle = {{{SlideSpeech}}},
  author = {Wang, Haoxu and Yu, Fan and Shi, Xian and Wang, Yuezhang and Zhang, Shiliang and Li, Ming},
  year = {2023},
  month = dec,
  number = {arXiv:2309.05396},
  eprint = {2309.05396},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.05396},
  urldate = {2025-03-23},
  abstract = {Multi-Modal automatic speech recognition (ASR) techniques aim to leverage additional modalities to improve the performance of speech recognition systems. While existing approaches primarily focus on video or contextual information, the utilization of extra supplementary textual information has been overlooked. Recognizing the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: Accepted by ICASSP 2024},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\MRE5M8IQ\\Wang et al. - 2023 - SlideSpeech A Large-Scale Slide-Enriched Audio-Visual Corpus.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\ZR9WHPTY\\2309.html}
}

@misc{wangSurveyVideoDiffusion2025,
  title = {Survey of {{Video Diffusion Models}}: {{Foundations}}, {{Implementations}}, and {{Applications}}},
  shorttitle = {Survey of {{Video Diffusion Models}}},
  author = {Wang, Yimu and Liu, Xuye and Pang, Wei and Ma, Li and Yuan, Shuai and Debevec, Paul and Yu, Ning},
  year = {2025},
  month = apr,
  number = {arXiv:2504.16081},
  eprint = {2504.16081},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16081},
  urldate = {2025-06-02},
  abstract = {Recent advances in diffusion models have revolutionized video generation, offering superior temporal consistency and visual quality compared to traditional generative adversarial networks-based approaches. While this emerging field shows tremendous promise in applications, it faces significant challenges in motion consistency, computational efficiency, and ethical considerations. This survey provides a comprehensive review of diffusion-based video generation, examining its evolution, technical foundations, and practical applications. We present a systematic taxonomy of current methodologies, analyze architectural innovations and optimization strategies, and investigate applications across low-level vision tasks such as denoising and super-resolution. Additionally, we explore the synergies between diffusionbased video generation and related domains, including video representation learning, question answering, and retrieval. Compared to the existing surveys (Lei et al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which focus on specific aspects of video generation, such as human video synthesis (Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our work provides a broader, more updated, and more fine-grained perspective on diffusion-based approaches with a special section for evaluation metrics, industry solutions, and training engineering techniques in video generation. This survey serves as a foundational resource for researchers and practitioners working at the intersection of diffusion models and video generation, providing insights into both the theoretical frameworks and practical implementations that drive this rapidly evolving field. A structured list of related works involved in this survey is also available on https://github.com/Eyeline-Research/Survey-Video-Diffusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\9Z3S88GB\\Wang et al. - 2025 - Survey of Video Diffusion Models Foundations, Implementations, and Applications.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\KUT6WRVR\\2504.html}
}

@misc{wangTransPixelerAdvancingTexttoVideo2025,
  title = {{{TransPixeler}}: {{Advancing Text-to-Video Generation}} with {{Transparency}}},
  shorttitle = {{{TransPixeler}}},
  author = {Wang, Luozhou and Li, Yijun and Chen, Zhifei and Wang, Jui-Hsien and Zhang, Zhifei and Zhang, He and Lin, Zhe and Chen, Yingcong},
  year = {2025},
  month = jan,
  number = {arXiv:2501.03006},
  eprint = {2501.03006},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.03006},
  urldate = {2025-03-19},
  abstract = {Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixeler, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://wileewang.github.io/TransPixar/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\A2RE2ZJD\\Wang et al. - 2025 - TransPixeler Advancing Text-to-Video Generation with Transparency.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\L68HMA2Z\\2501.html}
}

@misc{wanWanOpenAdvanced2025,
  title = {Wan: {{Open}} and {{Advanced Large-Scale Video Generative Models}}},
  shorttitle = {Wan},
  author = {Wan, Team and Wang, Ang and Ai, Baole and Wen, Bin and Mao, Chaojie and Xie, Chen-Wei and Chen, Di and Yu, Feiwu and Zhao, Haiming and Yang, Jianxiao and Zeng, Jianyuan and Wang, Jiayu and Zhang, Jingfeng and Zhou, Jingren and Wang, Jinkai and Chen, Jixuan and Zhu, Kai and Zhao, Kang and Yan, Keyu and Huang, Lianghua and Feng, Mengyang and Zhang, Ningyi and Li, Pandeng and Wu, Pingyu and Chu, Ruihang and Feng, Ruili and Zhang, Shiwei and Sun, Siyang and Fang, Tao and Wang, Tianxing and Gui, Tianyi and Weng, Tingyu and Shen, Tong and Lin, Wei and Wang, Wei and Wang, Wei and Zhou, Wenmeng and Wang, Wente and Shen, Wenting and Yu, Wenyuan and Shi, Xianzhong and Huang, Xiaoming and Xu, Xin and Kou, Yan and Lv, Yangyu and Li, Yifei and Liu, Yijing and Wang, Yiming and Zhang, Yingya and Huang, Yitong and Li, Yong and Wu, You and Liu, Yu and Pan, Yulin and Zheng, Yun and Hong, Yuntao and Shi, Yupeng and Feng, Yutong and Jiang, Zeyinzi and Han, Zhen and Wu, Zhi-Fan and Liu, Ziyu},
  year = {2025},
  month = apr,
  number = {arXiv:2503.20314},
  eprint = {2503.20314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.20314},
  urldate = {2025-06-18},
  abstract = {This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 60 pages, 33 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\M5UIREGR\\Wan et al. - 2025 - Wan Open and Advanced Large-Scale Video Generative Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\Z8UB24H2\\2503.html}
}

@misc{waseemVideoWorthThousand2024,
  title = {Video {{Is Worth}} a {{Thousand Images}}: {{Exploring}} the {{Latest Trends}} in {{Long Video Generation}}},
  shorttitle = {Video {{Is Worth}} a {{Thousand Images}}},
  author = {Waseem, Faraz and Shahzad, Muhammad},
  year = {2024},
  month = dec,
  number = {arXiv:2412.18688},
  eprint = {2412.18688},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.18688},
  urldate = {2025-06-02},
  abstract = {An image may convey a thousand words, but a video composed of hundreds or thousands of image frames tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI's Sora, the current state-of-the-art system, is still limited to producing videos that are up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative AI with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 35 pages, 18 figures, Manuscript submitted to ACM},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\5HAN6IFD\\Waseem and Shahzad - 2024 - Video Is Worth a Thousand Images Exploring the Latest Trends in Long Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\E2J2U4G7\\2412.html}
}

@misc{waseemVideoWorthThousand2024a,
  title = {Video {{Is Worth}} a {{Thousand Images}}: {{Exploring}} the {{Latest Trends}} in {{Long Video Generation}}},
  shorttitle = {Video {{Is Worth}} a {{Thousand Images}}},
  author = {Waseem, Faraz and Shahzad, Muhammad},
  year = {2024},
  month = dec,
  number = {arXiv:2412.18688},
  eprint = {2412.18688},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.18688},
  urldate = {2025-06-02},
  abstract = {An image may convey a thousand words, but a video composed of hundreds or thousands of image frames tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI's Sora, the current state-of-the-art system, is still limited to producing videos that are up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative AI with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 35 pages, 18 figures, Manuscript submitted to ACM},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\AA9X4D7E\\Waseem and Shahzad - 2024 - Video Is Worth a Thousand Images Exploring the Latest Trends in Long Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\SGDXUUTM\\2412.html}
}

@misc{weiDreamRelationRelationCentricVideo2025,
  title = {{{DreamRelation}}: {{Relation-Centric Video Customization}}},
  shorttitle = {{{DreamRelation}}},
  author = {Wei, Yujie and Zhang, Shiwei and Yuan, Hangjie and Gong, Biao and Tang, Longxiang and Wang, Xiang and Qiu, Haonan and Li, Hengjia and Tan, Shuai and Zhang, Yingya and Shan, Hongming},
  year = {2025},
  month = mar,
  number = {arXiv:2503.07602},
  eprint = {2503.07602},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.07602},
  urldate = {2025-03-19},
  abstract = {Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Page: https://dreamrelation.github.io},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\D4JFDGKF\\Wei et al. - 2025 - DreamRelation Relation-Centric Video Customization.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\HVQ73XGB\\2503.html}
}

@misc{weiEchoVideoIdentityPreservingHuman2025,
  title = {{{EchoVideo}}: {{Identity-Preserving Human Video Generation}} by {{Multimodal Feature Fusion}}},
  shorttitle = {{{EchoVideo}}},
  author = {Wei, Jiangchuan and Yan, Shiyue and Lin, Wenfeng and Liu, Boyuan and Chen, Renjie and Guo, Mingyu},
  year = {2025},
  month = feb,
  number = {arXiv:2501.13452},
  eprint = {2501.13452},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.13452},
  urldate = {2025-03-22},
  abstract = {Recent advancements in video generation have significantly impacted various downstream applications, particularly in identity-preserving video generation (IPT2V). However, existing methods struggle with "copy-paste" artifacts and low similarity issues, primarily due to their reliance on low-level facial image information. This dependence can result in rigid facial appearances and artifacts reflecting irrelevant details. To address these challenges, we propose EchoVideo, which employs two key strategies: (1) an Identity Image-Text Fusion Module (IITF) that integrates high-level semantic features from text, capturing clean facial identity representations while discarding occlusions, poses, and lighting variations to avoid the introduction of artifacts; (2) a two-stage training strategy, incorporating a stochastic method in the second phase to randomly utilize shallow facial information. The objective is to balance the enhancements in fidelity provided by shallow features while mitigating excessive reliance on them. This strategy encourages the model to utilize high-level features during training, ultimately fostering a more robust representation of facial identities. EchoVideo effectively preserves facial identities and maintains full-body integrity. Extensive experiments demonstrate that it achieves excellent results in generating high-quality, controllability and fidelity videos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\WSXA88HJ\\Wei et al. - 2025 - EchoVideo Identity-Preserving Human Video Generation by Multimodal Feature Fusion.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\8HGCCDC9\\2501.html}
}

@misc{weiPersonalizedImageGeneration2025,
  title = {Personalized {{Image Generation}} with {{Deep Generative Models}}: {{A Decade Survey}}},
  shorttitle = {Personalized {{Image Generation}} with {{Deep Generative Models}}},
  author = {Wei, Yuxiang and Zheng, Yiheng and Zhang, Yabo and Liu, Ming and Ji, Zhilong and Zhang, Lei and Zuo, Wangmeng},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13081},
  eprint = {2502.13081},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13081},
  urldate = {2025-03-20},
  abstract = {Recent advancements in generative models have significantly facilitated the development of personalized content creation. Given a small set of images with user-specific concept, personalized image generation allows to create images that incorporate the specified concept and adhere to provided text descriptions. Due to its wide applications in content creation, significant effort has been devoted to this field in recent years. Nonetheless, the technologies used for personalization have evolved alongside the development of generative models, with their distinct and interrelated components. In this survey, we present a comprehensive review of generalized personalized image generation across various generative models, including traditional GANs, contemporary text-to-image diffusion models, and emerging multi-model autoregressive models. We first define a unified framework that standardizes the personalization process across different generative models, encompassing three key components, i.e., inversion spaces, inversion methods, and personalization schemes. This unified framework offers a structured approach to dissecting and comparing personalization techniques across different generative architectures. Building upon this unified framework, we further provide an in-depth analysis of personalization techniques within each generative model, highlighting their unique contributions and innovations. Through comparative analysis, this survey elucidates the current landscape of personalized image generation, identifying commonalities and distinguishing features among existing methods. Finally, we discuss the open challenges in the field and propose potential directions for future research. We keep tracing related works at https://github.com/csyxwei/Awesome-Personalized-Image-Generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 39 pages; under submission; more information: https://github.com/csyxwei/Awesome-Personalized-Image-Generation},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\VP9SSUZM\\Wei et al. - 2025 - Personalized Image Generation with Deep Generative Models A Decade Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\QJ6RIDQI\\2502.html}
}

@misc{wuAutomatedMovieGeneration2025,
  title = {Automated {{Movie Generation}} via {{Multi-Agent CoT Planning}}},
  author = {Wu, Weijia and Zhu, Zeyu and Shou, Mike Zheng},
  year = {2025},
  month = mar,
  number = {arXiv:2503.07314},
  eprint = {2503.07314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.07314},
  urldate = {2025-03-20},
  abstract = {Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Code Included
\par
Comment: The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent
\par
Very good paper. MUST USE!},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\7R3X982A\\Wu et al. - 2025 - Automated Movie Generation via Multi-Agent CoT Planning.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\WYC5ZQGX\\2503.html}
}

@misc{wuMindTimeTemporallyControlled2025,
  title = {Mind the {{Time}}: {{Temporally-Controlled Multi-Event Video Generation}}},
  shorttitle = {Mind the {{Time}}},
  author = {Wu, Ziyi and Siarohin, Aliaksandr and Menapace, Willi and Skorokhodov, Ivan and Fang, Yuwei and Chordia, Varnith and Gilitschenski, Igor and Tulyakov, Sergey},
  year = {2025},
  month = mar,
  number = {arXiv:2412.05263},
  eprint = {2412.05263},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05263},
  urldate = {2025-03-19},
  abstract = {Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing commercial and open-source models by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: CVPR 2025. Project Page: https://mint-video.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\AXPBAWPB\\Wu et al. - 2025 - Mind the Time Temporally-Controlled Multi-Event Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\CM5UGNBG\\2412.html}
}

@misc{wuMovieBenchHierarchicalMovie2025,
  title = {{{MovieBench}}: {{A Hierarchical Movie Level Dataset}} for {{Long Video Generation}}},
  shorttitle = {{{MovieBench}}},
  author = {Wu, Weijia and Liu, Mingyu and Zhu, Zeyu and Xia, Xi and Feng, Haoen and Wang, Wen and Lin, Kevin Qinghong and Shen, Chunhua and Shou, Mike Zheng},
  year = {2025},
  month = mar,
  number = {arXiv:2411.15262},
  eprint = {2411.15262},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.15262},
  urldate = {2025-04-24},
  abstract = {Recent advancements in video generation models, like Stable Video Diffusion, show promising results, but primarily focus on short, single-scene videos. These models struggle with generating long videos that involve multiple scenes, coherent narratives, and consistent characters. Furthermore, there is no publicly available dataset tailored for the analysis, evaluation, and training of long video generation models. In this paper, we present MovieBench: A Hierarchical Movie-Level Dataset for Long Video Generation, which addresses these challenges by providing unique contributions: (1) movie-length videos featuring rich, coherent storylines and multi-scene narratives, (2) consistency of character appearance and audio across scenes, and (3) hierarchical data structure contains high-level movie information and detailed shot-level descriptions. Experiments demonstrate that MovieBench brings some new insights and challenges, such as maintaining character ID consistency across multiple scenes for various characters. The dataset will be public and continuously maintained, aiming to advance the field of long video generation. Data can be found at: https://weijiawu.github.io/MovieBench/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: The project website is at: https://weijiawu.github.io/MovieBench/. Code: https://github.com/showlab/MovieBecnh},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\Q932FCQ2\\Wu et al. - 2025 - MovieBench A Hierarchical Movie Level Dataset for Long Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\VW523BLL\\2411.html}
}

@misc{xiaoOmniGenUnifiedImage2024,
  title = {{{OmniGen}}: {{Unified Image Generation}}},
  shorttitle = {{{OmniGen}}},
  author = {Xiao, Shitao and Wang, Yueze and Zhou, Junjie and Yuan, Huaying and Xing, Xingrun and Yan, Ruiran and Li, Chaofan and Wang, Shuting and Huang, Tiejun and Liu, Zheng},
  year = {2024},
  month = nov,
  number = {arXiv:2409.11340},
  eprint = {2409.11340},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.11340},
  urldate = {2025-03-20},
  abstract = {The emergence of Large Language Models (LLMs) has unified language generation tasks and revolutionized human-machine interaction. However, in the realm of image generation, a unified model capable of handling various tasks within a single framework remains largely unexplored. In this work, we introduce OmniGen, a new diffusion model for unified image generation. OmniGen is characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports various downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional plugins. Moreover, compared to existing diffusion models, it is more user-friendly and can complete complex tasks end-to-end through instructions without the need for extra intermediate steps, greatly simplifying the image generation workflow. 3) Knowledge Transfer: Benefit from learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of the chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and we will release our resources at https://github.com/VectorSpaceLab/OmniGen to foster future advancements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Update the paper for OmniGen-v1},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\M5SJJX53\\Xiao et al. - 2024 - OmniGen Unified Image Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\XPY5SXHU\\2409.html}
}

@misc{xieDreamFactoryPioneeringMultiScene2024,
  title = {{{DreamFactory}}: {{Pioneering Multi-Scene Long Video Generation}} with a {{Multi-Agent Framework}}},
  shorttitle = {{{DreamFactory}}},
  author = {Xie, Zhifei and Tang, Daniel and Tan, Dingwei and Klein, Jacques and Bissyand, Tegawend F. and Ezzini, Saad},
  year = {2024},
  month = aug,
  number = {arXiv:2408.11788},
  eprint = {2408.11788},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.11788},
  urldate = {2025-04-24},
  abstract = {Current video generation models excel at creating short, realistic clips, but struggle with longer, multi-scene videos. We introduce {\textbackslash}texttt\{DreamFactory\}, an LLM-based framework that tackles this challenge. {\textbackslash}texttt\{DreamFactory\} leverages multi-agent collaboration principles and a Key Frames Iteration Design Method to ensure consistency and style across long videos. It utilizes Chain of Thought (COT) to address uncertainties inherent in large language models. {\textbackslash}texttt\{DreamFactory\} generates long, stylistically coherent, and complex videos. Evaluating these long-form videos presents a challenge. We propose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene Style Consistency Score. To further research in this area, we contribute the Multi-Scene Videos Dataset containing over 150 human-rated videos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Software Engineering},
  note = {Comment: 13 pages, 8 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\2GT7K276\\Xie et al. - 2024 - DreamFactory Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\BDSYZGMN\\2408.html}
}

@misc{xingSurveyVideoDiffusion2024,
  title = {A {{Survey}} on {{Video Diffusion Models}}},
  author = {Xing, Zhen and Feng, Qijun and Chen, Haoran and Dai, Qi and Hu, Han and Xu, Hang and Wu, Zuxuan and Jiang, Yu-Gang},
  year = {2024},
  month = sep,
  number = {arXiv:2310.10647},
  eprint = {2310.10647},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.10647},
  urldate = {2025-05-30},
  abstract = {The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\8BGWQY4G\\Xing et al. - 2024 - A Survey on Video Diffusion Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\3JXJAI8R\\2310.html}
}

@misc{xingSurveyVideoDiffusion2024a,
  title = {A {{Survey}} on {{Video Diffusion Models}}},
  author = {Xing, Zhen and Feng, Qijun and Chen, Haoran and Dai, Qi and Hu, Han and Xu, Hang and Wu, Zuxuan and Jiang, Yu-Gang},
  year = {2024},
  month = sep,
  number = {arXiv:2310.10647},
  eprint = {2310.10647},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.10647},
  urldate = {2025-06-02},
  abstract = {The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\247KBFCU\\Xing et al. - 2024 - A Survey on Video Diffusion Models.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\I763MJ9F\\2310.html}
}

@article{xingToonCrafterGenerativeCartoon2024,
  title = {{{ToonCrafter}}: {{Generative Cartoon Interpolation}}},
  shorttitle = {{{ToonCrafter}}},
  author = {Xing, Jinbo and Liu, Hanyuan and Xia, Menghan and Zhang, Yong and Wang, Xintao and Shan, Ying and Wong, Tien-Tsin},
  year = {2024},
  month = dec,
  journal = {ACM Trans. Graph.},
  volume = {43},
  number = {6},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3687761},
  urldate = {2025-03-20},
  abstract = {We introduce ToonCrafter, a novel approach that transcends traditional correspondence-based cartoon video interpolation, paving the way for generative interpolation. Traditional methods, that implicitly assume linear motion and the absence of complicated phenomena like dis-occlusion, often struggle with the exaggerated non-linear and large motions with occlusion commonly found in cartoons, resulting in implausible or even failed interpolation results. To overcome these limitations, we explore the potential of adapting live-action video priors to better suit cartoon interpolation within a generative framework. ToonCrafter effectively addresses the challenges faced when applying live-action video motion priors to generative cartoon interpolation. First, we design a toon rectification learning strategy that seamlessly adapts live-action video priors to the cartoon domain, resolving the domain gap and content leakage issues. Next, we introduce a dual-reference-based 3D decoder to compensate for lost details due to the highly compressed latent prior spaces, ensuring the preservation of fine details in interpolation results. Finally, we design a flexible sketch encoder that empowers users with interactive control over the interpolation results. Experimental results demonstrate that our proposed method not only produces visually convincing and more natural dynamics, but also effectively handles dis-occlusion. The comparative evaluation demonstrates the notable superiority of our approach over existing competitors. Code and model weights are available at https://doubiiu.github.io/projects/ToonCrafter},
  langid = {english},
  file = {C:\Users\melmoghany\Zotero\storage\RUNZD8BS\Xing et al. - 2024 - ToonCrafter Generative Cartoon Interpolation.pdf}
}

@misc{xiongAutoregressiveModelsVision2024,
  title = {Autoregressive {{Models}} in {{Vision}}: {{A Survey}}},
  shorttitle = {Autoregressive {{Models}} in {{Vision}}},
  author = {Xiong, Jing and Liu, Gongye and Huang, Lun and Wu, Chengyue and Wu, Taiqiang and Mu, Yao and Yao, Yuan and Shen, Hui and Wan, Zhongwei and Huang, Jinfa and Tao, Chaofan and Yan, Shen and Yao, Huaxiu and Kong, Lingpeng and Yang, Hongxia and Zhang, Mi and Sapiro, Guillermo and Luo, Jiebo and Luo, Ping and Wong, Ngai},
  year = {2024},
  month = nov,
  number = {arXiv:2411.05902},
  eprint = {2411.05902},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.05902},
  urldate = {2025-06-02},
  abstract = {Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, {\textbackslash}textit\{i.e.\}, pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: {\textbackslash}url\{https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\DVE2VPD8\\Xiong et al. - 2024 - Autoregressive Models in Vision A Survey.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\YUQT3JZX\\2411.html}
}

@misc{yangCogVideoXTexttoVideoDiffusion2025,
  title = {{{CogVideoX}}: {{Text-to-Video Diffusion Models}} with {{An Expert Transformer}}},
  shorttitle = {{{CogVideoX}}},
  author = {Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and Yin, Da and Zhang, Yuxuan and Wang, Weihan and Cheng, Yean and Xu, Bin and Gu, Xiaotao and Dong, Yuxiao and Tang, Jie},
  year = {2025},
  month = mar,
  number = {arXiv:2408.06072},
  eprint = {2408.06072},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06072},
  urldate = {2025-04-24},
  abstract = {We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by ICLR2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\X6P55RIB\\Yang et al. - 2025 - CogVideoX Text-to-Video Diffusion Models with An Expert Transformer.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\D2N4DC9Y\\2408.html}
}

@misc{yangCogVideoXTexttoVideoDiffusion2025a,
  title = {{{CogVideoX}}: {{Text-to-Video Diffusion Models}} with {{An Expert Transformer}}},
  shorttitle = {{{CogVideoX}}},
  author = {Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and Yin, Da and Zhang, Yuxuan and Wang, Weihan and Cheng, Yean and Xu, Bin and Gu, Xiaotao and Dong, Yuxiao and Tang, Jie},
  year = {2025},
  month = mar,
  number = {arXiv:2408.06072},
  eprint = {2408.06072},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06072},
  urldate = {2025-04-24},
  abstract = {We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by ICLR2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\ANENJFC6\\Yang et al. - 2025 - CogVideoX Text-to-Video Diffusion Models with An Expert Transformer.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\PJQDZ9EW\\2408.html}
}

@misc{yangCogVideoXTexttoVideoDiffusion2025b,
  title = {{{CogVideoX}}: {{Text-to-Video Diffusion Models}} with {{An Expert Transformer}}},
  shorttitle = {{{CogVideoX}}},
  author = {Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and Yin, Da and Zhang, Yuxuan and Wang, Weihan and Cheng, Yean and Xu, Bin and Gu, Xiaotao and Dong, Yuxiao and Tang, Jie},
  year = {2025},
  month = mar,
  number = {arXiv:2408.06072},
  eprint = {2408.06072},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06072},
  urldate = {2025-05-01},
  abstract = {We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by ICLR2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\95QXPWUS\\Yang et al. - 2025 - CogVideoX Text-to-Video Diffusion Models with An Expert Transformer.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\IHTV77EC\\2408.html}
}

@misc{yangCogVideoXTexttoVideoDiffusion2025c,
  title = {{{CogVideoX}}: {{Text-to-Video Diffusion Models}} with {{An Expert Transformer}}},
  shorttitle = {{{CogVideoX}}},
  author = {Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and Yin, Da and Zhang, Yuxuan and Wang, Weihan and Cheng, Yean and Xu, Bin and Gu, Xiaotao and Dong, Yuxiao and Tang, Jie},
  year = {2025},
  month = mar,
  number = {arXiv:2408.06072},
  eprint = {2408.06072},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06072},
  urldate = {2025-06-18},
  abstract = {We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by ICLR2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\GJWWVMFF\\Yang et al. - 2025 - CogVideoX Text-to-Video Diffusion Models with An Expert Transformer.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\HJU3TSYZ\\2408.html}
}

@misc{yangSEEDStoryMultimodalLong2024,
  title = {{{SEED-Story}}: {{Multimodal Long Story Generation}} with {{Large Language Model}}},
  shorttitle = {{{SEED-Story}}},
  author = {Yang, Shuai and Ge, Yuying and Li, Yang and Chen, Yukang and Ge, Yixiao and Shan, Ying and Chen, Yingcong},
  year = {2024},
  month = oct,
  number = {arXiv:2407.08683},
  eprint = {2407.08683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.08683},
  urldate = {2025-03-22},
  abstract = {With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing narrative texts and vivid images in an interleaved manner, has emerged as a valuable and practical task with broad applications. However, this task poses significant challenges, as it necessitates the comprehension of the complex interplay between texts and images, and the ability to generate long sequences of coherent, contextually relevant texts and visuals. In this work, we propose SEED-Story, a novel method that leverages a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. Our model, built upon the powerful comprehension capability of MLLM, predicts text tokens as well as visual tokens, which are subsequently processed with an adapted visual de-tokenizer to produce images with consistent characters and styles. We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner. Additionally, we present a large-scale and high-resolution dataset named StoryStream for training our model and quantitatively evaluating the task of multimodal story generation in various aspects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Code Included!
\par
Comment: Our models, codes and datasets are released in https://github.com/TencentARC/SEED-Story
\par
Dataset:
\par
1) StoryStream ``part of the paper``
\par
2) ``StorySalon''},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\T6NUWE8L\\Yang et al. - 2024 - SEED-Story Multimodal Long Story Generation with Large Language Model.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\62QRGJGR\\2407.html}
}

@misc{yaoConceptConductorOrchestrating2024,
  title = {Concept {{Conductor}}: {{Orchestrating Multiple Personalized Concepts}} in {{Text-to-Image Synthesis}}},
  shorttitle = {Concept {{Conductor}}},
  author = {Yao, Zebin and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie},
  year = {2024},
  month = sep,
  number = {arXiv:2408.03632},
  eprint = {2408.03632},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.03632},
  urldate = {2025-03-20},
  abstract = {The customization of text-to-image models has seen significant advancements, yet generating multiple personalized concepts remains a challenging task. Current methods struggle with attribute leakage and layout confusion when handling multiple concepts, leading to reduced concept fidelity and semantic consistency. In this work, we introduce a novel training-free framework, Concept Conductor, designed to ensure visual fidelity and correct layout in multi-concept customization. Concept Conductor isolates the sampling processes of multiple custom models to prevent attribute leakage between different concepts and corrects erroneous layouts through self-attention-based spatial guidance. Additionally, we present a concept injection technique that employs shape-aware masks to specify the generation area for each concept. This technique injects the structure and appearance of personalized concepts through feature fusion in the attention layers, ensuring harmony in the final image. Extensive qualitative and quantitative experiments demonstrate that Concept Conductor can consistently generate composite images with accurate layouts while preserving the visual details of each concept. Compared to existing baselines, Concept Conductor shows significant performance improvements. Our method supports the combination of any number of concepts and maintains high fidelity even when dealing with visually similar concepts. The code and models are available at https://github.com/Nihukat/Concept-Conductor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  note = {Comment: Github Page: https://github.com/Nihukat/Concept-Conductor},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\C6M5XGSK\\Yao et al. - 2024 - Concept Conductor Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\MV98894W\\2408.html}
}

@misc{yesiltepeMotionShopZeroShotMotion2024,
  title = {{{MotionShop}}: {{Zero-Shot Motion Transfer}} in {{Video Diffusion Models}} with {{Mixture}} of {{Score Guidance}}},
  shorttitle = {{{MotionShop}}},
  author = {Yesiltepe, Hidir and Meral, Tuna Han Salih and Dunlop, Connor and Yanardag, Pinar},
  year = {2024},
  month = dec,
  number = {arXiv:2412.05355},
  eprint = {2412.05355},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05355},
  urldate = {2025-03-19},
  abstract = {In this work, we propose the first motion transfer approach in diffusion transformer through Mixture of Score Guidance (MSG), a theoretically-grounded framework for motion transfer in diffusion models. Our key theoretical contribution lies in reformulating conditional score to decompose motion score and content score in diffusion models. By formulating motion transfer as a mixture of potential energies, MSG naturally preserves scene composition and enables creative scene transformations while maintaining the integrity of transferred motion patterns. This novel sampling operates directly on pre-trained video diffusion models without additional training or fine-tuning. Through extensive experiments, MSG demonstrates successful handling of diverse scenarios including single object, multiple objects, and cross-object motion transfer as well as complex camera motion transfer. Additionally, we introduce MotionBench, the first motion transfer dataset consisting of 200 source videos and 1000 transferred motions, covering single/multi-object transfers, and complex camera motions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://motionshop-diffusion.github.io},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\RJEVLWDU\\Yesiltepe et al. - 2024 - MotionShop Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\LP2STJVU\\2412.html}
}

@misc{yinNUWAXLDiffusionDiffusion2023,
  title = {{{NUWA-XL}}: {{Diffusion}} over {{Diffusion}} for {{eXtremely Long Video Generation}}},
  shorttitle = {{{NUWA-XL}}},
  author = {Yin, Shengming and Wu, Chenfei and Yang, Huan and Wang, Jianfeng and Wang, Xiaodong and Ni, Minheng and Yang, Zhengyuan and Li, Linjie and Liu, Shuguang and Yang, Fan and Fu, Jianlong and Ming, Gong and Wang, Lijuan and Liu, Zicheng and Li, Houqiang and Duan, Nan},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12346},
  eprint = {2303.12346},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12346},
  urldate = {2025-04-24},
  abstract = {In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ``coarse-to-fine'' process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26{\textbackslash}\%) at the same hardware setting when generating 1024 frames. The homepage link is {\textbackslash}url\{https://msra-nuwa.azurewebsites.net/\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\BHV4QGLL\\Yin et al. - 2023 - NUWA-XL Diffusion over Diffusion for eXtremely Long Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\39FWH3L5\\2303.html}
}

@misc{yinNUWAXLDiffusionDiffusion2023a,
  title = {{{NUWA-XL}}: {{Diffusion}} over {{Diffusion}} for {{eXtremely Long Video Generation}}},
  shorttitle = {{{NUWA-XL}}},
  author = {Yin, Shengming and Wu, Chenfei and Yang, Huan and Wang, Jianfeng and Wang, Xiaodong and Ni, Minheng and Yang, Zhengyuan and Li, Linjie and Liu, Shuguang and Yang, Fan and Fu, Jianlong and Ming, Gong and Wang, Lijuan and Liu, Zicheng and Li, Houqiang and Duan, Nan},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12346},
  eprint = {2303.12346},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12346},
  urldate = {2025-05-30},
  abstract = {In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ``coarse-to-fine'' process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26{\textbackslash}\%) at the same hardware setting when generating 1024 frames. The homepage link is {\textbackslash}url\{https://msra-nuwa.azurewebsites.net/\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\69PWL9ME\\Yin et al. - 2023 - NUWA-XL Diffusion over Diffusion for eXtremely Long Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\73FDYZ3U\\2303.html}
}

@misc{yuanIdentityPreservingTexttoVideoGeneration2025,
  title = {Identity-{{Preserving Text-to-Video Generation}} by {{Frequency Decomposition}}},
  author = {Yuan, Shenghai and Huang, Jinfa and He, Xianyi and Ge, Yunyuan and Shi, Yujun and Chen, Liuhan and Luo, Jiebo and Yuan, Li},
  year = {2025},
  month = mar,
  number = {arXiv:2411.17440},
  eprint = {2411.17440},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.17440},
  urldate = {2025-04-11},
  abstract = {Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model's ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V. Code: https://github.com/PKU-YuanGroup/ConsisID.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  note = {Comment: CVPR 2025},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\M5UB9DGK\\Yuan et al. - 2025 - Identity-Preserving Text-to-Video Generation by Frequency Decomposition.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\IIHKLTRB\\2411.html}
}

@misc{zhangFantasyIDFaceKnowledge2025,
  title = {{{FantasyID}}: {{Face Knowledge Enhanced ID-Preserving Video Generation}}},
  shorttitle = {{{FantasyID}}},
  author = {Zhang, Yunpeng and Wang, Qiang and Jiang, Fan and Fan, Yaqi and Xu, Mu and Qi, Yonggang},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13995},
  eprint = {2502.13995},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13995},
  urldate = {2025-04-11},
  abstract = {Tuning-free approaches adapting large-scale pre-trained video diffusion models for identity-preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present a novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, a multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing cross-attention to inject guidance cues into DiT layers, a learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our model's superiority over the current tuning-free IPT2V methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\7WTNPGH9\\Zhang et al. - 2025 - FantasyID Face Knowledge Enhanced ID-Preserving Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\KFZTTU7U\\2502.html}
}

@misc{zhangTrainingFreeMotionGuidedVideo2025,
  title = {Training-{{Free Motion-Guided Video Generation}} with {{Enhanced Temporal Consistency Using Motion Consistency Loss}}},
  author = {Zhang, Xinyu and Duan, Zicheng and Gong, Dong and Liu, Lingqiao},
  year = {2025},
  month = jan,
  number = {arXiv:2501.07563},
  eprint = {2501.07563},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.07563},
  urldate = {2025-03-19},
  abstract = {In this paper, we address the challenge of generating temporally consistent videos with motion guidance. While many existing methods depend on additional control modules or inference-time fine-tuning, recent studies suggest that effective motion guidance is achievable without altering the model architecture or requiring extra training. Such approaches offer promising compatibility with various video generation foundation models. However, existing training-free methods often struggle to maintain consistent temporal coherence across frames or to follow guided motion accurately. In this work, we propose a simple yet effective solution that combines an initial-noise-based approach with a novel motion consistency loss, the latter being our key innovation. Specifically, we capture the inter-frame feature correlation patterns of intermediate features from a video diffusion model to represent the motion pattern of the reference video. We then design a motion consistency loss to maintain similar feature correlation patterns in the generated video, using the gradient of this loss in the latent space to guide the generation process for precise motion control. This approach improves temporal consistency across various motion control tasks while preserving the benefits of a training-free setup. Extensive experiments show that our method sets a new standard for efficient, temporally coherent video generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\LX4WFJHN\\Zhang et al. - 2025 - Training-Free Motion-Guided Video Generation with Enhanced Temporal Consistency Using Motion Consist.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\SSNGLEJ7\\2501.html}
}

@misc{zhouStoryDiffusionConsistentSelfAttention2024,
  title = {{{StoryDiffusion}}: {{Consistent Self-Attention}} for {{Long-Range Image}} and {{Video Generation}}},
  shorttitle = {{{StoryDiffusion}}},
  author = {Zhou, Yupeng and Zhou, Daquan and Cheng, Ming-Ming and Feng, Jiashi and Hou, Qibin},
  year = {2024},
  month = may,
  number = {arXiv:2405.01434},
  eprint = {2405.01434},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.01434},
  urldate = {2025-06-18},
  abstract = {For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\U2797X4S\\Zhou et al. - 2024 - StoryDiffusion Consistent Self-Attention for Long-Range Image and Video Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\6RJV6XBL\\2405.html}
}

@misc{zhouStoryMakerHolisticConsistent2024,
  title = {{{StoryMaker}}: {{Towards Holistic Consistent Characters}} in {{Text-to-image Generation}}},
  shorttitle = {{{StoryMaker}}},
  author = {Zhou, Zhengguang and Li, Jing and Li, Huaxia and Chen, Nemo and Tang, Xu},
  year = {2024},
  month = sep,
  number = {arXiv:2409.12576},
  eprint = {2409.12576},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.12576},
  urldate = {2025-04-11},
  abstract = {Tuning-free personalized image generation methods have achieved significant success in maintaining facial consistency, i.e., identities, even with multiple characters. However, the lack of holistic consistency in scenes with multiple characters hampers these methods' ability to create a cohesive narrative. In this paper, we introduce StoryMaker, a personalization solution that preserves not only facial consistency but also clothing, hairstyles, and body consistency, thus facilitating the creation of a story through a series of images. StoryMaker incorporates conditions based on face identities and cropped character images, which include clothing, hairstyles, and bodies. Specifically, we integrate the facial identity information with the cropped character images using the Positional-aware Perceiver Resampler (PPR) to obtain distinct character features. To prevent intermingling of multiple characters and the background, we separately constrain the cross-attention impact regions of different characters and the background using MSE loss with segmentation masks. Additionally, we train the generation network conditioned on poses to promote decoupling from poses. A LoRA is also employed to enhance fidelity and quality. Experiments underscore the effectiveness of our approach. StoryMaker supports numerous applications and is compatible with other societal plug-ins. Our source codes and model weights are available at https://github.com/RedAIGC/StoryMaker.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Code included!
\par
Comment: 12 pages, 5 figures},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\YQ4FQHLP\\Zhou et al. - 2024 - StoryMaker Towards Holistic Consistent Characters in Text-to-image Generation.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\QWARZ8YV\\2409.html}
}

@misc{zhuMovieFactoryAutomaticMovie2023,
  title = {{{MovieFactory}}: {{Automatic Movie Creation}} from {{Text}} Using {{Large Generative Models}} for {{Language}} and {{Images}}},
  shorttitle = {{{MovieFactory}}},
  author = {Zhu, Junchen and Yang, Huan and He, Huiguo and Wang, Wenjing and Tuo, Zixi and Cheng, Wen-Huang and Gao, Lianli and Song, Jingkuan and Fu, Jianlong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.07257},
  eprint = {2306.07257},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.07257},
  urldate = {2025-05-30},
  abstract = {In this paper, we present MovieFactory, a powerful framework to generate cinematic-picture (3072\${\textbackslash}times\$1280), film-style (multi-scene), and multi-modality (sounding) movies on the demand of natural languages. As the first fully automated movie generation model to the best of our knowledge, our approach empowers users to create captivating movies with smooth transitions using simple text inputs, surpassing existing methods that produce soundless videos limited to a single scene of modest quality. To facilitate this distinctive functionality, we leverage ChatGPT to expand user-provided text into detailed sequential scripts for movie generation. Then we bring scripts to life visually and acoustically through vision generation and audio retrieval. To generate videos, we extend the capabilities of a pretrained text-to-image diffusion model through a two-stage process. Firstly, we employ spatial finetuning to bridge the gap between the pretrained image model and the new video dataset. Subsequently, we introduce temporal learning to capture object motion. In terms of audio, we leverage sophisticated retrieval models to select and align audio elements that correspond to the plot and visual content of the movie. Extensive experiments demonstrate that our MovieFactory produces movies with realistic visuals, diverse scenes, and seamlessly fitting audio, offering users a novel and immersive experience. Generated samples can be found in YouTube or Bilibili (1080P).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\melmoghany\\Zotero\\storage\\4I4VY6P5\\Zhu et al. - 2023 - MovieFactory Automatic Movie Creation from Text using Large Generative Models for Language and Imag.pdf;C\:\\Users\\melmoghany\\Zotero\\storage\\II74C3MA\\2306.html}
}
